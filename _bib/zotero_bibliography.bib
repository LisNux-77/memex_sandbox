
@article{mckinney_python_nodate,
	title = {Python for {Data} {Analysis}},
	language = {en},
	author = {McKinney, Wes},
	pages = {541},
	file = {McKinney_Python for Data Analysis.pdf:/home/lisnux/Zotero/storage/YAZ9SRQ2/McKinney_Python for Data Analysis.pdf:application/pdf}
}

@book{grant_data_2019,
	address = {Boca Raton},
	series = {{ASA}-{CRC} series on statistical reasoning in science and society},
	title = {Data visualization: charts, maps, and interactive graphics},
	isbn = {978-1-138-55359-0 978-1-138-70760-3},
	shorttitle = {Data visualization},
	language = {en},
	publisher = {CRC Press, Taylor \& Francis Group},
	author = {Grant, Robert},
	year = {2019},
	keywords = {Information visualization},
	file = {Grant_2019_Data visualization.pdf:/home/lisnux/Zotero/storage/EHS8IGYU/Grant_2019_Data visualization.pdf:application/pdf}
}

@article{reitz_hitchhikers_nodate,
	title = {The {Hitchhiker}’s {Guide} {To} {Python}},
	language = {en},
	author = {Reitz, Kenneth and Schlusser, Tanya},
	pages = {322},
	file = {Reitz_Schlusser_The Hitchhiker’s Guide To Python.pdf:/home/lisnux/Zotero/storage/IITQQSSG/Reitz_Schlusser_The Hitchhiker’s Guide To Python.pdf:application/pdf}
}

@article{ridge_crowdsourcing_nodate,
	title = {Crowdsourcing {Our} {Cultural} {Heritage}},
	language = {en},
	author = {Ridge, Mia},
	pages = {16},
	file = {Ridge_Crowdsourcing Our Cultural Heritage.pdf:/home/lisnux/Zotero/storage/HUAGZATX/Ridge_Crowdsourcing Our Cultural Heritage.pdf:application/pdf}
}

@article{van_hyning_harnessing_2019,
	title = {Harnessing crowdsourcing for scholarly and {GLAM} purposes},
	volume = {16},
	issn = {1741-4113, 1741-4113},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/lic3.12507},
	doi = {10.1111/lic3.12507},
	abstract = {Full text search is vital to research of many kinds, but automatically creating transcriptions of most handwritten materials and ornate print is not yet technologically achievable. Even companies that have invested heavily in Optical Character Recognition (OCR) and search optimization, such as Alphabet (Google's parent company), have yet to make (or make widely available) technology for parsing handwritten or ornate text. Crowdsourcing conducted by scholars and cultural heritage practitioners offers an opportunity for us to engage with a diverse public who are interested in transcribing historical, literary, and other documents, in order to advance human knowledge. Crowdsourcing is a great vehicle for engaging students of all walks of life, with primary sources. Virtual volunteers all around the world are eager to learn and contribute to the vast project of making the world's textual records more widely accessible, not only for search, but for those, such as blind and partially sighted people, who use screen readers.},
	language = {en},
	number = {3-4},
	urldate = {2020-10-10},
	journal = {Literature Compass},
	author = {Van Hyning, Victoria},
	month = apr,
	year = {2019},
	pages = {e12507},
	file = {Van Hyning_2019_Harnessing crowdsourcing for scholarly and GLAM purposes.pdf:/home/lisnux/Zotero/storage/VUHJ7N8Q/Van Hyning_2019_Harnessing crowdsourcing for scholarly and GLAM purposes.pdf:application/pdf}
}

@article{turchin_quantitative_2018,
	title = {Quantitative historical analysis uncovers a single dimension of complexity that structures global variation in human social organization},
	volume = {115},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1708800115},
	doi = {10.1073/pnas.1708800115},
	abstract = {Do human societies from around the world exhibit similarities in the way that they are structured, and show commonalities in the ways that they have evolved? These are long-standing questions that have proven difficult to answer. To test between competing hypotheses, we constructed a massive repository of historical and archaeological information known as “Seshat: Global History Databank.” We systematically coded data on 414 societies from 30 regions around the world spanning the last 10,000 years. We were able to capture information on 51 variables reflecting nine characteristics of human societies, such as social scale, economy, features of governance, and information systems. Our analyses revealed that these different characteristics show strong relationships with each other and that a single principal component captures around three-quarters of the observed variation. Furthermore, we found that different characteristics of social complexity are highly predictable across different world regions. These results suggest that key aspects of social organization are functionally related and do indeed coevolve in predictable ways. Our findings highlight the power of the sciences and humanities working together to rigorously test hypotheses about general rules that may have shaped human history.},
	language = {en},
	number = {2},
	urldate = {2020-10-10},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Turchin, Peter and Currie, Thomas E. and Whitehouse, Harvey and François, Pieter and Feeney, Kevin and Mullins, Daniel and Hoyer, Daniel and Collins, Christina and Grohmann, Stephanie and Savage, Patrick and Mendel-Gleason, Gavin and Turner, Edward and Dupeyron, Agathe and Cioni, Enrico and Reddish, Jenny and Levine, Jill and Jordan, Greine and Brandl, Eva and Williams, Alice and Cesaretti, Rudolf and Krueger, Marta and Ceccarelli, Alessandro and Figliulo-Rosswurm, Joe and Tuan, Po-Ju and Peregrine, Peter and Marciniak, Arkadiusz and Preiser-Kapeller, Johannes and Kradin, Nikolay and Korotayev, Andrey and Palmisano, Alessio and Baker, David and Bidmead, Julye and Bol, Peter and Christian, David and Cook, Connie and Covey, Alan and Feinman, Gary and Júlíusson, Árni Daníel and Kristinsson, Axel and Miksic, John and Mostern, Ruth and Petrie, Cameron and Rudiak-Gould, Peter and ter Haar, Barend and Wallace, Vesna and Mair, Victor and Xie, Liye and Baines, John and Bridges, Elizabeth and Manning, Joseph and Lockhart, Bruce and Bogaard, Amy and Spencer, Charles},
	month = jan,
	year = {2018},
	pages = {E144--E151},
	file = {Turchin et al_2018_Quantitative historical analysis uncovers a single dimension of complexity that.pdf:/home/lisnux/Zotero/storage/JKFF3Z6W/Turchin et al_2018_Quantitative historical analysis uncovers a single dimension of complexity that.pdf:application/pdf}
}

@article{abbott_venice_nodate,
	title = {{VENICE} {GETS} {A} {TIME} {MACHINE}},
	language = {en},
	author = {Abbott, Alison},
	pages = {4},
	file = {Abbott_VENICE GETS A TIME MACHINE.pdf:/home/lisnux/Zotero/storage/GTWG4TCD/Abbott_VENICE GETS A TIME MACHINE.pdf:application/pdf}
}

@article{barranha_derivative_2018,
	title = {Derivative {Narratives}: {The} {Multiple} {Lives} of a {Masterpiece} on the {Internet}},
	volume = {70},
	issn = {1350-0775, 1468-0033},
	shorttitle = {Derivative {Narratives}},
	url = {https://www.tandfonline.com/doi/full/10.1111/muse.12190},
	doi = {10.1111/muse.12190},
	language = {en},
	number = {1-2},
	urldate = {2020-10-10},
	journal = {Museum International},
	author = {Barranha, Helena},
	month = jan,
	year = {2018},
	pages = {22--33},
	file = {Barranha_2018_Derivative Narratives.pdf:/home/lisnux/Zotero/storage/NTBH96MN/Barranha_2018_Derivative Narratives.pdf:application/pdf}
}

@article{kidd_new_2019,
	title = {\textit{{With} {New} {Eyes} {I} {See}} : embodiment, empathy and silence in digital heritage interpretation},
	volume = {25},
	issn = {1352-7258, 1470-3610},
	shorttitle = {\textit{{With} {New} {Eyes} {I} {See}}},
	url = {https://www.tandfonline.com/doi/full/10.1080/13527258.2017.1341946},
	doi = {10.1080/13527258.2017.1341946},
	abstract = {With New Eyes I See (WNEIS) was an immersive and itinerant digital heritage encounter exploring the exploitation of empathy made possible in such emergent formats. Located ‘in the wild’, and timed to coincide with the 2014 Centenary of the First World War, WNEIS transformed Cardiff’s civic centre as previously inaccessible stories and archival materials were projected onto, and playfully manipulated by, buildings and the natural environment. The research that underpinned the project unearthed a hitherto untold story about the experiences and fates of those who left their posts at Amgueddfa Cymru – National Museum Wales to go and fight in WW1. Focusing on the story of Botanist Cyril Mortimer Green, and moving between past and present, known and unknown, presence and absence, participants encountered a re-scripting and multiple layering of the cityscape, and an uneasy archaeology of the museological endeavour. WNEIS foregrounded opportunities for touching, listening and feeling; as such it was a multimodal form of investigation for participants. This article uses focus group materials to explore the intersecting themes of ‘embodiment’, ‘empathy’ and ‘silence’ that emerged in reflections. It reveals an audience ready for digital cultural heritage that embraces ambiguity in the examination and negotiation of meaning.},
	language = {en},
	number = {1},
	urldate = {2020-10-10},
	journal = {International Journal of Heritage Studies},
	author = {Kidd, Jenny},
	month = jan,
	year = {2019},
	pages = {54--66},
	file = {Kidd_2019_iWith New Eyes I See-i.pdf:/home/lisnux/Zotero/storage/LKLVEMLF/Kidd_2019_iWith New Eyes I See-i.pdf:application/pdf}
}

@article{stokes_digital_2015,
	title = {Digital {Approaches} to {Paleography} and {Book} {History}: {Some} {Challenges}, {Present} and {Future}},
	volume = {2},
	issn = {2297-2668},
	shorttitle = {Digital {Approaches} to {Paleography} and {Book} {History}},
	url = {http://journal.frontiersin.org/Article/10.3389/fdigh.2015.00005/abstract},
	doi = {10.3389/fdigh.2015.00005},
	language = {en},
	urldate = {2020-10-10},
	journal = {Frontiers in Digital Humanities},
	author = {Stokes, Peter A.},
	month = oct,
	year = {2015},
	file = {Stokes_2015_Digital Approaches to Paleography and Book History.pdf:/home/lisnux/Zotero/storage/TM75FHYC/Stokes_2015_Digital Approaches to Paleography and Book History.pdf:application/pdf}
}

@article{drucker_is_2013,
	title = {Is {There} a “{Digital}” {Art} {History}?},
	volume = {29},
	issn = {0197-3762, 1477-2809},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01973762.2013.761106},
	doi = {10.1080/01973762.2013.761106},
	language = {en},
	number = {1-2},
	urldate = {2020-10-10},
	journal = {Visual Resources},
	author = {Drucker, Johanna},
	month = jun,
	year = {2013},
	pages = {5--13},
	file = {Drucker_2013_Is There a “Digital” Art History.pdf:/home/lisnux/Zotero/storage/FG9IHM5D/Drucker_2013_Is There a “Digital” Art History.pdf:application/pdf}
}

@article{windhager_visualization_2019,
	title = {Visualization of {Cultural} {Heritage} {Collection} {Data}: {State} of the {Art} and {Future} {Challenges}},
	volume = {25},
	issn = {1077-2626, 1941-0506, 2160-9306},
	shorttitle = {Visualization of {Cultural} {Heritage} {Collection} {Data}},
	url = {https://ieeexplore.ieee.org/document/8352050/},
	doi = {10.1109/TVCG.2018.2830759},
	abstract = {After decades of digitization, large cultural heritage collections have emerged on the web, which contain massive stocks of content from galleries, libraries, archives, and museums. This increase in digital cultural heritage data promises new modes of analysis and increased levels of access for academic scholars and casual users alike. Going beyond the standard representations of search-centric and grid-based interfaces, a multitude of approaches has recently started to enable visual access to cultural collections, and to explore them as complex and comprehensive information spaces by the means of interactive visualizations. In contrast to conventional web interfaces, we witness a widening spectrum of innovative visualization types specially designed for rich collections from the cultural heritage sector. This new class of information visualizations gives rise to a notable diversity of interaction and representation techniques while lending currency and urgency to a discussion about principles such as serendipity, generosity, and criticality in connection with visualization design. With this survey, we review information visualization approaches to digital cultural heritage collections and reﬂect on the state of the art in techniques and design choices. We contextualize our survey with humanist perspectives on the ﬁeld and point out opportunities for future research.},
	language = {en},
	number = {6},
	urldate = {2020-10-10},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Windhager, Florian and Federico, Paolo and Schreder, Gunther and Glinka, Katrin and Dork, Marian and Miksch, Silvia and Mayr, Eva},
	month = jun,
	year = {2019},
	pages = {2311--2330},
	file = {Windhager et al_2019_Visualization of Cultural Heritage Collection Data.pdf:/home/lisnux/Zotero/storage/8LSV586W/Windhager et al_2019_Visualization of Cultural Heritage Collection Data.pdf:application/pdf}
}

@article{kraker_case_2011,
	title = {The case for an open science in technology enhanced learning},
	volume = {3},
	issn = {1753-5255, 1753-5263},
	url = {http://www.inderscience.com/link.php?id=45454},
	doi = {10.1504/IJTEL.2011.045454},
	abstract = {In this paper, we make the case for an open science in technology enhanced learning (TEL). Open science means opening up the research process by making all of its outcomes, and the way in which these outcomes were achieved, publicly available on the World Wide Web. In our vision, the adoption of open science instruments provides a set of solid and sustainable ways to connect the disjoint communities in TEL. Furthermore, we envision that researchers in TEL would be able to reproduce the results from any paper using the instruments of open science. Therefore, we introduce the concept of open methodology, which stands for sharing the methodological details of the evaluation provided, and the tools used for data collection and analysis. We discuss the potential benefits, but also the issues of an open science, and conclude with a set of recommendations for implementing open science in TEL.},
	language = {en},
	number = {6},
	urldate = {2020-10-10},
	journal = {International Journal of Technology Enhanced Learning},
	author = {Kraker, Peter and Leony, Derick and Reinhardt, Wolfgang and Gü, N.A. and Beham, nter},
	year = {2011},
	pages = {643},
	file = {Kraker et al_2011_The case for an open science in technology enhanced learning.pdf:/home/lisnux/Zotero/storage/YR6F6W66/Kraker et al_2011_The case for an open science in technology enhanced learning.pdf:application/pdf}
}

@techreport{masuzzo_you_2017,
	type = {preprint},
	title = {Do you speak open science? {Resources} and tips to learn the language},
	shorttitle = {Do you speak open science?},
	url = {https://peerj.com/preprints/2689v1},
	abstract = {The internet era, large-scale computing and storage resources, mobile devices, social media, and their high uptake among different groups of people, have all deeply changed the way knowledge is created, communicated, and further deployed. These advances have enabled a radical transformation of the practice of science, which is now more open, more global and collaborative, and closer to society than ever. Open science has therefore become an increasingly important topic. Moreover, as open science is actively pursued by several high-profile funders and institutions, it has fast become a crucial matter to all researchers. However, because this widespread interest in open science has emerged relatively recently, its definition and implementation are constantly shifting and evolving, sometimes leaving researchers in doubt about how to adopt open science, and which are the best practices to follow.},
	language = {en},
	urldate = {2020-10-10},
	institution = {PeerJ Preprints},
	author = {Masuzzo, Paola and Martens, Lennart},
	month = jan,
	year = {2017},
	doi = {10.7287/peerj.preprints.2689v1},
	file = {Masuzzo_Martens_2017_Do you speak open science.pdf:/home/lisnux/Zotero/storage/TRWRW2JW/Masuzzo_Martens_2017_Do you speak open science.pdf:application/pdf}
}

@article{mikolov_distributed_2013,
	title = {Distributed {Representations} of {Words} and {Phrases} and their {Compositionality}},
	url = {http://arxiv.org/abs/1310.4546},
	abstract = {The recently introduced continuous Skip-gram model is an efﬁcient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain signiﬁcant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling.},
	language = {en},
	urldate = {2020-10-10},
	journal = {arXiv:1310.4546 [cs, stat]},
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	month = oct,
	year = {2013},
	note = {arXiv: 1310.4546},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Mikolov et al_2013_Distributed Representations of Words and Phrases and their Compositionality.pdf:/home/lisnux/Zotero/storage/XR6X8ECH/Mikolov et al_2013_Distributed Representations of Words and Phrases and their Compositionality.pdf:application/pdf}
}

@article{bojanowski_enriching_2017,
	title = {Enriching {Word} {Vectors} with {Subword} {Information}},
	url = {http://arxiv.org/abs/1607.04606},
	abstract = {Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.},
	language = {en},
	urldate = {2020-10-10},
	journal = {arXiv:1607.04606 [cs]},
	author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
	month = jun,
	year = {2017},
	note = {arXiv: 1607.04606},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Bojanowski et al_2017_Enriching Word Vectors with Subword Information.pdf:/home/lisnux/Zotero/storage/NXD7Y4MJ/Bojanowski et al_2017_Enriching Word Vectors with Subword Information.pdf:application/pdf}
}

@article{hermann_teaching_2015,
	title = {Teaching {Machines} to {Read} and {Comprehend}},
	url = {http://arxiv.org/abs/1506.03340},
	abstract = {Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we deﬁne a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.},
	language = {en},
	urldate = {2020-10-10},
	journal = {arXiv:1506.03340 [cs]},
	author = {Hermann, Karl Moritz and Kočiský, Tomáš and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},
	month = nov,
	year = {2015},
	note = {arXiv: 1506.03340},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	file = {Hermann et al. - 2015 - Teaching Machines to Read and Comprehend.pdf:/home/lisnux/Zotero/storage/2XZ7NGRA/Hermann et al. - 2015 - Teaching Machines to Read and Comprehend.pdf:application/pdf}
}

@article{bahdanau_neural_2016,
	title = {Neural {Machine} {Translation} by {Jointly} {Learning} to {Align} and {Translate}},
	url = {http://arxiv.org/abs/1409.0473},
	abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder–decoders and encode a source sentence into a ﬁxed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a ﬁxed-length vector is a bottleneck in improving the performance of this basic encoder–decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
	language = {en},
	urldate = {2020-10-10},
	journal = {arXiv:1409.0473 [cs, stat]},
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	month = may,
	year = {2016},
	note = {arXiv: 1409.0473},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Bahdanau et al_2016_Neural Machine Translation by Jointly Learning to Align and Translate.pdf:/home/lisnux/Zotero/storage/D4ZNWBTH/Bahdanau et al_2016_Neural Machine Translation by Jointly Learning to Align and Translate.pdf:application/pdf}
}

@article{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	language = {en},
	urldate = {2020-10-10},
	journal = {arXiv:1706.03762 [cs]},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
	note = {arXiv: 1706.03762},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Vaswani et al_2017_Attention Is All You Need.pdf:/home/lisnux/Zotero/storage/7EKZXGAJ/Vaswani et al_2017_Attention Is All You Need.pdf:application/pdf}
}

@article{wang_glue_2019,
	title = {{GLUE}: {A} {Multi}-{Task} {Benchmark} and {Analysis} {Platform} for {Natural} {Language} {Understanding}},
	shorttitle = {{GLUE}},
	url = {http://arxiv.org/abs/1804.07461},
	abstract = {For natural language understanding (NLU) technology to be maximally useful, it must be able to process language in a way that is not exclusive to a single task, genre, or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation (GLUE) benchmark, a collection of tools for evaluating the performance of models across a diverse set of existing NLU tasks. By including tasks with limited training data, GLUE is designed to favor and encourage models that share general linguistic knowledge across tasks. GLUE also includes a hand-crafted diagnostic test suite that enables detailed linguistic analysis of models. We evaluate baselines based on current methods for transfer and representation learning and ﬁnd that multi-task training on all tasks performs better than training a separate model per task. However, the low absolute performance of our best model indicates the need for improved general NLU systems.},
	language = {en},
	urldate = {2020-10-10},
	journal = {arXiv:1804.07461 [cs]},
	author = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
	month = feb,
	year = {2019},
	note = {arXiv: 1804.07461},
	keywords = {Computer Science - Computation and Language},
	file = {Wang et al_2019_GLUE.pdf:/home/lisnux/Zotero/storage/JB8684BU/Wang et al_2019_GLUE.pdf:application/pdf}
}

@article{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be ﬁnetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspeciﬁc architecture modiﬁcations.},
	language = {en},
	urldate = {2020-10-10},
	journal = {arXiv:1810.04805 [cs]},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv: 1810.04805},
	keywords = {Computer Science - Computation and Language},
	file = {Devlin et al_2019_BERT.pdf:/home/lisnux/Zotero/storage/6V5C97FC/Devlin et al_2019_BERT.pdf:application/pdf}
}

@article{ribeiro_why_2016,
	title = {"{Why} {Should} {I} {Trust} {You}?": {Explaining} the {Predictions} of {Any} {Classifier}},
	shorttitle = {"{Why} {Should} {I} {Trust} {You}?},
	url = {http://arxiv.org/abs/1602.04938},
	abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classiﬁer in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the ﬂexibility of these methods by explaining diﬀerent models for text (e.g. random forests) and image classiﬁcation (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classiﬁer, and identifying why a classiﬁer should not be trusted.},
	language = {en},
	urldate = {2020-10-10},
	journal = {arXiv:1602.04938 [cs, stat]},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	month = aug,
	year = {2016},
	note = {arXiv: 1602.04938},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Ribeiro et al_2016_Why Should I Trust You.pdf:/home/lisnux/Zotero/storage/U5HM8R6W/Ribeiro et al_2016_Why Should I Trust You.pdf:application/pdf}
}

@inproceedings{bender_climbing_2020,
	address = {Online},
	title = {Climbing towards {NLU}: {On} {Meaning}, {Form}, and {Understanding} in the {Age} of {Data}},
	shorttitle = {Climbing towards {NLU}},
	url = {https://www.aclweb.org/anthology/2020.acl-main.463},
	doi = {10.18653/v1/2020.acl-main.463},
	abstract = {The success of the large neural language models on many NLP tasks is exciting. However, we ﬁnd that these successes sometimes lead to hype in which these models are being described as “understanding” language or capturing “meaning”. In this position paper, we argue that a system trained only on form has a priori no way to learn meaning. In keeping with the ACL 2020 theme of “Taking Stock of Where We’ve Been and Where We’re Going”, we argue that a clear understanding of the distinction between form and meaning will help guide the ﬁeld towards better science around natural language understanding.},
	language = {en},
	urldate = {2020-10-10},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Bender, Emily M. and Koller, Alexander},
	year = {2020},
	pages = {5185--5198},
	file = {Bender_Koller_2020_Climbing towards NLU.pdf:/home/lisnux/Zotero/storage/T463AU4Q/Bender_Koller_2020_Climbing towards NLU.pdf:application/pdf}
}

@article{bolukbasi_man_2016,
	title = {Man is to {Computer} {Programmer} as {Woman} is to {Homemaker}? {Debiasing} {Word} {Embeddings}},
	shorttitle = {Man is to {Computer} {Programmer} as {Woman} is to {Homemaker}?},
	url = {http://arxiv.org/abs/1607.06520},
	abstract = {The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is ﬁrst shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender deﬁnition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between between the words receptionist and female, while maintaining desired associations such as between the words queen and female. We deﬁne metrics to quantify both direct and indirect gender biases in embeddings, and develop algorithms to “debias” the embedding. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms signiﬁcantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.},
	language = {en},
	urldate = {2020-10-10},
	journal = {arXiv:1607.06520 [cs, stat]},
	author = {Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James and Saligrama, Venkatesh and Kalai, Adam},
	month = jul,
	year = {2016},
	note = {arXiv: 1607.06520},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Bolukbasi et al_2016_Man is to Computer Programmer as Woman is to Homemaker.pdf:/home/lisnux/Zotero/storage/8BLV79Y7/Bolukbasi et al_2016_Man is to Computer Programmer as Woman is to Homemaker.pdf:application/pdf}
}

@article{sap_social_2020,
	title = {Social {Bias} {Frames}: {Reasoning} about {Social} and {Power} {Implications} of {Language}},
	shorttitle = {Social {Bias} {Frames}},
	url = {http://arxiv.org/abs/1911.03891},
	abstract = {Warning: this paper contains content that may be offensive or upsetting.},
	language = {en},
	urldate = {2020-10-10},
	journal = {arXiv:1911.03891 [cs]},
	author = {Sap, Maarten and Gabriel, Saadia and Qin, Lianhui and Jurafsky, Dan and Smith, Noah A. and Choi, Yejin},
	month = apr,
	year = {2020},
	note = {arXiv: 1911.03891},
	keywords = {Computer Science - Computation and Language},
	file = {Sap et al_2020_Social Bias Frames.pdf:/home/lisnux/Zotero/storage/P6H7QFTQ/Sap et al_2020_Social Bias Frames.pdf:application/pdf}
}

@article{lecun_yann_deep_2015,
	title = {Deep {Learning}},
	doi = {http://dx-doi-org.uaccess.univie.ac.at/10.1038/nature14539},
	author = {Lecun Yann},
	month = may,
	year = {2015}
}

@book{gold_debates_2019,
	title = {Debates in the {Digital} {Humanities} 2019},
	isbn = {978-1-4529-6166-8 978-1-5179-0693-1},
	url = {http://www.jstor.org/stable/10.5749/j.ctvg251hk},
	language = {en},
	urldate = {2020-10-10},
	publisher = {University of Minnesota Press},
	editor = {Gold, Matthew K. and Klein, Lauren F.},
	month = apr,
	year = {2019},
	doi = {10.5749/j.ctvg251hk},
	file = {Gold_Klein_2019_Debates in the Digital Humanities 2019.pdf:/home/lisnux/Zotero/storage/3C4S5SS4/Gold_Klein_2019_Debates in the Digital Humanities 2019.pdf:application/pdf}
}

@article{svensson_three_2020,
	title = {Three {Premises} of {Big} {Digital} {Humanities}},
	language = {en},
	author = {Svensson, Patrik},
	year = {2020},
	pages = {50},
	file = {Svensson_2020_Three Premises of Big Digital Humanities.pdf:/home/lisnux/Zotero/storage/JUPRH4ZB/Svensson_2020_Three Premises of Big Digital Humanities.pdf:application/pdf}
}

@book{bhattacharyya_deep_2020,
	address = {Boston},
	title = {Deep learning},
	isbn = {978-3-11-067079-0},
	language = {en},
	publisher = {DE GRUYTER},
	editor = {Bhattacharyya, Siddhartha and Sasel, Vaclav and Hassanien, Aboul Ella and Saha, Satadal and Tripathy, B. K.},
	year = {2020},
	file = {Bhattacharyya et al_2020_Deep learning.pdf:/home/lisnux/Zotero/storage/IQ834U48/Bhattacharyya et al_2020_Deep learning.pdf:application/pdf}
}

@article{mnih_scalable_nodate,
	title = {A {Scalable} {Hierarchical} {Distributed} {Language} {Model}},
	abstract = {Neural probabilistic language models (NPLMs) have been shown to be competitive with and occasionally superior to the widely-used n-gram language models. The main drawback of NPLMs is their extremely long training and testing times. Morin and Bengio have proposed a hierarchical language model built around a binary tree of words, which was two orders of magnitude faster than the nonhierarchical model it was based on. However, it performed considerably worse than its non-hierarchical counterpart in spite of using a word tree created using expert knowledge. We introduce a fast hierarchical language model along with a simple feature-based algorithm for automatic construction of word trees from the data. We then show that the resulting models can outperform non-hierarchical neural models as well as the best n-gram models.},
	language = {en},
	author = {Mnih, Andriy and Hinton, Geoffrey E},
	pages = {8},
	file = {Mnih_Hinton_A Scalable Hierarchical Distributed Language Model.pdf:/home/lisnux/Zotero/storage/4VYTZD96/Mnih_Hinton_A Scalable Hierarchical Distributed Language Model.pdf:application/pdf}
}

@inproceedings{pennington_glove_2014,
	address = {Doha, Qatar},
	title = {Glove: {Global} {Vectors} for {Word} {Representation}},
	shorttitle = {Glove},
	url = {http://aclweb.org/anthology/D14-1162},
	doi = {10.3115/v1/D14-1162},
	abstract = {Recent methods for learning vector space representations of words have succeeded in capturing ﬁne-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efﬁciently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75\% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.},
	language = {en},
	urldate = {2020-10-10},
	booktitle = {Proceedings of the 2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
	year = {2014},
	pages = {1532--1543},
	file = {Pennington et al_2014_Glove.pdf:/home/lisnux/Zotero/storage/VUT5EWG9/Pennington et al_2014_Glove.pdf:application/pdf}
}

@inproceedings{conneau_supervised_2017,
	address = {Copenhagen, Denmark},
	title = {Supervised {Learning} of {Universal} {Sentence} {Representations} from {Natural} {Language} {Inference} {Data}},
	url = {http://aclweb.org/anthology/D17-1070},
	doi = {10.18653/v1/D17-1070},
	abstract = {Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like SkipThought vectors (Kiros et al., 2015) on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available1.},
	language = {en},
	urldate = {2020-10-10},
	booktitle = {Proceedings of the 2017 {Conference} on {Empirical} {Methods} in {Natural}           {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Conneau, Alexis and Kiela, Douwe and Schwenk, Holger and Barrault, Loïc and Bordes, Antoine},
	year = {2017},
	pages = {670--680},
	file = {Conneau et al_2017_Supervised Learning of Universal Sentence Representations from Natural Language.pdf:/home/lisnux/Zotero/storage/533TK22K/Conneau et al_2017_Supervised Learning of Universal Sentence Representations from Natural Language.pdf:application/pdf}
}

@book{cd_manning_foundations_1999,
	title = {Foundations of statistical natural language processing},
	isbn = {978-0-262-13360-9},
	author = {C.D. Manning, H. Schütze},
	year = {1999}
}

@book{cd_manning_introduction_2008,
	title = {Introduction to information retrieval},
	isbn = {978-0-521-86571-5},
	author = {C.D. Manning, H. Schütze},
	year = {2008}
}

@book{kumar_mastering_2016,
	address = {Birmingham, UK},
	title = {Mastering {Text} {Mining} with {R}},
	isbn = {978-1-78355-181-1},
	url = {http://search.ebscohost.com/login.aspx?direct=true&db=nlebk&AN=1445459&site=ehost-live},
	abstract = {About This BookDevelop all the relevant skills for building text-mining apps with R with this easy-to-follow guideGain in-depth understanding of the text mining process with lucid implementation in the R languageExample-rich guide that lets you gain high-quality information from text dataWho This Book Is ForIf you are an R programmer, analyst, or data scientist who wants to gain experience in performing text data mining and analytics with R, then this book is for you. Exposure to working with statistical methods and language processing would be helpful.What You Will LearnGet acquainted with some of the highly efficient R packages such as OpenNLP and RWeka to perform various steps in the text mining processAccess and manipulate data from different sources such as JSON and HTTPProcess text using regular expressionsGet to know the different approaches of tagging texts, such as POS tagging, to get started with text analysisExplore different dimensionality reduction techniques, such as Principal Component Analysis (PCA), and understand its implementation in RDiscover the underlying themes or topics that are present in an unstructured collection of documents, using common topic models such as Latent Dirichlet Allocation (LDA)Build a baseline sentenceIn DetailThis book will help you develop a thorough understanding of the steps in the text mining process and gain confidence in applying the concepts to build text-data driven products.Starting with basic information about the statistics concepts used in text mining, the book will teach you how to access, cleanse, and process text using the R language and teach you how to analyze them. It will equip you with the tools and the associated knowledge about different tagging, chunking, and entailment approaches and their usage in natural language processing.Moving on, the book will teach you different dimensionality reduction techniques and their implementation in R. Next, we will cover pattern recognition in text data utilizing classification mechanisms, perform entity recognition, and develop an ontology learning framework.By the end of the book, you will develop a practical application from the concepts learned, and will understand how text mining can be leveraged to analyze the massively available data on social media.},
	language = {English},
	publisher = {Packt Publishing},
	author = {Kumar, Ashish and Paul, Avinash},
	year = {2016},
	keywords = {Application software--Development, COMPUTERS / Data Science / Data Visualization, Data mining, R (Computer program language), Text processing (Computer science)}
}

@article{sebastiani_machine_2002,
	title = {Machine learning in automated text categorization},
	volume = {34},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/505282.505283},
	doi = {10.1145/505282.505283},
	language = {en},
	number = {1},
	urldate = {2020-10-10},
	journal = {ACM Computing Surveys},
	author = {Sebastiani, Fabrizio},
	month = mar,
	year = {2002},
	pages = {1--47},
	file = {Sebastiani_2002_Machine learning in automated text categorization.pdf:/home/lisnux/Zotero/storage/I4KTWZAS/Sebastiani_2002_Machine learning in automated text categorization.pdf:application/pdf}
}

@book{ekstein_text_2019,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Text, {Speech}, and {Dialogue}: 22nd {International} {Conference}, {TSD} 2019, {Ljubljana}, {Slovenia}, {September} 11–13, 2019, {Proceedings}},
	volume = {11697},
	isbn = {978-3-030-27946-2 978-3-030-27947-9},
	shorttitle = {Text, {Speech}, and {Dialogue}},
	url = {http://link.springer.com/10.1007/978-3-030-27947-9},
	language = {en},
	urldate = {2020-10-10},
	publisher = {Springer International Publishing},
	editor = {Ekštein, Kamil},
	year = {2019},
	doi = {10.1007/978-3-030-27947-9},
	file = {Ekštein_2019_Text, Speech, and Dialogue.pdf:/home/lisnux/Zotero/storage/Q39KB9RS/Ekštein_2019_Text, Speech, and Dialogue.pdf:application/pdf}
}

@article{davis_universal_nodate,
	title = {The {Universal} {Computer}},
	language = {en},
	author = {Davis, Martin},
	pages = {232},
	file = {Davis_The Universal Computer.pdf:/home/lisnux/Zotero/storage/XR9U2IRX/Davis_The Universal Computer.pdf:application/pdf}
}

@misc{gulliver_100_2012,
	title = {100 {Commandments} of {Twitter} for {Academics}},
	url = {http://chronicle.com/article/10-Commandments-of-Twitter-for/131813/},
	language = {en},
	author = {Gulliver, Katrina},
	month = may,
	year = {2012}
}

@article{machlis_22_2011,
	title = {22 free tools for data visualization and analysis},
	url = {http://www.computerworld.com/article/2506820/business-intelligence/chart-and-image-gallery-30-free-tools-for-data-visualization-and-analysis.html},
	abstract = {Got data? These useful tools can turn it into informative, engaging graphics.},
	language = {en},
	journal = {Computerworld},
	author = {Machlis, Sharon},
	month = apr,
	year = {2011},
	keywords = {act\_Visualizing, AnalyzeQualitatively, AnalyzeStatistically, meta\_GiveOverview, obj\_Tools}
}

@misc{gulliver_100_2012-1,
	title = {100 {Commandments} of {Twitter} for {Academics}},
	url = {http://chronicle.com/article/10-Commandments-of-Twitter-for/131813/},
	language = {en},
	author = {Gulliver, Katrina},
	month = may,
	year = {2012}
}

@article{machlis_22_2011-1,
	title = {22 free tools for data visualization and analysis},
	url = {http://www.computerworld.com/article/2506820/business-intelligence/chart-and-image-gallery-30-free-tools-for-data-visualization-and-analysis.html},
	abstract = {Got data? These useful tools can turn it into informative, engaging graphics.},
	language = {en},
	journal = {Computerworld},
	author = {Machlis, Sharon},
	month = apr,
	year = {2011},
	keywords = {act\_Visualizing, AnalyzeQualitatively, AnalyzeStatistically, meta\_GiveOverview, obj\_Tools}
}

@misc{scheinfeldt_3_2009,
	title = {3 {Innovation} {Killers} in {Digital} {Humanities}},
	url = {http://www.foundhistory.org/2009/10/16/3-innovation-killers-in-digital-humanities/},
	language = {en},
	urldate = {2009-10-16},
	journal = {Found History},
	author = {Scheinfeldt, Tom},
	month = oct,
	year = {2009},
	keywords = {meta\_Assessing, obj\_DigitalHumanities}
}

@misc{ciula_bibliography_2013,
	title = {A {Bibliography} of {Publications} {Related} to the {Text} {Encoding} {Initiative}},
	url = {http://www.tei-c.org/Support/Learn/tei_bibliography.xml},
	journal = {Text Encoding Inititative},
	collaborator = {Ciula, Arianna},
	year = {2013}
}

@article{pivorun_biotelemetry_1976,
	title = {A biotelemetry study of the thermoregulatory patterns of {Tamias} striatus and {Eutamias} minimus during hibernation},
	volume = {53},
	issn = {0300-9629},
	url = {http://www.sciencedirect.com/science/article/pii/S0300962976800345},
	abstract = {The hibernation period of Tamias striatus is composed of a test drop, pre-plateau and plateau stage. The number of test drops during the test drop stage increases with a lowering of ambient temperature. The number of torpor bouts during the pre-plateau stage increases with a lowering of ambient temperature. The duration of bouts of torpor during the plateau stage increases linearly with a decrease in ambient temperature. (duration in hr = 139·60−5·93 Ta).
Tamias striatus displays a control on the rate of cooling during the entry into a torpor bout. Differential rates of cooling are observed between torpor bouts occurring during the test drop and post-test drop stages at the ambient temperatures of 8°, 6° and 3°C but not at 16°, 13° and 10°C. The hibernation period of Eutamias minimus is composed of a pre-plateau and plateau stage. Eutamias minimus displays significantly longer durations of torpor during the plateau stage than Tamias striatus. Eutamias minimus displays a control on the rate of cooling during the entry into a torpor bout. Eutamias minimus is a “deeper” hibernator than Tamias striatus, i.e. less prone to arousals.},
	language = {en},
	number = {3},
	journal = {Comparative biochemistry and physiology. A, Comparative physiology},
	author = {Pivorun, E B},
	year = {1976},
	pages = {265--271}
}

@misc{underwood_brief_2012,
	title = {A brief outburst about numbers.},
	url = {http://tedunderwood.wordpress.com/2012/01/03/a-brief-outburst-about-numbers/},
	abstract = {In responding to Stanley Fish last week, I tried to acknowledge that the "digital humanities," in spite of their name, are not centrally about numbers. The movement is very broad, and at the broade...},
	language = {en},
	urldate = {2012-01-05},
	journal = {The Stone and the Shell},
	author = {Underwood, Ted},
	month = jan,
	year = {2012}
}

@book{siemens_companion_2004,
	address = {Oxford},
	edition = {Hardcover},
	series = {Blackwell {Companions} to {Literature} and {Culture}},
	title = {A {Companion} to {Digital} {Humanities}},
	isbn = {978-1-4051-0321-3},
	url = {http://www.digitalhumanities.org/companion/},
	abstract = {This Companion offers a thorough, concise overview of the emerging field of humanities computing. Contains 37 original articles written by leaders in the field. Addresses the central concerns shared by those interested in the subject. Major sections focus on the experience of particular disciplines in applying computational methods to research problems; the basic principles of humanities computing; specific applications and methods; and production, dissemination and archiving. Accompanied by a website featuring supplementary materials, standard readings in the field and essays to be included in future editions of the Companion.},
	language = {en},
	urldate = {2010-05-17},
	publisher = {Blackwell Publishing Professional},
	editor = {Siemens, Ray and Unsworth, John and Schreibman, Susan},
	year = {2004},
	keywords = {meta\_GiveOverview, *****, act\_Publishing, meta\_Theorizing, t\_Encoding, X-CHECK}
}

@article{jockers_comparative_2010,
	title = {A comparative study of machine learning methods for authorship attribution},
	volume = {25},
	url = {http://llc.oxfordjournals.org/content/25/2/215.abstract},
	doi = {10.1093/llc/fqq001},
	abstract = {We compare and benchmark the performance of five classification methods, four of which are taken from the machine learning literature, in a classic authorship attribution problem involving the Federalist Papers. Cross-validation results are reported for each method, and each method is further employed in classifying the disputed papers and the few papers that are generally understood to be coauthored. These tests are performed using two separate feature sets: a “raw” feature set containing all words and word bigrams that are common to all of the authors, and a second “pre-processed” feature set derived by reducing the raw feature set to include only words meeting a minimum relative frequency threshold. Each of the methods tested performed well, but nearest shrunken centroids and regularized discriminant analysis had the best overall performances with 0/70 cross-validation errors.},
	language = {en},
	number = {2},
	urldate = {2011-12-14},
	journal = {Literary and Linguistic Computing},
	author = {Jockers, Matthew L. and Witten, Daniela M.},
	month = jun,
	year = {2010},
	keywords = {AnalyzeStatistically, meta\_Theorizing, bigdata, t\_MachineLearning, t\_Stylometry},
	pages = {215--223}
}

@inproceedings{yang_comparative_1997,
	title = {A {Comparative} {Study} on {Feature} {Selection} in {Text} {Categorization}},
	abstract = {This paper is a comparative study of feature selection methods in statistical learning of text categorization. The focus is on aggressive dimensionality reduction. Five methods were evaluated, including term selection based on document frequency (DF), information gain (IG), mutual information (MI), a  Ø  2  -test (CHI), and term strength (TS). We found IG and CHI most effective in our experiments. Using IG thresholding with a knearest neighbor classifier on the Reuters corpus, removal of up to 98\% removal of unique terms actually yielded an improved classification accuracy (measured by average precision) . DF thresholding performed similarly. Indeed we found strong correlations between the DF, IG and CHI values of a term. This suggests that DF thresholding, the simplest method with the lowest cost in computation, can be reliably used instead of IG or CHI when the computation of these measures are too expensive. TS compares favorably with the other methods with up to 50\% vocabulary redu...},
	publisher = {Morgan Kaufmann Publishers},
	author = {Yang, Yiming and Pedersen, Jan O.},
	year = {1997},
	keywords = {AnalyzeStatistically, bigdata, t\_MachineLearning},
	pages = {412--420}
}

@inproceedings{burgoyne_comparative_2007,
	address = {Vienna, Austria},
	title = {A {Comparative} {Survey} of {Image} {Binarisation} {Algorithms} for {Optical} {Recognition} on {Degraded} {Musical} {Sources}},
	volume = {509-12},
	url = {http://www.aruspix.net/publications/burgoyne07comparative.pdf},
	abstract = {Binarisation of greyscale images is a critical step in optic
al music recognition (OMR) preprocessing. Binarising mu-
sic documents is particularly challenging because of the
nature of music notation, even more so when the sources
are degraded, e.g., with ink bleed-through from the other
side of the page. This paper presents a comparative eval-
uation of 25 binarisation algorithms tested on a set of 100
music pages. A real-world OMR infrastructure for early
music (Aruspix) was used to perform an objective, goal-
directed evaluation of the algorithms’ performance. Our
results differ significantly from the ones obtained in stud-
ies on non-music documents, which highlights the impor-
tance of developing tools specific to our community.},
	language = {en},
	booktitle = {Proceedings of the 8th {International} {Conference} on {Music} {Information} {Retrieval} ({ISMIR} 2007)},
	author = {Burgoyne, John Ashley and Pugin, Laurent and Eustace, Greg and Fujinaga, Ichiro},
	year = {2007},
	keywords = {bigdata, act\_DataRecognition, obj\_SheetMusic}
}

@inproceedings{vicknair_comparison_2010,
	title = {A comparison of a graph database and a relational database},
	isbn = {978-1-4503-0064-3},
	url = {http://noduslabs.com/research/pathways-meaning-circulation-text-network-analysis/},
	doi = {10.1145/1900008.1900067},
	abstract = {In this work we propose a method and algorithm for identifying the pathways for meaning circulation within a text. This is done by visualizing normalized textual data as a graph and deriving the key metrics for the concepts and for the text as a whole using network analysis. The resulting data and graph representation are then used to detect the key concepts, which function as junctions for meaning circulation within a text, contextual clusters comprised of word communities (themes), as well as the most often used pathways for meaning circulation. We then discuss several practical applications of our method ranging from automatic recovery of hidden agendas within a text and intertextual navigation graph-interfaces, to enhancing reading and writing, quick text summarization, as well as group sentiment profiling and text diagramming. We also make a quick overview of the existing computer-assisted text analysis (and, specifically, network text analysis), and text visualization methods in order to position our research in relation to the other available approaches.},
	language = {en},
	urldate = {2013-05-12},
	publisher = {ACM Press},
	author = {Vicknair, Chad and Macias, Michael and Zhao, Zhendong and Nan, Xiaofei and Chen, Yixin and Wilkins, Dawn},
	year = {2010},
	pages = {1}
}

@article{evans_computational_2014,
	title = {A {Computational} {Approach} to {Qualitative} {Analysis} in {Large} {Textual} {Datasets}},
	volume = {9},
	url = {http://dx.doi.org/10.1371/journal.pone.0087908},
	doi = {10.1371/journal.pone.0087908},
	abstract = {In this paper I introduce computational techniques to extend qualitative analysis into the study of large textual datasets. I demonstrate these techniques by using probabilistic topic modeling to analyze a broad sample of 14,952 documents published in major American newspapers from 1980 through 2012. I show how computational data mining techniques can identify and evaluate the significance of qualitatively distinct subjects of discussion across a wide range of public discourse. I also show how examining large textual datasets with computational methods can overcome methodological limitations of conventional qualitative methods, such as how to measure the impact of particular cases on broader discourse, how to validate substantive inferences from small samples of textual data, and how to determine if identified cases are part of a consistent temporal pattern.},
	language = {en},
	number = {2},
	urldate = {2014-02-04},
	journal = {PLoS ONE},
	author = {Evans, Michael S.},
	month = feb,
	year = {2014},
	keywords = {AnalyzeStatistically, bigdata},
	file = {Evans_2014_A Computational Approach to Qualitative Analysis in Large Textual Datasets.pdf:/home/lisnux/Zotero/storage/U4KUQEJJ/Evans_2014_A Computational Approach to Qualitative Analysis in Large Textual Datasets.pdf:application/pdf}
}

@article{benardou_conceptual_2010,
	title = {A conceptual model for scholarly research activity},
	journal = {iConference 2010 Proceedings},
	author = {Benardou, Agiatis and Constantopoulos, Panos and Dallas, Costis and Gavrilis, Dimitris},
	year = {2010},
	keywords = {meta\_Assessing, obj\_DigitalHumanities, obj\_Research},
	pages = {26--32}
}

@misc{gibbs_conversation_2011,
	title = {A {Conversation} with {Data}: {Prospecting} {Victorian} {Words} and {Ideas} [from: {Victorian} {Studies}, 54.1, 2011, 69-77, 10.1353/vic.2011.0146]},
	url = {http://www.dancohen.org/2012/05/30/a-conversation-with-data-prospecting-victorian-words-and-ideas/},
	language = {en},
	journal = {Dancohen.org},
	author = {Gibbs, Frederick W. and Cohen, Daniel J.},
	year = {2011},
	keywords = {X-CHECK}
}

@article{vis_critical_2013,
	title = {A critical reflection on {Big} {Data}: {Considering} {APIs}, researchers and tools as data makers},
	volume = {18},
	copyright = {Authors submitting a paper to First Monday automatically agree to confer a limited license to First Monday if and when the manuscript is accepted for publication. This license allows First Monday to publish a manuscript in a given issue. Authors have a choice of: 1. Dedicating the article to the public domain. This allows anyone to make any use of the article at any time, including commercial use. A good way to do this is to use the Creative Commons Public Domain Dedication Web form; see  http://creativecommons.org/license/publicdomain-2?lang=en . 2. Retaining some rights while allowing some use. For example, authors may decide to disallow commercial use without permission. Authors may also decide whether to allow users to make modifications (e.g. translations, adaptations) without permission. A good way to make these choices is to use a Creative Commons license. * Go to  http://creativecommons.org/license/ . * Choose and select a license. * What to do next — you can then e–mail the license html code to yourself. Do this, and then forward that e–mail to First Monday’s editors. Put your name in the subject line of the e–mail with your name and article title in the e–mail. Background information about Creative Commons licenses can be found at  http://creativecommons.org/about/licenses/ . 3. Retaining full rights, including translation and reproduction rights. Authors may use the statement: © Author 2008 All Rights Reserved. Authors may choose to use their own wording to reserve copyright. If you choose to retain full copyright, please add your copyright statement to the end of the article. Authors submitting a paper to First Monday do so in the understanding that Internet publishing is both an opportunity and challenge. In this environment, authors and publishers do not always have the means to protect against unauthorized copying or editing of copyright–protected works.},
	issn = {13960466},
	shorttitle = {A critical reflection on {Big} {Data}},
	url = {http://firstmonday.org/ojs/index.php/fm/article/view/4878},
	doi = {10.5210/fm.v18i10.4878},
	abstract = {This paper looks at how data is ‘made’, by whom and how. Rather than assuming data already exists ‘out there’, waiting to simply be recovered and turned into findings, the paper examines how data is co–produced through dynamic research intersections. A particular focus is the intersections between the application programming interface (API), the researcher collecting the data as well as the tools used to process it. In light of this, this paper offers three new ways to define and think about Big Data and proposes a series of practical suggestions for making data.},
	language = {en},
	number = {10},
	urldate = {2014-01-28},
	journal = {First Monday},
	author = {Vis, Farida},
	month = oct,
	year = {2013},
	keywords = {bigdata}
}

@phdthesis{simonis_framework_2004,
	type = {Thesis ({PhD} level)},
	title = {A framework for processing and presenting parallel text corpora},
	url = {https://publikationen.uni-tuebingen.de/xmlui/handle/10900/48620},
	abstract = {Diese Arbeit stellt ein erweiterbares System für die Bearbeitung und Präsentation von multi-modalen, parallelen Textkorpora vor. Es kann dazu verwendet werden um digitale Dokumente in vielerlei Formaten wie zum Beispiel einfache Textdateien, XML-Dateien oder Graphiken zu bearbeiten wobei bearbeiten in diesem Zusammenhang vor allem strukturieren und verlinken bedeutet. Diese Strukturierung nach einem neu entwickelten Kodierungschema kann zum Beispiel auf formalen, linguistischen, semantischen, historischen oder auch vielen anderen Gesichtspunkten beruhen. Die Dokumente können gleichzeitig mit beliebig vielen parallelen und sich möglicherweise auch überlappenden Strukturen versehen werden und bezüglich jeder dieser Strukturen auch miteinander verknüpft werden. Die unterschiedlichen Strukturen können je nach Art entweder automatisch oder halbautomatisch erzeugt werden oder sie können vom Benutzer manuell spezifiziert werden. Als Grundlage des vorgestellten Systems dient XTE, ein einfaches aber zugleich mächtiges, externe Kodierungsschema das sowohl als eine XML DTD als auch als ein XML Schema verwirklicht wurde. XTE ist besonders zum Kodieren von vielen, sich gegenseitig überlappenden Hierarchien in multi-modalen Dokumenten und zum Verknüpfen dieser Strukturen über mehrere Dokumente hinweg, geeignet. Zusammen mit XTE wurden zwei ausgereifte Anwendungen zum Betrachten und Bearbeiten von XTE-kodierten Dokumenten sowie zum komfortablen Arbeiten mit den so erstellten Ergebnisdokumenten geschaffen. Diese Anwendungen wurden als anpassbares und erweiterbares System konzipiert, das möglichst einfach für andere Einsatzgebiete und an neue Benutzerwünsche angepasst werden können soll. Die Kombination einer klassischen Synopse zusammen mit den vorhandenen Erweiterungsmöglichkeiten mittels Wörterbüchern, Lexika und Multi-Media Elementen die das System bietet, machen es zu einem Werkzeug das auf vielen Gebieten, angefangen von der Text-Analyse und dem Sprachenlernen über die Erstellung textkritischer Editionen bis hin zum elektronischen Publizieren, einsetzbar ist. Neben diesem System sind als weitere Ergebnisse dieser Arbeit verschiedene Werkzeuge für die Softwaredokumentation entstanden und zur Dokumentation des Systems eingesetzt worden. Weiterhin wurde eine neuartige, mehrsprachige, graphische Benutzeroberfläche entwickelt, die unter anderem in dem hier beschriebenen System eingesetz wurde.},
	language = {en},
	school = {Tübingen},
	author = {Simonis, Volker},
	collaborator = {Güntzer, Ulrich and Loos, Rüdiger},
	year = {2004},
	note = {Online-Ressource
Tübingen, Univ., Diss, 2004},
	keywords = {AnalyzeStatistically, bigdata{\textasciitilde}, goal\_Dissemination, t\_XML}
}

@inproceedings{li_framework_2009,
	address = {Stroudsburg, PA, USA},
	series = {{ACL} '09},
	title = {A framework of feature selection methods for text categorization},
	isbn = {978-1-932432-46-6},
	url = {http://dl.acm.org/citation.cfm?id=1690219.1690243},
	abstract = {In text categorization, feature selection (FS) is a strategy that aims at making text classifiers more efficient and accurate. However, when dealing with a new task, it is still difficult to quickly select a suitable one from various FS methods provided by many previous studies. In this paper, we propose a theoretic framework of FS methods based on two basic measurements: frequency measurement and ratio measurement. Then six popular FS methods are in detail discussed under this framework. Moreover, with the guidance of our theoretical analysis, we propose a novel method called weighed frequency and odds (WFO) that combines the two measurements with trained weights. The experimental results on data sets from both topic-based and sentiment classification tasks show that this new method is robust across different tasks and numbers of selected features.},
	language = {en},
	urldate = {2012-12-03},
	booktitle = {Proceedings of the {Joint} {Conference} of the 47th {Annual} {Meeting} of the {ACL} and the 4th {International} {Joint} {Conference} on {Natural} {Language} {Processing} of the {AFNLP}: {Volume} 2 - {Volume} 2},
	publisher = {Association for Computational Linguistics},
	author = {Li, Shoushan and Xia, Rui and Zong, Chengqing and Huang, Chu-Ren},
	year = {2009},
	keywords = {X-CHECK, bigdata},
	pages = {692--700}
}

@article{dalbello_genealogy_2011,
	title = {A genealogy of digital humanities},
	volume = {67},
	issn = {0022-0418},
	url = {http://www.emeraldinsight.com/10.1108/00220411111124550},
	doi = {10.1108/00220411111124550},
	language = {en},
	number = {3},
	urldate = {2011-08-31},
	journal = {Journal of Documentation},
	author = {Dalbello, Marija},
	year = {2011},
	keywords = {meta\_GiveOverview, meta\_Assessing, obj\_DigitalHumanities},
	pages = {480--506}
}

@article{underwood_genealogy_2017,
	title = {A {Genealogy} of {Distant} {Reading}},
	volume = {11},
	url = {http://www.digitalhumanities.org/dhq/vol/11/2/000317/000317.html},
	abstract = {It has recently become common to describe all empirical approaches to literature as subfields of digital humanities. This essay argues that distant reading has a largely distinct genealogy stretching back many decades before the advent of the internet – a genealogy that is not for the most part centrally concerned with computers. It would be better to understand this field as a conversation between literary studies and social science, inititated by scholars like Raymond Williams and Janice Radway, and moving slowly toward an explicitly experimental method. Candor about the social-scientific dimension of distant reading is needed now, in order to refocus a research agenda that can drift into diffuse exploration of digital tools. Clarity on this topic might also reduce miscommunication between distant readers and digital humanists.},
	language = {en},
	number = {2},
	urldate = {2017-10-20},
	journal = {DHQ},
	author = {Underwood, Ted},
	year = {2017},
	keywords = {act\_Conceptualizing, obj\_Methods}
}

@misc{williamson_hidden_nodate,
	title = {A hidden computing curriculum? {How} 'learning to code' campaigns and edtech industry helped shape school policy},
	shorttitle = {A hidden computing curriculum?},
	url = {http://codeactsineducation.wordpress.com/2014/09/05/hidden-computing-curriculum/},
	abstract = {By Ben Williamson In the last few years, the idea of ‘learning to code’ and ‘digital making’ has grown from a minority focus among computing educators, grassroots computing organizations, and compu...},
	urldate = {2014-09-05},
	journal = {code acts in education},
	author = {Williamson, Ben},
	keywords = {meta\_Assessing, act\_Teaching/Learning, obj\_Code}
}

@article{tabak_hybrid_2017,
	title = {A {Hybrid} {Model} for {Managing} {DH} {Projects}},
	volume = {11},
	number = {1},
	journal = {Digital Humanities Quarterly},
	author = {Tabak, Edin},
	year = {2017},
	keywords = {meta\_ProjectManagement}
}

@misc{tanner_new_2012,
	title = {A {New} {Approach} to {Measuring} {Impact} for {Digitised} {Resources}: do they change people’s lives?},
	url = {http://simon-tanner.blogspot.co.uk/2012/03/new-approach-to-measuring-impact-for.html},
	abstract = {This is a work in progress - more my notes and queries than a proper paper, stuff will change, references will be added. I wanted most to get this out there and to get your views, your inputs and your insights. Please comment, your thoughts are valued!},
	language = {en},
	journal = {When the Data hits the Fan!},
	author = {Tanner, Simon},
	month = mar,
	year = {2012}
}

@article{baroni_new_2006,
	title = {A {New} {Approach} to the {Study} of {Translationese}: {Machine}-learning the {Difference} between {Original} and {Translated} {Text}},
	volume = {21},
	issn = {0268-1145, 1477-4615},
	shorttitle = {A {New} {Approach} to the {Study} of {Translationese}},
	url = {http://llc.oxfordjournals.org/content/21/3/259},
	doi = {10.1093/llc/fqi039},
	abstract = {In this article we describe an approach to the identification of ‘translationese’ based on monolingual comparable corpora and machine learning techniques for text categorization. The article reports on experiments in which support vector machines (SVMs) are employed to recognize translated text in a corpus of Italian articles from the geopolitical domain. An ensemble of SVMs reaches 86.7\% accuracy with 89.3\% precision and 83.3\% recall on this task. A preliminary analysis of the features used by the SVMs suggests that the distribution of function words and morphosyntactic categories in general, and personal pronouns and adverbs in particular, are among the cues used by the SVMs to perform the discrimination task. A follow-up experiment shows that the performance attained by SVMs is well above the average performance of ten human subjects, including five professional translators, on the same task. Our results offer solid evidence supporting the translationese hypothesis, and our method seems to have promising applications in translation studies and in quantitative style analysis in general. Implications for the machine learning/text categorization community are equally important, both because this is a novel application and especially because we provide explicit evidence that a relatively knowledge-poor machine learning algorithm can outperform human beings in a text classification task.},
	language = {en},
	number = {3},
	urldate = {2013-03-19},
	journal = {Literary and Linguistic Computing},
	author = {Baroni, Marco and Bernardini, Silvia},
	month = sep,
	year = {2006},
	keywords = {bigdata, t\_MachineLearning},
	pages = {259--274},
	file = {Baroni_Bernardini_2006_A New Approach to the Study of Translationese.pdf:/home/lisnux/Zotero/storage/7XQW3F3K/Baroni_Bernardini_2006_A New Approach to the Study of Translationese.pdf:application/pdf}
}

@article{siemens_new_2002,
	title = {A {New} {Computer}-{Assisted} {Literary} {Criticism}?},
	volume = {36},
	issn = {0010-4817},
	url = {http://www.jstor.org/stable/30200526},
	abstract = {If there is such a thing as a new computer-assisted literary criticism, its expression lies in a model that is as broad-based as that presented in John Smith's seminal article, "Computer Criticism," and is as encompassing of the discipline of literary studies as it is tied to the evolving nature of the electronic literary text that lies at the heart of its intersection with computing. It is the desire to establish the parameters of such a model for the interaction between literary studies and humanities computing - for a model of the new computer-assisted literary criticism - that gave rise to the papers in this collection and to the several conference panel-presentations and discussions that, in their print form, these papers represent.},
	language = {en},
	number = {3},
	urldate = {2011-10-11},
	journal = {Computers and the Humanities},
	author = {Siemens, Raymond G.},
	year = {2002},
	note = {ArticleType: research-article / Issue Title: A New Computer-Assisted Literary Criticism? / Full publication date: Aug., 2002 / Copyright © 2002 Springer},
	keywords = {meta\_Assessing, obj\_DigitalHumanities, goal\_Interpretation, obj\_Literature},
	pages = {259--267}
}

@book{bod_new_2013,
	address = {Oxford},
	title = {A new history of the humanities: the search for principles and patterns from {Antiquity} to the present},
	isbn = {978-0-19-966521-1},
	shorttitle = {A new history of the humanities},
	abstract = {Many histories of science have been written, but A New History of the Humanities offers the first overarching history of the humanities from Antiquity to the present. There are already historical studies of musicology, logic, art history, linguistics, and historiography, but this volume gathers these, and many other humanities disciplines, into a single coherent account.

Its central theme is the way in which scholars throughout the ages and in virtually all civilizations have sought to identify patterns in texts, art, music, languages, literature, and the past. What rules can we apply if we wish to determine whether a tale about the past is trustworthy? By what criteria are we to distinguish consonant from dissonant musical intervals? What rules jointly describe all possible grammatical sentences in a language? How can modern digital methods enhance pattern-seeking in the humanities? Rens Bod contends that the hallowed opposition between the sciences (mathematical, experimental, dominated by universal laws) and the humanities (allegedly concerned with unique events and hermeneutic methods) is a mistake born of a myopic failure to appreciate the pattern-seeking that lies at the heart of this inquiry. A New History of the Humanities amounts to a persuasive plea to give Panini, Valla, Bopp, and countless other often overlooked intellectual giants their rightful place next to the likes of Galileo, Newton, and Einstein.},
	language = {en},
	publisher = {Oxford Univ. Press},
	author = {Bod, Rens},
	year = {2013},
	keywords = {meta\_Assessing, obj\_DigitalHumanities, bigdata{\textasciitilde}}
}

@inproceedings{qi_new_2011,
	title = {A new method for visual stylometry on impressionist paintings},
	doi = {10.1109/ICASSP.2011.5946912},
	abstract = {A new emerging field, that of visual stylometry of art, proposes to apply image analysis and machine learning tools to high-resolution digital images of artwork in order to assist art connoisseurs in determining the painting's likely creator. The premise is that each artist's brushwork is likely to contain features that are characteristic of the artist's unique habitual physical movements; these features could be identified and characterized through machine learning. In this paper, we describe a new technique for this problem. We extract, as features for our classifier, parameters of both Hidden Markov Tree models and linear predictor models of the painting's wavelet coefficients. We then use the FINE dimensionality reduction technique [1] to produce an unsupervised low-dimensional embedding of the data. Tests on two dataset consisting of over 100 high-resolution digital images of impressionist paintings by Van Gogh and contemporaries shows good separation between paintings of Van Gogh and others is achieved via this unsupervised process. We further show (through comparison with the alternative) that our method benefits greatly from (1) using only background sections of each painting in our analysis, (2) the FINE technique, and (3) the use of both HMT and linear predictor features together in the same analysis. All three of these technique choices are new in this paper. We hope that our method can be a tool, used carefully in conjunction with the connoisseur's expertise and other examinations, to determine a painting's true authorship.},
	booktitle = {2011 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Qi, Hanchao and Hughes, S.},
	month = may,
	year = {2011},
	keywords = {act\_StylisticAnalysis, goal\_Analysis, obj\_Music},
	pages = {2036--2039}
}

@book{mcgann_new_2014,
	address = {Boston, MA},
	title = {A {New} {Republic} of {Letters}. {Memory} and {Scholarship} in the {Age} of {Digital} {Reproduction}},
	url = {http://www.hup.harvard.edu/catalog.php?isbn=9780674728691},
	abstract = {Jerome McGann's manifesto argues that the history of texts and how they are preserved and accessed for interpretation are the overriding subjects of humanist study in the digital age. Theory and philosophy no longer suffice as an intellectual framework. But philology -- out of fashion for decades -- models these concerns with surprising fidelity.},
	language = {en},
	urldate = {2014-03-30},
	publisher = {Harvard Univ. Press},
	author = {McGann, Jerome},
	year = {2014},
	keywords = {meta\_GiveOverview, meta\_Assessing, obj\_DigitalHumanities, bigdata{\textasciitilde}, obj\_Text}
}

@article{battersby_new_2015,
	title = {A new {Shakespeare} play has just been discovered},
	url = {http://www.independent.co.uk/arts-entertainment/theatre-dance/news/fake-shakespeare-play-double-falsehood-is-genuine-after-all-10167657.html},
	abstract = {A lost play once claimed to be by Shakespeare but subsequently poo-pooed as a forgery, is now “strongly” believed to be genuine according to new research.},
	language = {en},
	urldate = {2015-04-11},
	journal = {The Independent},
	author = {Battersby, Matilda},
	month = apr,
	year = {2015},
	keywords = {act\_StylisticAnalysis, goal\_Analysis, obj\_Text}
}

@article{youmans_new_1991,
	title = {A {New} {Tool} for {Discourse} {Analysis}: {The} {Vocabulary}-{Management} {Profile}},
	volume = {67},
	copyright = {Copyright © 1991 Linguistic Society of America},
	issn = {00978507},
	url = {http://www.jstor.org/stable/415076},
	abstract = {A computer is used to count the number of new vocabulary words introduced into a text over a moving interval thirty-five words long. The number of new words in each successive interval is plotted at the midpoint of the interval, generating a curve called the Vocabulary-Management Profile (VMP). Analysis of VMPs for passages from James Joyce and George Orwell illustrates that clearcut peaks and valleys on VMPs correlate closely with constituent boundaries and information flow in discourse. VMPs for these authors show surprisingly regular alternations between peaks and valleys (that is, between new and repeated vocabulary), reflecting two competing principles that necessarily underlie all normal discourse: innovation and coherence.},
	language = {en},
	number = {4},
	journal = {Language},
	author = {Youmans, Gilbert},
	year = {1991},
	keywords = {AnalyzeQualitatively, AnalyzeStatistically, t\_DiscourseAnalysis},
	pages = {pp. 763--789}
}

@inproceedings{zhang_novel_2009,
	address = {Stroudsburg, PA, USA},
	series = {People's {Web} '09},
	title = {A novel approach to automatic gazetteer generation using {Wikipedia}},
	isbn = {978-1-932432-55-8},
	url = {http://dl.acm.org/citation.cfm?id=1699765.1699766},
	abstract = {Gazetteers or entity dictionaries are important knowledge resources for solving a wide range of NLP problems, such as entity extraction. We introduce a novel method to automatically generate gazetteers from seed lists using an external knowledge resource, the Wikipedia. Unlike previous methods, our method exploits the rich content and various structural elements of Wikipedia, and does not rely on language- or domain-specific knowledge. Furthermore, applying the extended gazetteers to an entity extraction task in a scientific domain, we empirically observed a significant improvement in system accuracy when compared with those using seed gazetteers.},
	language = {en},
	urldate = {2013-05-06},
	booktitle = {Proceedings of the 2009 {Workshop} on {The} {People}'s {Web} {Meets} {NLP}: {Collaboratively} {Constructed} {Semantic} {Resources}},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Ziqi and Iria, José},
	year = {2009},
	keywords = {bigdata, t\_NamedEntityRecognition},
	pages = {1--9}
}

@misc{lieberman_aiden_picture_2011,
	title = {A {Picture} is {Worth} 500 {Billion} {Words}},
	url = {http://tedxtalks.ted.com/video/TEDxBoston-Erez-Lieberman-Aid-2},
	language = {en},
	collaborator = {Lieberman Aiden, Erez and Michel, Jean-Baptiste},
	month = sep,
	year = {2011},
	keywords = {AnalyzeStatistically, obj\_Tools}
}

@article{nicholas_policy_2009,
	title = {A {Policy} {Checklist} for {Enabling} {Persistence} of {Identifiers}},
	volume = {15},
	issn = {1082-9873},
	url = {http://www.dlib.org/dlib/january09/nicholas/01nicholas.html},
	doi = {10.1045/january2009-nicholas},
	abstract = {One of the main tasks of the Persistent Identifier Linking Infrastructure (PILIN) project on persistent identifiers was to establish a policy framework for managing identifiers and identifier providers. A major finding from the project was that policy is far more important in guaranteeing persistence of identifiers than technology. Key policy questions for guaranteeing identifier persistence include: what entities should be assigned persistent identifiers, how should those identifiers be exposed to services, and what guarantees does the provider make on how long various facets of the identifiers will persist.

To make an informed decision about what to identify, information modelling of the domain is critical. Identifier managers need to know what can be identified discretely (including not only concrete artefacts like files, but also abstractions such as works, versions, presentations, and aggregations); and for which of those objects it is a priority for users and managers to keep track. Without working out what actually needs to be identified, the commitment to keep identifiers persistent becomes meaningless.

To make sure persistent identifiers meet these requirements, the PILIN project has formulated a six-point checklist for integrating identifiers into information management, which we present here.},
	language = {en},
	number = {1/2},
	urldate = {2013-02-01},
	journal = {D-Lib Magazine},
	author = {Nicholas, Nick and Ward, Nigel and Blinco, Kerry},
	month = jan,
	year = {2009},
	keywords = {obj\_Research, act\_Identifying}
}

@article{juola_prototype_2006,
	title = {A {Prototype} for {Authorship} {Attribution} {Studies}},
	volume = {21},
	url = {http://llc.oxfordjournals.org/content/21/2/169.abstract},
	doi = {10.1093/llc/fql019},
	abstract = {Despite a century of research, statistical and computational methods for authorship attribution are neither reliable, well-regarded, widely used, or well-understood. This article presents a survey of the current state of the art as well as a framework for uniform and unified development of a tool to apply the state of the art, despite the wide variety of methods and techniques used. The usefulness of the framework is confirmed by the development of a tool using that framework that can be applied to authorship analysis by researchers without a computing specialization. Using this tool, it may be possible both to expand the pool of available researchers as well as to enhance the quality of the overall solutions [for example, by incorporating improved algorithms as discovered through empirical analysis (Juola, P. (2004a). Ad-hoc Authorship Attribution Competition. In Proceedings 2004 Joint International Conference of the Association for Literary and Linguistic Computing and the Association for Computers and the Humanities (ALLC/ACH 2004), Göteborg, Sweden)].},
	language = {en},
	number = {2},
	urldate = {2011-12-14},
	journal = {Literary and Linguistic Computing},
	author = {Juola, Patrick and Sofko, John and Brennan, Patrick},
	month = jun,
	year = {2006},
	keywords = {AnalyzeStatistically, meta\_Theorizing, t\_Stylometry},
	pages = {169 --178},
	file = {Juola et al_2006_A Prototype for Authorship Attribution Studies.pdf:/home/lisnux/Zotero/storage/DG2TXQZS/Juola et al_2006_A Prototype for Authorship Attribution Studies.pdf:application/pdf}
}

@techreport{heuser_quantitative_2012,
	address = {Standford CA},
	title = {A {Quantitative} {Literary} {History} of 2,958 {Nineteenth}-{Century} {British} {Novels}: {The} {Semantic} {Cohort} {Method}},
	url = {http://litlab.stanford.edu/LiteraryLabPamphlet4.pdf},
	abstract = {The nineteenth century in Britain saw tumultuous changes that reshaped the fabric of society and altered the course of modernization. It also saw the rise of the novel to the height of its cultural power as the most important literary form of the period. This paper reports on a long-term experiment in tracing such macroscopic changes in the novel during this crucial period. Specifically, we present findings on two interrelated transformations in novelistic language that reveal a systemic concretization in language and fundamental change in the social spaces of the novel. We show how these shifts have consequences for setting, characterization, and narration as well as implications for the responsiveness of the novel to the dramatic changes in British society.

This paper has a second strand as well. This project was simultaneously an experiment in developing quantitative and computational methods for tracing changes in literary language. We wanted to see how far quantifiable features such as word usage could be pushed toward the investigation of literary history. Could we leverage quantitative methods in ways that respect the nuance and complexity we value in the humanities? To this end, we present a second set of results, the techniques and methodological lessons gained in the course of designing and running this project.},
	language = {en},
	institution = {Literary Lab, Stanford University},
	author = {Heuser, Ryan and Le-Khac, Long},
	month = may,
	year = {2012},
	keywords = {bigdata}
}

@article{carroll_rationale_2013,
	title = {A rationale for evolutionary studies of literature},
	volume = {3},
	url = {http://www.jbe-platform.com/content/journals/10.1075/ssol.3.1.03car},
	doi = {10.1075/ssol.3.1.03car},
	abstract = {I identify converging lines of evidence for the proposition that the human mind has evolved, argue that the evolved character of the mind influences the products of the mind, including literature, and conclude that scholarly and scientific commentary on literature would benefit from being explicitly lodged within an evolutionary conceptual framework. I argue that a biocultural perspective has comprehensive scope and can encompass all the topics to which other schools of literary theory give attention. To support this contention, I appeal to axiomatic logic: the behavior of any organism is a result of interactions between its genetically determined characteristics and its environmental influences. Summarizing the debate over the adaptive function of literature, I argue that literature and its oral antecedents are adaptations, not merely by-products of adaptations.},
	language = {en},
	number = {1},
	journal = {Scientific Study of Literature},
	author = {Carroll, Joseph},
	year = {2013},
	pages = {8--15}
}

@article{merriman_science_2015,
	title = {A {Science} of {Literature}},
	url = {http://bostonreview.net/books-ideas/ben-merriman-moretti-jockers-digital-humanities},
	language = {eng},
	journal = {Boston Review},
	author = {Merriman, Ben},
	year = {2015}
}

@article{burrows_second_2012,
	title = {A {Second} {Opinion} on '{Shakespeare} and {Authorship} {Studies} in the {Twenty}-{First} {Century}},
	volume = {63},
	url = {http://muse.jhu.edu/login?auth=0&type=summary&url=/journals/shakespeare_quarterly/v063/63.3.burrows.html},
	abstract = {In a review essay entitled “Shakespeare and Authorship Studies in the Twenty-First Century,” published in Shakespeare Quarterly (62 [2011]: 106–42), Brian Vickers makes a scathing attack upon computational stylistics as a scholarly enterprise and as represented by the book he is reviewing. His principal target is the use of word-frequency patterns as evidence of authorship. He argues that his current approach to the attribution of authorship, an approach that deals in collocations of words, is effective and superior. The book in question, Shakespeare, Computers, and the Mystery of Authorship, edited by Hugh Craig and Arthur Kinney (2009) is best assessed by scholars deeply versed in Shakespeare studies; the consensus may be far more favorable than Vickers expects. The main purpose of this “Second Opinion” is neither to defend Craig and Kinney nor to offer a general survey of the field, but to show that such methods are well suited to the work of attribution. The present essay concludes that, when Vickers overcomes several shortcomings in his own new method, it may well become a useful addition to the scholar’s armory. The reason all this matters is that new methods of analysis are yielding very accurate results. As is sometimes the case with advanced bibliographical methods and the close study of printing-house practice, apparent technical complexity is a barrier to their acceptance. But all of these methods should be judged by their results.},
	language = {en},
	number = {3},
	journal = {Shakespeare Quarterly},
	author = {Burrows, John F.},
	year = {2012},
	keywords = {X-CHECK},
	pages = {355--92}
}

@misc{riddell_simple_2012,
	title = {A {Simple} {Topic} {Model} ({Mixture} of {Unigrams})},
	url = {http://ariddell.org/weblog/2012/07/22/simple-topic-model/},
	abstract = {NB: This is an extended version of the appendix of my paper exploring trends in German Studies in the US between 1928 and 2006. In that paper I used a topic model (Latent Dirichlet Allocation); this tutorial is intended to help readers understand how LDA works.},
	language = {en},
	journal = {ariddell.org},
	author = {Riddell, Allen Beye},
	month = jul,
	year = {2012},
	keywords = {goal\_Analysis, act\_ContentAnalysis, t\_TopicModeling}
}

@book{ellegard_statistical_1962,
	address = {Göteborg},
	title = {A statistical method for determining authorship: the {Junius} letters, 1769-1772.},
	shorttitle = {A statistical method for determining authorship},
	language = {en},
	author = {Ellegård, Alvar},
	year = {1962},
	keywords = {AnalyzeStatistically, t\_Stylometry}
}

@techreport{van_der_graaf_surfboard_2011,
	title = {A {Surfboard} for {Riding} the {Wave}. {Towards} a four country action programme on research data},
	url = {A Surfboard for Riding the Wave. Towards a four country action programme on research data},
	abstract = {This paper presents an overview of the present situation with regard to research data in Denmark, Germany, the Netherlands and the United Kingdom and offers broad outlines for a possible action programme for the four countries in realising the envisaged collaborative data infrastructure. An action programme at the level of four countries needs the involvement of all stakeholders from the scientific community. We
identified four key drivers: incentives; training in relation to researchers in their role as data producers and users of information infrastructures; infrastructure; funding of the infrastructure in relation to further developments in data logistics.},
	language = {en},
	institution = {KE Knowledge Exchange},
	author = {van der Graaf, Maurits and Waaijers, Leo},
	month = nov,
	year = {2011},
	keywords = {meta\_Assessing, meta\_Advocating, obj\_Infrastructures}
}

@article{stamatatos_survey_2009,
	title = {A survey of modern authorship attribution methods},
	volume = {60},
	issn = {1532-2882},
	url = {http://dl.acm.org/citation.cfm?id=1527090.1527102},
	doi = {10.1002/asi.v60:3},
	abstract = {Authorship attribution supported by statistical or computational methods has a long history starting from the 19th century and is marked by the seminal study of Mosteller and Wallace (1964) on the authorship of the disputed “Federalist Papers.” During the last decade, this scientific field has been developed substantially, taking advantage of research advances in areas such as machine learning, information retrieval, and natural language processing. The plethora of available electronic texts (e.g., e-mail messages, online forum messages, blogs, source code, etc.) indicates a wide variety of applications of this technology, provided it is able to handle short and noisy text from multiple candidate authors. In this article, a survey of recent advances of the automated approaches to attributing authorship is presented, examining their characteristics for both text representation and text classification. The focus of this survey is on computational requirements and settings rather than on linguistic or literary issues. We also discuss evaluation methodologies and criteria for authorship attribution studies and list open questions that will attract future work in this area. © 2009 Wiley Periodicals, Inc.},
	language = {en},
	number = {3},
	urldate = {2011-12-14},
	journal = {J. Am. Soc. Inf. Sci. Technol.},
	author = {Stamatatos, Efstathios},
	month = mar,
	year = {2009},
	keywords = {AnalyzeStatistically, meta\_GiveOverview, *****, t\_Stylometry},
	pages = {538--556}
}

@article{nadeau_survey_2007,
	title = {A survey of named entity recognition and classification},
	volume = {30},
	url = {http://www.ingentaconnect.com/content/jbp/li/2007/00000030/00000001/art00002?token=005219458c2514faa7e2a46762c6b635d3b662a2553492b467c673f7b2f267738703375686f497c05b},
	doi = {10.1075/li.30.1.03nad},
	abstract = {This survey covers fifteen years of research in the Named Entity Recognition and Classification (NERC) field, from 1991 to 2006. We report observations about languages, named entity types, domains and textual genres studied in the literature. From the start, NERC systems have been developed using hand-made rules, but now machine learning techniques are widely used. These techniques are surveyed along with other critical aspects of NERC such as features and evaluation methods. Features are word-level, dictionary-level and corpus-level representations of words in a document. Evaluation techniques, ranging from intuitive exact match to very complex matching techniques with adjustable cost of errors, are an indisputable key to progress.},
	language = {en},
	number = {1},
	journal = {Lingvisticae Investigationes},
	author = {Nadeau, David and Sekine, Satoshi},
	year = {2007},
	keywords = {*****, t\_NamedEntityRecognition},
	pages = {3--26},
	file = {Nadeau_Sekine_2007_A survey of named entity recognition and classification.pdf:/home/lisnux/Zotero/storage/MFL2P4PR/Nadeau_Sekine_2007_A survey of named entity recognition and classification.pdf:application/pdf}
}

@techreport{edgar_survey_2010,
	title = {A survey of the scholarly journals using open journal systems. {Scholarly} and {Research} {Communication}},
	url = {http://src-online.ca/index.php/src/article/view/24/41},
	abstract = {A survey of 998 scholarly journals that use Open Journal Systems (OJS), an open source journal software platform, captures the characteristics of an emerging class of scholar-publisher open access journals. The journals in the sample follow traditional norms for peer-reviewing, acceptance rates, and disciplinary focus, but as a group are distinguished by the number that offer open access to their content, growth rates in new titles, participation rates from developing countries, and extremely low operating budgets. The survey also documents the limited degree to which open source software can alter a field of communication, for OJS appears to have created a third path, dedicated to maximizing access to research and scholarship, as an alternative to traditional scholarly society and commercial publishing routes.},
	language = {en},
	institution = {Stanford University},
	author = {Edgar, B.D. and Willinsky, John},
	year = {2010},
	keywords = {obj\_Tools, act\_Publishing, obj\_ResearchResults}
}

@article{kim_survey_2019,
	title = {A {Survey} on {Sentiment} and {Emotion} {Analysis} for {Computational} {Literary} {Studies}},
	copyright = {CC BY-SA 4.0},
	url = {http://www.zfdg.de/2019_008},
	doi = {10.17175/2019_008},
	urldate = {2020-07-28},
	journal = {Zeitschrift für digitale Geisteswissenschaften},
	author = {Kim, Evgeny and Klinger, Roman},
	year = {2019},
	note = {Publisher: Herzog August Bibliothek
Version Number: 1.0},
	keywords = {obj\_Text, act\_Annotating, t\_SentimentAnalysis}
}

@article{labbe_tool_2006,
	title = {A {Tool} for {Literary} {Studies}: {Intertextual} {Distance} and {Tree} {Classification}},
	volume = {21},
	shorttitle = {A {Tool} for {Literary} {Studies}},
	url = {http://llc.oxfordjournals.org/content/21/3/311.abstract},
	doi = {10.1093/llc/fqi063},
	abstract = {How to measure proximities and oppositions in large text corpora? Intertextual distance provides a simple and interesting solution. Its properties make it a good tool for text classification, and especially for tree-analysis which is fully presented and discussed here. In order to measure the quality of this classification, two indices are proposed. The method presented provides an accurate tool for literary studies—as is demonstrated by applying it to two areas of French literature, Racine's tragedies and an authorship attribution experiment.},
	language = {en},
	number = {3},
	urldate = {2011-10-05},
	journal = {Literary and Linguistic Computing},
	author = {Labbé, Cyril and Labbé, Dominique},
	year = {2006},
	keywords = {AnalyzeStatistically, bigdata, t\_Stylometry},
	pages = {311 --326},
	file = {Labbé_Labbé_2006_A Tool for Literary Studies.pdf:/home/lisnux/Zotero/storage/TVH7XPAN/Labbé_Labbé_2006_A Tool for Literary Studies.pdf:application/pdf}
}

@article{waltman_unified_2010,
	title = {A unified approach to mapping and clustering of bibliometric networks},
	volume = {4},
	issn = {17511577},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S1751157710000660},
	doi = {10.1016/j.joi.2010.07.002},
	abstract = {In the analysis of bibliometric networks, researchers often use mapping and clustering techniques in a combined fashion. Typically, however, mapping and clustering techniques that are used together rely on very different ideas and assumptions. We propose a unified approach to mapping and clustering of bibliometric networks. We show that the VOS mapping technique and a weighted and parameterized variant of modularity-based clustering can both be derived from the same underlying principle. We illustrate our proposed approach by producing a combined mapping and clustering of the most frequently cited publications that appeared in the field of information science in the period 1999-2008. (C) 2010 Elsevier Ltd. All rights reserved.},
	language = {en},
	number = {4},
	urldate = {2013-04-05},
	journal = {Journal of Informetrics},
	author = {Waltman, Ludo and van Eck, Nees Jan and Noyons, Ed C.M.},
	month = oct,
	year = {2010},
	keywords = {act\_Visualizing, bigdata},
	pages = {629--635},
	file = {Waltman et al_2010_A unified approach to mapping and clustering of bibliometric networks.pdf:/home/lisnux/Zotero/storage/PJ4QJPZT/Waltman et al_2010_A unified approach to mapping and clustering of bibliometric networks.pdf:application/pdf}
}

@article{smithies_view_2011,
	title = {A {View} from {IT}},
	volume = {5},
	url = {http://digitalhumanities.org/dhq/vol/5/3/000107/000107.html},
	abstract = {As digital humanities projects grow in size and complexity university programs will need to adapt, balancing the needs of technological systems with the imperatives of the humanities tradition. While it makes sense to adapt the accumulated expertise of the commercial and government IT sectors, care needs to be taken to ensure any new approaches enhance rather than undermine the aims of the humanities generally. While digital humanists are uniquely positioned to help the humanities, care needs to be taken to ensure new project management and design techniques sourced from the IT world are applied critically and do not undermine the core aims of the discipline. If these caveats are kept in mind the IT world has a lot to offer digital humanists, however, especially in the field of Enterprise Architecture (EA), which aims to produce a holistic, high level view of technological systems with a view to understanding social and cultural as well as technological issues.},
	language = {en},
	number = {2},
	urldate = {2011-11-30},
	journal = {Digital Humanities Quarterly},
	author = {Smithies, James},
	year = {2011},
	keywords = {X-CHECK}
}

@inproceedings{barga_virtual_2007,
	title = {A {Virtual} {Research} {Environment} ({VRE}) for {Bioscience} {Researchers}},
	isbn = {978-0-7695-2992-9},
	url = {http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=04401895},
	doi = {10.1109/ADVCOMP.2007.14},
	abstract = {In this paper we present the Research Information Centre (RIC), a virtual research environment being jointly developed by the Technical Computing Group at Microsoft and The British Library. We view researchers as extreme information workers and the purpose of the RIC is to support researchers in managing the increasingly complex range of tasks involved in carrying out research. Our first implementation of the RIC is focused on the biomedical researcher, leveraging commercial off-the-shelf software to the extent possible. However, the base architecture of RIC is designed so that it can be re-used for research in other domains.},
	language = {en},
	urldate = {2011-08-03},
	publisher = {IEEE},
	author = {Barga, Roger and Andrews, Stephen and Parastatidis, Savas},
	year = {2007},
	keywords = {meta\_GiveOverview, obj\_Infrastructures, obj\_VREs},
	pages = {31--38}
}

@misc{noauthor_vos_2011,
	title = {A vos marques, prêts, bloguez !},
	url = {http://bublog.upmf-grenoble.fr/2011/11/17/bloguez/},
	language = {fr},
	urldate = {2011-11-26},
	journal = {Le temps d'un blog},
	month = nov,
	year = {2011}
}

@article{noauthor_aahc_2000,
	title = {{AAHC} {Suggested} {Guidelines} for {Evaluating} {Digital} {Media} {Activities} in {Tenure}, {Review}, and {Promotion} - {Neue} {Version}},
	volume = {3},
	url = {http://hdl.handle.net/2027/spo.3310410.0003.311},
	abstract = {Note: These recommendations were developed in consultation with the Modern Language Association and the American Political Science Association.},
	language = {en},
	number = {3},
	journal = {Journal of the Association for History and Computing},
	year = {2000},
	keywords = {meta\_Assessing, meta\_DefinePolicy}
}

@article{nicholas_abstract_2010,
	title = {Abstract {Modelling} of {Digital} {Identifiers}},
	volume = {62},
	issn = {1361-3200},
	url = {http://www.ariadne.ac.uk/issue62/nicholas-et-al/},
	abstract = {Discussion of digital identifiers, and persistent identifiers in particular, has often been confused by differences in underlying assumptions and approaches. To bring more clarity to such discussions, the PILIN Project has devised an abstract model of identifiers and identifier services, which is presented here in summary. Given such an abstract model, it is possible to compare different identifier schemes, despite variations in terminology; and policies and strategies can be formulated for persistence without committing to particular systems. The abstract model is formal and layered; in this article, we give an overview of the distinctions made in the model. This presentation is not exhaustive, but it presents some of the key concepts represented, and some of the insights that result.

The main goal of the Persistent Identifier Linking Infrastructure (PILIN) project [1] has been to scope the infrastructure necessary for a national persistent identifier service. There are a variety of approaches and technologies already on offer for persistent digital identification of objects. But true identity persistence cannot be bound to particular technologies, domain policies, or information models: any formulation of a persistent identifier strategy needs to outlast current technologies, if the identifiers are to remain persistent in the long term.

For that reason, PILIN has modelled the digital identifier space in the abstract. It has arrived at an ontology [2] and a service model [3] for digital identifiers, and for how they are used and managed, building on previous work in the identifier field [4] (including the thinking behind URI [5], DOI [6], XRI [7] and ARK [8]), as well as semiotic theory [9]. The ontology, as an abstract model, addresses the question ‘what is (and isn’t) an identifier?’ and ‘what does an identifier management system do?’. This more abstract view also brings clarity to the ongoing conversation of whether URIs can be (and should be) universal persistent identifiers.},
	language = {en},
	urldate = {2010-02-23},
	journal = {Ariadne},
	author = {Nicholas, Nick and Ward, Nigel and Blinco, Kerry},
	month = jan,
	year = {2010},
	keywords = {bigdata, obj\_Research, bigdata{\textasciitilde}, act\_Identifying}
}

@article{noauthor_academia_2012,
	title = {Academia in the {Age} of {Digital} {Reproduction}; {Or}, the {Journal} {System}, {Redeemed}},
	url = {http://thedisorderofthings.com/2012/02/08/academia-in-the-age-of-digital-reproduction-or-the-journal-system-redeemed/},
	abstract = {It took at least 200 years for the novel to emerge as an expressive form after the invention of the printing press.

So said Bob Stein in an interesting roundtable on the digital university from ...},
	language = {en},
	urldate = {2012-02-09},
	journal = {The Disorder Of Things},
	month = feb,
	year = {2012},
	keywords = {act\_Publishing, meta\_Advocating, obj\_ResearchResults}
}

@misc{carrigan_academic_2013,
	title = {Academic blogging – both/and rather than either/or},
	url = {http://markcarrigan.net/2013/01/10/academic-blogging-bothand-rather-than-eitheror/},
	language = {en},
	journal = {mikecarrigan.net},
	author = {Carrigan, Mike},
	month = jan,
	year = {2013},
	keywords = {meta\_Assessing, act\_Publishing, obj\_ResearchResults}
}

@article{xiao_academic_2014,
	title = {Academic opinions of {Wikipedia} and open-access publishing},
	volume = {38},
	issn = {1468-4527},
	language = {en},
	number = {3},
	urldate = {2014-05-14},
	journal = {Online Information Review},
	author = {Xiao, Lu and Askin, Nicole},
	month = apr,
	year = {2014},
	pages = {2--2}
}

@misc{hanson_academic_2008,
	title = {Academic {Publishing} in the {Digital} {Age}},
	url = {http://hastac.org/forums/hastac-scholars-discussions/academic-publishing-digital-age},
	language = {en},
	journal = {HASTAC Scholars Forum},
	author = {Hanson, Christopher},
	year = {2008}
}

@techreport{finch_accessibility_2012,
	title = {Accessibility, sustainability, excellence: how to expand access to research publications},
	url = {http://www.researchinfonet.org/publish/finch/},
	abstract = {Report of the Working Group on Expanding Access to Published Research Findings – the Finch Group},
	language = {en},
	author = {Finch},
	month = jun,
	year = {2012}
}

@article{dunning_accurate_1993,
	title = {Accurate {Methods} for the {Statistics} of {Surprise} and {Coincidence}},
	volume = {19},
	url = {http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.14.5962},
	abstract = {Much work has been done on the statistical analysis of text. In some cases reported in the literature, inappropriate statistical methods have been used, and statistical significance of results have not been addressed. In particular, asymptotic normality assumptions have often been used unjustifiably, leading to flawed results.This assumption of normal distribution limits the ability to analyze rare events. Unfortunately rare events do make up a large fraction of real text.However, more applicable methods based on likelihood ratio tests are available that yield good results with relatively small samples. These tests can be implemented efficiently, and have been used for the detection of composite terms and for the determination of domain-specific terms. In some cases, these measures perform much better than the methods previously used. In cases where traditional contingency table methods work well, the likelihood ratio tests described here are nearly identical.This paper describes the basis of a measure based on likelihood ratios that can be applied to the analysis of text.},
	language = {en},
	number = {1},
	journal = {COMPUTATIONAL LINGUISTICS},
	author = {Dunning, Ted},
	year = {1993},
	keywords = {AnalyzeStatistically, bigdata, t\_Stylometry},
	pages = {61--74}
}

@article{anderson-wilk_achieving_2011,
	title = {Achieving rigor and relevance in online multimedia scholarly publishing},
	volume = {16},
	url = {http://firstmonday.org/htbin/cgiwrap/bin/ojs/index.php/fm/article/view/3762/3119},
	abstract = {This paper discusses the importance of relevance and rigor in scholarly publishing in a new media–rich world. We defend that scholarship should be useful and engaging to audiences through the use of new media, and at the same time scholarly publishers must develop and maintain methods of ensuring content accuracy and providing quality controls in the production of scholarly multimedia products. We review examples and a case study of existing scholarly publishing venues that attempt to maintain quality control standards while embracing innovative multimedia formats. We also present lessons learned from the case experience and challenges that face us in the scholarly publication of multimedia.},
	language = {en},
	number = {12},
	journal = {First Monday},
	author = {Anderson-Wilk, Mark and Hino, Jeff},
	year = {2011}
}

@article{jarvisalo_action_1975,
	title = {Action of propranolol on mitochondrial functions--effects on energized ion fluxes in the presence of valinomycin},
	volume = {24},
	issn = {0006-2952},
	url = {http://www.sciencedirect.com/science/article/pii/000629527590009X},
	abstract = {The effects of propranolol on mitochondrial ion fluxes have been studied in oscillatory conditions in the presence of potassium plus valinomycin. The drug was able to decrease slightly the rate of swelling and that of passive contraction a little more. When the concentration of external salt was below 10 mM, propranolol decreased the extent of swelling; above 15 mM it was increased by the drug. The inhibition of contraction was abolished by nigericin or raised pH and was less evident with ATP rather than succinate as the source of energy. Propranolol only slightly affected the oxidative phosphorylation and latent ATPase activity of mitochondria. At low concentrations of salts the rate of valinomycin-stimulated respiration and ATPase activity were both inhibited. At higher concentrations of the salts, both were stimulated. It is suggested that the stimulation of respiration and ATPase activity result from increased ion uptake. Propranolol seems to stabilize an “energized” state, possibly by inhibiting the entry of protons, thereby retarding the equilibration of ionic gradients across the inner membrane.},
	language = {en},
	number = {18},
	journal = {Biochemical pharmacology},
	author = {Järvisalo, J and Saris, N E},
	month = sep,
	year = {1975},
	pages = {1701--1705}
}

@book{hardmeier_ad_2000,
	address = {Amsterdam},
	title = {Ad fontes! {Quellen} erfassen - lesen - deuten. {Was} ist {Computerphilologie}? {Ansatzpunkte} und {Methodologie}-{Instrumente} und {Praxis}. {Contributions} to the {Conference} {Computerphilologie} 5},
	isbn = {978-90-5383-677-4},
	language = {de},
	publisher = {VU University Press},
	editor = {Hardmeier, Christof},
	year = {2000}
}

@phdthesis{czmiel_adaquate_2003,
	type = {Thesis ({MA} level)},
	title = {Adäquate {Markupsysteme} für die digitale {Behandlung} altägyptischer {Texte}},
	url = {http://old.hki.uni-koeln.de/studium/MA/MA_czmiel.pdf},
	language = {de},
	school = {Köln},
	author = {Czmiel, Alexander},
	month = oct,
	year = {2003},
	keywords = {t\_Encoding, t\_XML, goal\_Enrichment}
}

@article{jessen_aggregated_2012,
	title = {Aggregated trustworthiness: {Redefining} online credibility through social validation},
	volume = {17},
	url = {http://firstmonday.org/htbin/cgiwrap/bin/ojs/index.php/fm/article/view/3731/3132},
	abstract = {This article investigates the impact of social dynamics on online credibility. Empirical studies by Pettingill (2006) and Hargittai, et al. (2010) suggest that social validation and online trustees play increasingly important roles when evaluating credibility online. This dynamic puts pressure on the dominant theory of online credibility presented by Fogg and Tseng (1999). To remedy this problem we present a new theory we call “aggregated trustworthiness” based on social dynamics and online navigational practices.},
	language = {en},
	number = {1-2},
	journal = {First Monday},
	author = {Jessen, Johan and Jørgensen, Anker Helms},
	year = {2012},
	keywords = {meta\_Assessing, obj\_ResearchResults}
}

@article{piotrowski_aint_2020,
	title = {Ain’t {No} {Way} {Around} {It}: {Why} {We} {Need} to {Be} {Clear} {About} {What} {We} {Mean} by “{Digital} {Humanities}”},
	shorttitle = {Ain’t {No} {Way} {Around} {It}},
	author = {Piotrowski, Michael},
	year = {2020}
}

@inproceedings{van_zundert_alfalab_2009,
	title = {Alfalab: {Construction} and {Deconstruction} of a {Digital} {Humanities} {Experiment}},
	isbn = {978-0-7695-3877-8},
	shorttitle = {Alfalab},
	doi = {10.1109/e-Science.2009.8},
	abstract = {This paper presents project 'Alfalab'. Alfalab is a collaborative frame work project of the Royal Netherlands Academy of Arts and Sciences (KNAW). It explores the success and fail factors for virtual research collaboration and supporting digital infrastructure in the Humanities. It does so by delivering a virtual research environment engineered through a virtual R\&D collaborative and by drawing in use cases and feedback from Humanities researchers from two research fields: textual historical text research and historical GIS-application. The motivation for the project is found in a number of commonly stated factors that seem to be inhibiting general application of virtualized research infrastructure in the Humanities. The paper outlines the project's motivation, key characteristics and implementation. One of the pilot applications is described in greater detail.},
	language = {en},
	booktitle = {Fifth {IEEE} {International} {Conference} on e-{Science}, 2009. e-{Science} '09},
	publisher = {IEEE},
	author = {van Zundert, J. and Zeldenrust, D. and Beaulieu, A.},
	month = dec,
	year = {2009},
	keywords = {obj\_VREs},
	pages = {1--5},
	file = {van Zundert et al_2009_Alfalab.pdf:/home/lisnux/Zotero/storage/ZW6EXUER/van Zundert et al_2009_Alfalab.pdf:application/pdf}
}

@incollection{ramsay_algorithmic_2008,
	address = {Oxford},
	series = {Blackwell {Companions} to {Literature} and {Culture}},
	title = {Algorithmic {Criticism}},
	isbn = {978-1-4051-4864-1},
	url = {http://www.digitalhumanities.org/companionDLS/},
	abstract = {A Companion to Digital Literary Studies provides the most thorough single-source exploration to date of the many intersections between literary studies and computation. These intersections are documented and explored by leading scholars, theorists, and practitioners and together they record the history, survey the present, and speculate on the future of this emerging and multi-faceted discipline. Articles are grouped into sections which gather and engage the most important scholarship from the field on topics ranging from scholarly editing and literary criticism, to interactive fiction, multimedia, immersive environments, and beyond.The Companion opens with period-specific surveys and engagements of how computational methods have been applied to a number of areas of literary studies, followed by an overview of the field's major theoretical and methodological perspectives related to new modes of writing and reading enabled by electronic text. Further sections investigate the new genres of electronic literature, including hypertext literature, installations, gaming, and weblogs, while providing an overview of formatting concerns and best practices for digital preservation. This volume is an accessible and practical resource for those who want to understand, use, or create digital literature.},
	language = {en},
	urldate = {2010-02-24},
	booktitle = {Companion to {Digital} {Literary} {Studies}},
	publisher = {Blackwell Publishing Professional},
	author = {Ramsay, Stephen},
	month = dec,
	year = {2008},
	note = {Review: http://www.digitalstudies.org/ojs/index.php/digital\_studies/article/view/233/275},
	keywords = {AnalyzeStatistically, meta\_Assessing, *****, obj\_Literature}
}

@article{burrows_all_2007,
	title = {All the {Way} {Through}: {Testing} for {Authorship} in {Different} {Frequency} {Strata}},
	volume = {22},
	shorttitle = {All the {Way} {Through}},
	url = {http://llc.oxfordjournals.org/content/22/1/27.abstract},
	doi = {10.1093/llc/fqi067},
	abstract = {This article describes the operation of two new tests of authorship and offers some results. Both tests rely on controlled contrasts of word-frequency and both exclude the very common words, which have been put to such good use in recent years. One test treats of words used with some consistency by a target-author but more sporadically by others. The second treats of words used sporadically by the target-author but not by most others. (The inclusion of words that some other authors use avoids the strict constraint that has impoverished this form of evidence.) In suitable cases, both tests prove very accurate. The fact that evidence of authorship can be detected in these three distinct frequency-strata helps to explain why such tests should work at all and so encourages the development of even better ones.},
	language = {en},
	number = {1},
	urldate = {2011-12-14},
	journal = {Literary and Linguistic Computing},
	author = {Burrows, John},
	month = apr,
	year = {2007},
	keywords = {t\_Stylometry},
	pages = {27 --47}
}

@article{meister_alle_1994,
	title = {Alle {Erklärung} muß fort und nur {Beschreibung} an ihre {Stelle} treten. {Vor}-Überlegungen zu einer computergestützten empirischen {Analyse} fiktionaler {Handlungen}},
	language = {de},
	journal = {Internationales Jahrbuch für Germanistik 2 (1994), 30-58},
	author = {Meister, Jan Christoph},
	year = {1994},
	keywords = {AnalyzeStatistically, meta\_Theorizing, obj\_Literature}
}

@misc{priem_alt-metrics_2010,
	title = {Alt-metrics: {A} {Manifesto}},
	url = {http://altmetrics.org/manifesto/},
	language = {en},
	journal = {altmetrics.org},
	author = {Priem, Jason and Taraborelli, D. and Groth, P. and Neylon, C.},
	year = {2010},
	keywords = {bigdata{\textasciitilde}}
}

@article{jannidis_alte_2005,
	title = {Alte {Romane} und neue {Bibliotheken}. {Zum} {Projekt} eines digitalen historischen {Referenzkorpus} des {Deutschen}},
	language = {de},
	journal = {Die innovative Bibliothek. Elmar Mittler zum 65. Geburtstag},
	author = {Jannidis, Fotis and Lauer, Gerhard and Rapp, Andrea},
	year = {2005},
	note = {Jannidis, Fotis/ Gerhard Lauer/Andrea Rapp: Alte Romane und neue Bibliotheken. Zum Projekt eines digitalen historischen Referenzkorpus des Deutschen. In: Erand Kolding Nielsen/Klaus G. Saur/Klaus Ceynowa (Hg.): Die innovative Bibliothek. Elmar Mittler zum 65. Geburtstag. München: Saur 2005, S. 139-150.},
	keywords = {goal\_Enrichment, goal\_Creation},
	pages = {139--150}
}

@article{stinson_amazing_2014,
	title = {An {Amazing} {Discovery}: {Andy} {Warhol}’s {Groundbreaking} {Computer} {Art}},
	shorttitle = {An {Amazing} {Discovery}},
	url = {http://www.wired.com/2014/04/an-amazing-discovery-andy-warhols-seminal-computer-art/},
	abstract = {Until now, it was unknown if Warhol had made any digital artworks on his own time.},
	urldate = {2014-04-28},
	journal = {Wired.com},
	author = {Stinson, Liz},
	month = apr,
	year = {2014},
	keywords = {act\_Preservation, obj\_Data}
}

@article{merriam_application_2003,
	title = {An {Application} of {Authorship} {Attribution} by {Intertextual} {Distance} in {English}},
	copyright = {© Tous droits réservés},
	issn = {1638-9808},
	url = {http://corpus.revues.org/35?&id=35},
	abstract = {Une application d’attribution d’auteur au moyen de la distance intertextuelle en anglais Le calcul de distance intertextuelle que C. et D. Labbé appliquent aux textes français peut être utilisé pour différencier les œuvres d’au moins deux auteurs dramatiques contemporains de l’époque élisabéthaine, William Shakespeare et Thomas Middleton. Bien que les 46 textes sous étude, transcrits avec une orthographe moderne, ne soient pas lemmatisés et que seuls des échantillons de textes de même longueur aient été utilisés, les indices de distance intertextuelle qu’on a pu ainsi établir empiriquement sont du même ordre de grandeur que ceux qu’ont établis C. et D. Labbé pour le français. Timon of Athens considéré comme étant pour deux-tiers de Shakespeare et pour un tiers de Middleton se place entre le groupe des œuvres de Shakespeare et celui des œuvres de Middleton dans une analyse multidimensionnelle de 1035 distances intertextuelles.},
	language = {fr},
	number = {2},
	urldate = {2013-07-06},
	journal = {Corpus},
	author = {Merriam, Thomas},
	collaborator = {Luong, Xuan},
	month = dec,
	year = {2003},
	keywords = {AnalyzeStatistically}
}

@misc{prescott_electric_2012,
	title = {An {Electric} {Current} of the {Imagination}: {What} the {Digital} {Humanities} {Are} and {What} {They} {Might} {Become}},
	shorttitle = {An {Electric} {Current} of the {Imagination}},
	url = {http://journalofdigitalhumanities.org/1-2/an-electric-current-of-the-imagination-by-andrew-prescott/},
	language = {en},
	urldate = {2012-11-25},
	journal = {Journal of Digital Humanities},
	author = {Prescott, Andrew},
	month = jun,
	year = {2012},
	keywords = {meta\_Assessing, obj\_DigitalHumanities, act\_Conceptualizing}
}

@article{forman_extensive_2003,
	title = {An extensive empirical study of feature selection metrics for text classification},
	volume = {3},
	issn = {1532-4435},
	url = {http://dl.acm.org/citation.cfm?id=944919.944974},
	abstract = {Machine learning for text classification is the cornerstone of document categorization, news filtering, document routing, and personalization. In text domains, effective feature selection is essential to make the learning task efficient and more accurate. This paper presents an empirical comparison of twelve feature selection methods (e.g. Information Gain) evaluated on a benchmark of 229 text classification problem instances that were gathered from Reuters, TREC, OHSUMED, etc. The results are analyzed from multiple goal perspectives-accuracy, F-measure, precision, and recall-since each is appropriate in different situations. The results reveal that a new feature selection metric we call 'Bi-Normal Separation' (BNS), outperformed the others by a substantial margin in most situations. This margin widened in tasks with high class skew, which is rampant in text classification problems and is particularly challenging for induction algorithms. A new evaluation methodology is offered that focuses on the needs of the data mining practitioner faced with a single dataset who seeks to choose one (or a pair of) metrics that are most likely to yield the best performance. From this perspective, BNS was the top single choice for all goals except precision, for which Information Gain yielded the best result most often. This analysis also revealed, for example, that Information Gain and Chi-Squared have correlated failures, and so they work poorly together. When choosing optimal pairs of metrics for each of the four performance goals, BNS is consistently a member of the pair---e.g., for greatest recall, the pair BNS + F1-measure yielded the best performance on the greatest number of tasks by a considerable margin.},
	language = {en},
	urldate = {2012-12-03},
	journal = {J. Mach. Learn. Res.},
	author = {Forman, George},
	month = mar,
	year = {2003},
	keywords = {bigdata},
	pages = {1289--1305}
}

@phdthesis{deininghaus_interactive_2010,
	address = {Aachen, RWTH Aachen University},
	type = {Diploma {Thesis}},
	title = {An {Interactive} {Surface} for {Literary} {Criticism}},
	url = {http://hci.rwth-aachen.de/materials/publications/deininghaus2010b.pdf},
	abstract = {Professionals in literary studies regularly need to work with large amounts of
complex text. Special forms of editions have been developed to make these texts
accessible to the literary critic. Nowadays, there exist both traditional printed
editions and digital editions for use with standard computers.
In this thesis, we present an examination of the characteristics of these editions,
their users, and the users’ work processes from an HCI perspective. Based on these
findings, we propose a design for a working environment that enables the user to
read, navigate, and personalize printed and digital information on one integrative
interactive surface, benefiting from the respective advantages of both mediums.
This also leads to a promising new way of structuring content in text editions.
Furthermore, we portray the evaluation and refinement of this proposal through
two full design iterations using prototypes — each iteration backed by a qualitative
user study — and outline possible future work.},
	language = {en},
	author = {Deininghaus, Stephan},
	year = {2010},
	keywords = {AnalyzeQualitatively, obj\_Tools, bigdata{\textasciitilde}, obj\_Literature, t\_Usability}
}

@inproceedings{hakinson_interchange_2010,
	address = {Utrecht, Netherlands},
	title = {An {Interchange} {Format} for {Optical} {Music} {Recognition} {Applications}},
	url = {http://ismir2010.ismir.net/proceedings/ismir2010-11.pdf},
	abstract = {Page
appearance and
layout for
music notation is a
cri
tical component of the overall musical information
contained in a document
.
To capture and transfe
r this
information, we outline an interchange format for OMR
applications, the OMR Interchange Package (OIP)
format,
which is
designed to allow layout information
and page images to be
preserved and
transferred along
with semantic musical content. We ident
ify a number of
uses for this format that can enhance digital
representations of music, and introduce a novel
idea for
distributed optical music recognition
system
based on this
format},
	language = {en},
	booktitle = {In {Proceedings} of the 11th {International} {Society} for {Music} {Information} {Retrieval} {Conference} ({ISMIR} 2010)},
	author = {Hakinson, A. and Pugin, Laurent and Fujinaga, I.},
	year = {2010},
	keywords = {act\_DataRecognition, obj\_SheetMusic},
	pages = {51--56}
}

@article{feinerer_introduction_2008,
	title = {An {Introduction} to {Text} {Mining} in {R}},
	volume = {8},
	url = {http://140.247.115.171/CRAN/doc/Rnews/Rnews_2008-2.pdf#page=19},
	abstract = {Text mining has gained big interest both in aca-
demic research as in business intelligence applica-
tions within the last decade. There is an enormous
amount of textual data available in machine readable
format which can be easily accessed via the Internet
or databases. This ranges from scientific articles, ab-
stracts and books to memos, letters, online forums,
mailing lists, blogs, and other communication media
delivering sensible information.
Text mining is a highly interdisciplinary research
field utilizing techniques from computer science, lin-
guistics, and statistics. For the latter R is one of the
leading computing environments offering a broad
range of statistical methods. However, until recently,
R has lacked an explicit framework for text mining
purposes. This has changed with the
tm
(Feinerer,
2008; Feinerer et al., 2008) package which provides
a text mining infrastructure for R. This allows R
users to work efficiently with texts and correspond-
ing meta data and transform the texts into structured
representations where existing R methods can be ap-
plied, e.g., for clustering or classification.
In this article we will give a short overview of the
tm package. Then we will present two exemplary
text mining applications, one in the field of stylome-
try and authorship attribution, the second is an anal-
ysis of a mailing list. Our aim is to show that
tm provides the necessary abstraction over the actual text
management so that the reader can use his own texts
for a wide variety of text mining techniques.},
	language = {en},
	number = {2},
	journal = {Rnews},
	author = {Feinerer, Ingo},
	year = {2008},
	keywords = {AnalyzeStatistically, bigdata},
	pages = {19--22}
}

@article{guyon_introduction_2003,
	title = {An introduction to variable and feature selection},
	volume = {3},
	issn = {1532-4435},
	url = {http://dl.acm.org/citation.cfm?id=944919.944968},
	abstract = {Variable and feature selection have become the focus of much research in areas of application for which datasets with tens or hundreds of thousands of variables are available. These areas include text processing of internet documents, gene expression array analysis, and combinatorial chemistry. The objective of variable selection is three-fold: improving the prediction performance of the predictors, providing faster and more cost-effective predictors, and providing a better understanding of the underlying process that generated the data. The contributions of this special issue cover a wide range of aspects of such problems: providing a better definition of the objective function, feature construction, feature ranking, multivariate feature selection, efficient search methods, and feature validity assessment methods.},
	language = {en},
	urldate = {2012-12-03},
	journal = {J. Mach. Learn. Res.},
	author = {Guyon, Isabelle and Elisseeff, André},
	month = mar,
	year = {2003},
	keywords = {AnalyzeStatistically, bigdata},
	pages = {1157--1182}
}

@misc{froehlich_introductory_2014,
	title = {An introductory bibliography to corpus linguistics},
	url = {http://hfroehlich.wordpress.com/2014/05/11/intro-bibliography-corpus-linguistics/},
	abstract = {This is a short bibliography meant to get you started in corpus linguistics – it is by no means comprehensive, but should serve to be a good introductory overview of the field.},
	language = {en},
	journal = {http://hfroehlich.wordpress.com},
	author = {Froehlich, Heather},
	year = {2014}
}

@article{burrows_ocean_1989,
	title = {‘{An} ocean where each kind. . .’: {Statistical} analysis and some major determinants of literary style},
	volume = {23},
	issn = {0010-4817, 1572-8412},
	shorttitle = {‘{An} ocean where each kind. . .’},
	url = {http://www.springerlink.com/content/7121357k22l8u511/},
	doi = {10.1007/BF02176636},
	abstract = {The statistical analysis of literary texts has yielded valuable results, not least when it has treated of the frequency patterns of very common words. But, whereas particular frequency patterns have usually been examined as discrete phenomena, it is possible to correlate the frequency profiles of all the very common words, to subject the resulting correlation matrix to eigen analysis, and to present the results in graphic form. The specimens offered here deal, first, with differences among Jane Austen's characters and, secondly, with differences between authors. The most striking general differences among the authors studied relate to historical eras and authorial gender.},
	language = {en},
	number = {4-5},
	urldate = {2011-12-08},
	journal = {Computers and the Humanities},
	author = {Burrows, J. F.},
	month = aug,
	year = {1989},
	keywords = {bigdata, t\_Stylometry},
	pages = {309--321}
}

@article{guerrini_analyzing_2011,
	title = {Analyzing {Culture} with {Google} {Books}: {Is} {It} {Social} {Science}?},
	url = {http://www.miller-mccune.com/media/culturomics-an-idea-whose-time-has-come-34742/},
	language = {en},
	journal = {Miller-McCune Magazine},
	author = {Guerrini, Anita},
	month = aug,
	year = {2011},
	keywords = {AnalyzeStatistically}
}

@book{baayen_analyzing_2008,
	address = {Cambridge},
	edition = {1. publ.},
	title = {Analyzing linguistic data. {A} practical introduction to statistics using {R}},
	isbn = {978-0-521-70918-7},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Baayen, R Harald},
	year = {2008},
	keywords = {AnalyzeStatistically, meta\_GiveOverview, bigdata, obj\_Language}
}

@article{cohen_analyzing_2010,
	chapter = {Books},
	title = {Analyzing {Literature} by {Words} and {Numbers} - {Humanities} 2.0},
	issn = {0362-4331},
	url = {http://www.nytimes.com/2010/12/04/books/04victorian.html},
	abstract = {Victorian Literature, Statistically Analyzed With New Process},
	language = {en},
	urldate = {2011-04-06},
	journal = {The New York Times},
	author = {Cohen, Patricia},
	month = dec,
	year = {2010},
	keywords = {AnalyzeStatistically, meta\_GiveOverview, bigdata{\textasciitilde}}
}

@book{rommel_and_1995,
	title = {And {Trace} it in this {Poem} {Every} {Line}. {Methoden} und {Verfahren} computerunterstuetzter {Textanalyse} am {Beispiel} von {Lord} {Byrons} {Don} {Juan}},
	language = {en},
	publisher = {Tübingen: Gunter Narr 1995},
	author = {Rommel, Thomas},
	year = {1995},
	note = {Rommel, Thomas: And Trace it in this Poem Every Line. Methoden und Verfahren computerunterstuetzter Textanalyse am Beispiel von Lord Byrons Don Juan. Tübingen: Gunter Narr 1995.},
	keywords = {AnalyzeQualitatively, meta\_Theorizing, obj\_Literature}
}

@misc{beieler_animated_2013,
	type = {Blog},
	title = {Animated {Protest} {Mapping} (1979-2013)},
	url = {http://johnbeieler.org/blog/2013/07/31/animated-protest-mapping/},
	abstract = {Many people, including myself, were interested in how protest behavior changes over time. I created an animated protest map (events of every type over time).},
	language = {en},
	journal = {john Beieler},
	author = {Beieler, John},
	year = {2013},
	note = {Political Science},
	keywords = {meta\_GiveOverview, obj\_Events, obj\_Maps}
}

@article{wolfe_annotations_2008,
	title = {Annotations and the collaborative digital library: {Effects} of an aligned annotation interface on student argumentation and reading strategies},
	volume = {3},
	issn = {1556-1607, 1556-1615},
	shorttitle = {Annotations and the collaborative digital library},
	url = {http://link.springer.com/article/10.1007/s11412-008-9040-x},
	doi = {10.1007/s11412-008-9040-x},
	abstract = {Recent research on annotation interfaces provides provocative evidence that anchored, annotation-based discussion environments may lead to better conversations about a text. However, annotation interfaces raise complicated tradeoffs regarding screen real estate and positioning. It is argued that solving this screen real estate problem requires limiting the number of annotations displayed to users. In order to understand which annotations have the most learning value for students, this paper presents two complementary studies examining the effects of annotations on students performing a reading-to-write task. The first study used think-aloud protocols and a within-subjects methodology, finding that annotations appeared to provoke students to reflect more critically upon the primary text. This effect was particularly strong when students encountered pairs of annotations presenting different viewpoints on the same section of text. Student interviews suggested that annotations were most helpful when they caused the reader to consider and weigh conflicting viewpoints. The second study used a between-subjects methodology and a more naturalistic task to provide complementary evidence that annotations encourage more reflective responses to a text. This study found that students who received annotated materials both perceived themselves and were perceived by instructors as less reliant on unreflective summary strategies than students who received the same content but in a different format. These findings indicate that the learning value of an annotation lies in its ability to provoke students to consider and weigh new perspectives on the primary text. When selected effectively, annotations provide a critical scaffolding that can support students’ critical thinking and argumentation activities. Collaborative digital libraries and applications for the Web 2.0 should be designed with this learning framework in mind.},
	number = {2},
	urldate = {2014-03-26},
	author = {Wolfe, Joanna},
	month = jun,
	year = {2008},
	note = {00036 bibtex: Wolfe2008Annotations 
biblatexdata[journaltitle=International Journal of Computer-Supported Collaborative Learning;langid=english;shortjournal=Computer Supported Learning]},
	keywords = {Activity: Annotate, Object: Texts},
	pages = {141--164}
}

@book{herkt_anwendungsmoglichkeiten_1991,
	address = {Bochum},
	series = {Bochumer historische {Studien} / {Mittelalterliche} {Geschichte}},
	title = {Anwendungsmöglichkeiten computergestützter {Erfassungs}- und {Auswertungshilfen} am {Beispiel} der {Güter}- und {Einkünfteverzeichnisse} des {Kollegiatstiftes} {St}. {Mauritz} in {Münster}},
	isbn = {978-3-88339-902-7},
	url = {http://brockmeyer-verlag.de/shop/article_406/Herkt,-Matthias%3A-Anwendungsm%C3%B6glichkeiten-computergest%C3%BCtzter-Erfassungs-u.-Auswertungshilfen-am-Beispiel-der-G%C3%BCter--und-Eink%C3%BCnfteverzeichnisse-des-Kollegiatstiftes-St.-Mauritz-in-M%C3%BCnster..html?shop_param=cid%3D20%26aid%3D406%26},
	abstract = {Wirtschaftsgeschichtliche Massenquellen zur Grundbesitz- und Einkünfteentwicklung einer bedeutenden geistlichen Institution des westfälischen Raumes werden hier untersucht. Es geht dabei auch um die Nützlichkeit moderner Datenbank-Verwaltungssysteme für die Organisation von Massenquellen zum Zweck der quantitativen Analyse im Bereich der Historiographie des Mittelalters.},
	language = {de},
	number = {9},
	publisher = {Brockmeyer},
	author = {Herkt, Matthias},
	year = {1991},
	note = {Bochum, Univ., Diss., 1991},
	keywords = {AnalyzeStatistically, bigdata, obj\_Documents}
}

@book{mosteller_applied_1983,
	edition = {2nd ed.},
	title = {Applied {Bayesian} and {Classical} {Inference}: {The} {Case} of the {Federalist} {Papers}},
	isbn = {978-0-387-90991-2},
	shorttitle = {Applied {Bayesian} and {Classical} {Inference}},
	abstract = {The new version has two additions. First, at the suggestion of Stephen Stigler I we have replaced the Table of Contents by what he calls an Analytic Table of Contents. Following the title of each section or subsection is a description of the content of the section. This material helps the reader in several ways, for example: by giving a synopsis of the book, by explaining where the various data tables are and what they deal with, by telling what theory is described where. We did several distinct full studies for the Federalist papers as well as many minor side studies. Some or all may offer information both to the applied and the theoretical reader. We therefore try to give in this Contents more than the few cryptic words in a section heading to {\textasciitilde}peed readers in finding what they want. Seconq, we have prepared an extra chapter dealing with authorship work published from. about 1969 to 1983. Although a chapter cannot compre- hensively Gover a field where many books now appear, it can mention most ofthe book-length works and the main thread of authorship' studies published in English. We founq biblical authorship studies so extensive and com- plicated that we thought it worthwhile to indicate some papers that would bring out the controversies that are taking place. We hope we have given the flavor of developments over the 15 years mentioned. We have also corrected a few typographical errors.},
	language = {en},
	publisher = {Springer New York},
	author = {Mosteller, F. and Wallace, D. L.},
	month = jan,
	year = {1983},
	keywords = {AnalyzeStatistically, t\_Stylometry}
}

@book{kansa_archaeology_2011,
	address = {Cotsen Institute of Archaeology at U C L A},
	title = {Archaeology 2. 0 and {Beyond}},
	url = {http://www.escholarship.org/uc/item/1r6137tb#page-2},
	language = {en},
	author = {Kansa, Eric C and Kansa, Sarah Whitcher and Watrall, Ethan},
	month = aug,
	year = {2011},
	keywords = {meta\_GiveOverview, X-CHECK, obj\_Artefacts}
}

@article{science_and_technology_art_2011,
	title = {Art criticism and computers: {Painting} by numbers. {Digital} analysis is invading the world of the connoisseur},
	url = {http://www.economist.com/node/21524699},
	abstract = {UDGING artistic styles, and the similarities between them, might be thought one bastion of human skill that machines could never storm. Not so, if Lior Shamir at Lawrence Technological University in Michigan is correct. A paper he has just published in Leonardo suggests that computers may have just as good an eye for style as humans do—and, in some cases, may see connections between artists that human critics have missed.},
	language = {en},
	urldate = {2011-08-22},
	journal = {The Economist},
	author = {Science {and} Technology},
	month = jul,
	year = {2011},
	keywords = {AnalyzeStatistically, t\_Stylometry, bigdata{\textasciitilde}, act\_RelationalAnalysis, obj\_Images},
	pages = {67}
}

@misc{rockwell_as_2010,
	title = {As {Transparent} as {Infrastructure}: {On} the research of cyberinfrastructure in the humanities},
	url = {http://cnx.org/content/m34315/latest/},
	language = {en},
	journal = {Connexions Web site},
	author = {Rockwell, Geoffrey},
	month = may,
	year = {2010},
	keywords = {meta\_GiveOverview, meta\_Assessing, act\_Conceptualizing, obj\_Infrastructures}
}

@article{bush_as_1945,
	title = {As {We} {May} {Think}},
	url = {http://www.theatlantic.com/magazine/archive/1945/07/as-we-may-think/3881/?single_page=true},
	language = {en},
	journal = {The Atlantic},
	author = {Bush, Vannevar},
	month = jul,
	year = {1945},
	note = {doi: 10.3998/3336451.0001.101},
	keywords = {*****, act\_Conceptualizing}
}

@incollection{friedlander_asking_2009,
	address = {Washington},
	title = {Asking {Questions} and {Building} a {Research} {Agenda} for {Digital} {Scholarship}},
	url = {http://www.clir.org/activities/digitalscholar2/friedlander.pdf},
	language = {en},
	booktitle = {Working {Together} or {Apart}: {Promoting} the {Next} {Generation} of {Digital} {Scholarship}},
	publisher = {Council on Library and Information Resources (CLIR)},
	author = {Friedlander, Amy},
	year = {2009},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities, meta\_Advocating},
	pages = {1--15}
}

@book{harley_assessing_2010,
	address = {Berkeley},
	title = {Assessing the {Future} {Landscape} of {Scholarly} {Communication}: {An} {Exploration} of {Faculty} {Values} and {Needs} in {Seven} {Disciplines}},
	url = {http://www.escholarship.org/uc/item/15x7385g},
	abstract = {Since 2005, the Center for Studies in Higher Education (CSHE), with generous funding from the
Andrew W. Mellon Foundation, has been conducting research to understand the needs and
practices of faculty for in-progress scholarly communication (i.e., forms of communication
employed as research is being executed) as well as archival publication. This report brings
together the responses of 160 interviewees across 45, mostly elite, research institutions in seven
selected academic fields: archaeology, astrophysics, biology, economics, history, music, and
political science. The overview document summarizes the main practices we explored across all
seven disciplines: tenure and promotion, dissemination, sharing, collaboration, resource creation
and consumption, and public engagement. We published the report online in such a way that
readers can search various topics within and across case studies.∗ Our premise has always been
that disciplinary conventions matter and that social realities (and individual personality) will
dictate how new practices, including those under the rubric of Web 2.0 or cyberinfrastructure,
are adopted by scholars. That is, the academic values embodied in disciplinary cultures, as well
as the interests of individual players, have to be considered when envisioning new schemata for
the communication of scholarship at its various stages.
We identified five key topics, addressed in detail in the case studies, that require real attention:
(1) The development of more nuanced tenure and promotion practices that do not rely
exclusively on the imprimatur of the publication or easily gamed citation metrics,
(2) A reexamination of the locus, mechanisms, timing, and meaning of peer review,
(3) Competitive, high-quality, and affordable journals and monograph publishing platforms
(with strong editorial boards, peer review, and sustainable business models),
(4) New models of publication that can accommodate arguments of varied length, rich
media, and embedded links to data; plus institutional assistance to manage permissions
of copyrighted material, and
(5) Support for managing and preserving new research methods and products, including
components of natural language processing, visualization, complex distributed
databases, and GIS, among many others.
Although robust infrastructures are needed locally and beyond, the sheer diversity of scholars’
needs across the disciplines and the rapid evolution of the technologies themselves means that
one-size-fits-all solutions will almost always fall short. As faculty continue to innovate and pursue
new avenues in their research, both the technical and human infrastructure will have to evolve
with the ever-shifting needs of scholars. This infrastructure will, by necessity, be built within the
context of disciplinary conventions, reward systems, and the practice of peer review, all of which
undergird the growth and evolution of superlative academic endeavors.},
	language = {en},
	publisher = {CSHE, UC Berkeley},
	author = {Harley, Diane and Acord, Sophia Krzys and Earl-Novell, Sarah and Lawrence, Shannon and King, C. Judson},
	month = jan,
	year = {2010},
	keywords = {goal\_Dissemination, obj\_ResearchResults}
}

@misc{montgomery_assessing_2011,
	title = {Assessing {Work} on {Digital} {Projects} for {Hiring} and {Tenure}: {Discouraging} {Young} {Scholars}?},
	url = {http://hastac.org/blogs/katherine-f-montgomery/2011/11/27/assessing-work-digital-projects-hiring-and-tenure-discouragi},
	abstract = {One of the longest-running digital projects at the University of Iowa is the Walt Whitman Archive, a rich and constantly-growing interactive collection of Whitman’s life, letters, manuscripts, writing, criticism, recordings, and any and all digital (or digitizable) material on Walt Whitman.},
	language = {en},
	urldate = {2011-11-29},
	journal = {HASTAC Blog},
	author = {Montgomery, Katherine F.},
	month = nov,
	year = {2011}
}

@book{love_attributing_2002,
	address = {Cambridge [u.a.]},
	title = {Attributing authorship : an introduction},
	isbn = {978-0-521-78948-6 978-0-521-78339-2},
	shorttitle = {Attributing authorship},
	url = {http://www.cambridge.org/gb/academic/subjects/literature/literary-theory/attributing-authorship-introduction},
	abstract = {Description
    Contents
    Resources
    Courses
    About the Authors

    Recent literary scholarship has seen a shift of interest away from questions of attribution. Yet these questions remain urgent and important for any historical study of writing, and have been given a powerful new impetus by advances in statistical studies of language and the coming on line of large databases of texts in machine-searchable form. The present book is the first comprehensive survey of the field from a literary perspective to appear for forty years. It covers both traditional and computer based approaches to attribution, and evaluates each in respect of their potentialities and limitations. It revisits a number of famous controversies, including those concerning the authorship of the Homeric poems, books from the Old and New Testaments, and the plays of Shakespeare. Written with wit as well as erudition Attributing Authorship will make this intriguing field accessible for students and scholars alike.},
	language = {en},
	publisher = {Cambridge Univ. Press},
	author = {Love, Harold},
	year = {2002},
	keywords = {AnalyzeStatistically, meta\_GiveOverview, t\_Stylometry}
}

@article{thiel_auf_2011,
	chapter = {Feuilleton},
	title = {Auf dem {Weg} zur digitalen {Großarchitektur}},
	issn = {0174-4909},
	url = {http://www.faz.net/aktuell/feuilleton/forschung-und-lehre/digital-humanities-auf-dem-weg-zur-digitalen-grossarchitektur-11561481.html},
	abstract = {Zentralisiert, standardisiert, europäisiert: Die Geistes- und Sozialwissenschaften überlegen, wie die Strukturen ihrer Forschung dem Wandel der Wissensordnung anzupassen sind.},
	language = {de},
	urldate = {2011-12-16},
	journal = {FAZ.NET},
	author = {Thiel, Thomas},
	month = dec,
	year = {2011},
	keywords = {meta\_Assessing, obj\_DigitalHumanities, obj\_Infrastructures}
}

@article{craig_authorial_1999,
	title = {Authorial attribution and computational stylistics: if you can tell authors apart, have you learned anything about them?},
	volume = {14},
	shorttitle = {Authorial attribution and computational stylistics},
	url = {http://llc.oxfordjournals.org/content/14/1/103.abstract},
	doi = {10.1093/llc/14.1.103},
	abstract = {Within stylometrics, the disciplines of authorial attribution and descriptive stylistics hitherto have been pursued separately. The first has achieved mainstream status within literary studies, while the second is little recognized. A study of the plays of Thomas Middleton compared with a large control sample shows that it is possible to achieve a good classification of his work in 2,000 word segments as against those of his contemporaries, using frequencies of very common words. Such a result can serve as the basis both for testing Middleton's hand in some disputed plays and for a description of his style. Most of The Revenger's Tragedy, part of The Yorkshire Tragedy, and all of The Second Maiden's Tragedy prove to be in a style similar to that of Middleton's uncontested plays. This style, judging by an examination of instances in context of the ten word types most strongly correlated with the Middleton-other discriminant function, is (among other things) rich in deictics and poor in conjunctions, features readily accommodated to previous descriptions including Middleton's own. It is concluded that classification and description can be mutually supportive: the first confirms the validity of the second, while the second helps to establish the stylistic mechanisms underlying a successful classification.},
	language = {en},
	number = {1},
	urldate = {2012-02-08},
	journal = {Literary and Linguistic Computing},
	author = {Craig, Hugh},
	month = apr,
	year = {1999},
	keywords = {AnalyzeStatistically, meta\_Theorizing, bigdata},
	pages = {103 --113}
}

@incollection{hoover_authorial_2010,
	title = {Authorial {Style}},
	abstract = {"Inspired by exploring the language of poems, plays and prose, Mick Short's classic introduction to stylistics, language and style represents the state-of-the-art in literary stylistics and encompasses the full breadth of current research in the discipline. Written by leading scholars in the field, chapters cover a variety of methodological and analytical approaches, from traditional qualitative analysis to more recent developments in cognitive and corpus stylistics. Addressing the three, key literary genres of poetry, drama and narrative, Language and style is divided into carefully balanced sections. Based on original research, each chapter demonstrates a particular analytic technique and explains how this might be applied to a text from one of the literary genres. Framed by helpful introductory material covering the foundational principles of stylistics, the chapters act as practical exemplars of how to carry out stylistic analysis. Comprehensive and engaging, this invaluable resource is essential reading for anyone interested in stylistics"},
	language = {en},
	booktitle = {Language and {Style}: {Essays} in {Honour} of {Mick} {Short}},
	publisher = {Palgrave},
	author = {Hoover, David L. and Mcintyre, Dan and Busse, Beatrix},
	year = {2010},
	pages = {250--271}
}

@article{allison_authorship_2019,
	title = {Authorship {After} {AI}},
	url = {https://www.publicbooks.org/authorship-after-ai/},
	abstract = {Authorship attribution is helpful if you suspect fraud: for instance, if you believe that Shakespeare wasn’t educated enough to write the plays, or that Charlotte Brontë’s Jane Eyre was really ...},
	language = {en-US},
	urldate = {2019-06-27},
	journal = {Public Books},
	author = {Allison, Sarah},
	month = jun,
	year = {2019},
	keywords = {obj\_Literature, act\_StylisticAnalysis, goal\_Analysis, obj\_Persons}
}

@book{juola_authorship_2008,
	title = {Authorship {Attribution}},
	isbn = {978-1-60198-118-9},
	language = {en},
	publisher = {Now Publishers,},
	author = {Juola, Patrick},
	month = mar,
	year = {2008},
	keywords = {AnalyzeStatistically, meta\_GiveOverview, t\_Stylometry}
}

@article{juola_authorship_2006,
	title = {Authorship attribution},
	volume = {1},
	language = {en},
	number = {3},
	journal = {Foundations and Trends in Information Retrieval},
	author = {Juola, Patrick},
	year = {2006},
	keywords = {AnalyzeStatistically, meta\_GiveOverview, *****, t\_Stylometry},
	pages = {233--334}
}

@book{mcgann_new_2014-1,
	address = {Boston, MA},
	title = {A {New} {Republic} of {Letters}. {Memory} and {Scholarship} in the {Age} of {Digital} {Reproduction}},
	url = {http://www.hup.harvard.edu/catalog.php?isbn=9780674728691},
	abstract = {Jerome McGann's manifesto argues that the history of texts and how they are preserved and accessed for interpretation are the overriding subjects of humanist study in the digital age. Theory and philosophy no longer suffice as an intellectual framework. But philology -- out of fashion for decades -- models these concerns with surprising fidelity.},
	language = {en},
	urldate = {2014-03-30},
	publisher = {Harvard Univ. Press},
	author = {McGann, Jerome},
	year = {2014},
	keywords = {meta\_GiveOverview, meta\_Assessing, obj\_DigitalHumanities, bigdata{\textasciitilde}, obj\_Text}
}

@article{piotrowski_aint_2020-1,
	title = {Ain’t {No} {Way} {Around} {It}: {Why} {We} {Need} to {Be} {Clear} {About} {What} {We} {Mean} by “{Digital} {Humanities}”},
	shorttitle = {Ain’t {No} {Way} {Around} {It}},
	author = {Piotrowski, Michael},
	year = {2020}
}

@book{baayen_analyzing_2008-1,
	address = {Cambridge},
	edition = {1. publ.},
	title = {Analyzing linguistic data. {A} practical introduction to statistics using {R}},
	isbn = {978-0-521-70918-7},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Baayen, R Harald},
	year = {2008},
	keywords = {AnalyzeStatistically, meta\_GiveOverview, bigdata, obj\_Language}
}

@misc{beieler_animated_2013-1,
	type = {Blog},
	title = {Animated {Protest} {Mapping} (1979-2013)},
	url = {http://johnbeieler.org/blog/2013/07/31/animated-protest-mapping/},
	abstract = {Many people, including myself, were interested in how protest behavior changes over time. I created an animated protest map (events of every type over time).},
	language = {en},
	journal = {john Beieler},
	author = {Beieler, John},
	year = {2013},
	note = {Political Science},
	keywords = {meta\_GiveOverview, obj\_Events, obj\_Maps}
}

@techreport{schumacher_big_2016,
	address = {Göttingen},
	title = {Big {Data} in den {Geisteswissenschaften}: {Konzept} für eine {Lehr}- und {Lernmittelsammlung}},
	url = {http://webdoc.sub.gwdg.de/pub/mon/dariah-de/dwp-2016-15.pdf},
	abstract = {Dieses Working Paper beschreibt die Inhalte, Darstellungsformen und die medialen Umsetzungsmöglichkeiten einer Lehrmittelsammlung zum Thema „Big Data Methodik in den Geistes- und Kulturwissenschaften”. Zudem wird ein   Disseminationskonzept entwickelt, das aufzeigt, auf welche Weise Inhalte, Themen und Instrumente dieses transdisziplinären Bereichs in
den jeweiligen Fachdisziplinen vermittelt werden können. Das hier   vorgestellte Konzept einer Lehr- und Lernmittelsammlung ist auf die Nutzung und Anwendung von Big Data Technologien
und Methoden für geistes- und kulturwissenschaftliche Forschungsfragen ausgerichtet.},
	language = {de},
	number = {15},
	institution = {DARIAH-DE},
	author = {Schumacher, Mareike and Held, Marcus and Falk, Claudia and Pernes, Stefan},
	year = {2016},
	note = {00000},
	keywords = {obj\_DigitalHumanities, obj\_Methods, obj\_Data, obj\_Humanities},
	pages = {26}
}

@article{schoch_big_2013,
	title = {Big? {Smart}? {Clean}? {Messy}? {Data} in the {Humanities}},
	volume = {2},
	shorttitle = {Data in the {Humanities}},
	url = {http://journalofdigitalhumanities.org/2-3/big-smart-clean-messy-data-in-the-humanities/},
	language = {English},
	number = {3},
	urldate = {2014-09-23},
	journal = {Journal of Digital Humanities},
	author = {Schöch, Christof},
	year = {2013}
}

@book{wright_cataloging_2014,
	address = {Oxford ; New York},
	title = {Cataloging the {World}: {Paul} {Otlet} and the {Birth} of the {Information} {Age}},
	isbn = {978-0-19-993141-5},
	shorttitle = {Cataloging the {World}},
	abstract = {The dream of capturing and organizing knowledge is as old as history. From the archives of ancient Sumeria and the Library of Alexandria to the Library of Congress and Wikipedia, humanity has wrestled with the problem of harnessing its intellectual output. The timeless quest for wisdom has been as much about information storage and retrieval as creative genius.In Cataloging the World, Alex Wright introduces us to a figure who stands out in the long line of thinkers and idealists who devoted themselves to the task. Beginning in the late nineteenth century, Paul Otlet, a librarian by training, worked at expanding the potential of the catalog card, the world's first information chip. From there followed universal libraries and museums, connecting his native Belgium to the world by means of a vast intellectual enterprise that attempted to organize and code everything ever published. Forty years before the first personal computer and fifty years before the first browser, Otlet envisioned a network of "electric telescopes" that would allow people everywhere to search through books, newspapers, photographs, and recordings, all linked together in what he termed, in 1934, a réseau mondial--essentially, a worldwide web.Otlet's life achievement was the construction of the Mundaneum--a mechanical collective brain that would house and disseminate everything ever committed to paper. Filled with analog machines such as telegraphs and sorters, the Mundaneum--what some have called a "Steampunk version of hypertext"--was the embodiment of Otlet's ambitions. It was also short-lived. By the time the Nazis, who were pilfering libraries across Europe to collect information they thought useful, carted away Otlet's collection in 1940, the dream had ended. Broken, Otlet died in 1944.Wright's engaging intellectual history gives Otlet his due, restoring him to his proper place in the long continuum of visionaries and pioneers who have struggled to classify knowledge, from H.G. Wells and Melvil Dewey to Vannevar Bush, Ted Nelson, Tim Berners-Lee, and Steve Jobs. Wright shows that in the years since Otlet's death the world has witnessed the emergence of a global network that has proved him right about the possibilities--and the perils--of networked information, and his legacy persists in our digital world today, captured for all time.},
	language = {en},
	publisher = {Oxford University Press},
	author = {Wright, Alex},
	month = jun,
	year = {2014},
	keywords = {obj\_DigitalHumanities}
}

@incollection{miner_chapter_2012,
	address = {Boston},
	title = {Chapter 2 - {The} {Seven} {Practice} {Areas} of {Text} {Analytics}},
	isbn = {978-0-12-386979-1},
	url = {http://www.sciencedirect.com/science/article/pii/B9780123869791000025},
	abstract = {Prelude

Presently, text mining is in a loosely organized set of competing technologies that function as analytical “city-states” with no clear dominance among them. To further complicate matters, different areas of text mining are in different stages of maturity. Some technology is easily accessible by practitioners today via commercial software (some of which is included with this book), while other areas are only now emerging from academia into the practical realm.

We can relate these technologies to seven different practice areas in text mining that are covered in the chapters in this book. In summary, this book is strongest in the practice area of document classification, solid in concept extraction and document clustering, reasonably useful on web mining, light on information extraction and natural language processing, and almost silent on the (most popular) practice area of search and information retrieval.

The unifying theme behind each of these technologies is the need to “turn text into numbers” so that powerful analytical algorithms can be applied to large document databases. Converting text into a structured, numerical format and applying analytic algorithms both require knowing how to use and combine techniques for handling text, ranging from individual words to documents to entire document databases.

Next, we provide a decision tree to help you determine which practice area is appropriate to satisfy your needs. Finally, we provide tables to relate the practice areas to appropriate technologies and show which chapter in this book deals with that subject area. That is the most organization that we can impose on the current disordered state of text mining technology. Our goal in this book is to provide an introduction to each of the seven practice areas and cover in depth only those areas that are accessible for nonexperts. We will follow that theme in Part I of the book to provide you with the basics you need to perform the tutorials. Very quickly, you will be learning by doing.},
	language = {en},
	urldate = {2012-04-25},
	booktitle = {Practical {Text} {Mining} and {Statistical} {Analysis} for {Non}-structured {Text} {Data} {Applications}},
	publisher = {Academic Press},
	author = {Miner, Gary},
	year = {2012},
	keywords = {bigdata, goal\_Analysis, obj\_Text, act\_ContentAnalysis},
	pages = {29--41}
}

@misc{meeks_comprehending_2011,
	title = {Comprehending the {Digital} {Humanities}},
	url = {https://dhs.stanford.edu/comprehending-the-digital-humanities/},
	language = {en},
	urldate = {2011-05-12},
	journal = {Digital Humanities Specialist},
	author = {Meeks, Elijah},
	month = feb,
	year = {2011},
	keywords = {act\_Visualizing, obj\_DigitalHumanities, act\_Conceptualizing}
}

@article{wisbey_computer_1991,
	title = {Computer und {Philologie} in {Vergangenheit}, {Gegenwart} und {Zukunft}},
	language = {de},
	journal = {Gärtner/Sappler/Trauth 1991},
	author = {Wisbey, Roy},
	year = {1991},
	note = {Wisbey, Roy: Computer und Philologie in Vergangenheit, Gegenwart und Zukunft. In: Gärtner/Sappler/Trauth 1991, S. 346-361.},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities},
	pages = {346--361}
}

@article{jannidis_computerphilologie_1998,
	title = {Computerphilologie},
	language = {de},
	journal = {Metzler Lexikon Literatur- und Kulturtheorie},
	author = {Jannidis, Fotis},
	year = {1998},
	note = {Jannidis, Fotis: Computerphilologie. In: Ansgar Nünning (Hg.): Metzler Lexikon Literatur- und Kulturtheorie. Stuttgart, Weimar 1998, S. 70-72.},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities, goal\_Analysis, goal\_Enrichment},
	pages = {70--72}
}

@incollection{jannidis_computerphilologie_2007,
	address = {Stuttgart},
	title = {Computerphilologie},
	volume = {2 (Methoden und Theorien)},
	language = {de},
	booktitle = {Handbuch {Literaturwissenschaft}},
	publisher = {Metzler},
	author = {Jannidis, Fotis},
	editor = {Anz, Thomas},
	year = {2007},
	keywords = {AnalyzeStatistically, meta\_GiveOverview, obj\_Tools, obj\_DigitalHumanities, *****, act\_Publishing, t\_Encoding, X-CHECK, goal\_Enrichment, x\_astree},
	pages = {27--40}
}

@book{burton_computing_2002,
	address = {Urbana},
	title = {Computing in the {Social} {Sciences} and {Humanities}},
	isbn = {978-0-252-02685-0},
	url = {http://www.press.uillinois.edu/books/catalog/72exq4pd9780252026850.html},
	abstract = {A lively, hands-on introduction for teachers and scholars in the humanities and social sciences, this book-and-CD package will inspire even the faint-hearted to take the technological bull by the horns and make efficient, informed use of computer and Internet resources.

New technology is changing the very nature of research and teaching in the social sciences and humanities. From specialized online forums to Web-based teaching and distance learning, computers are being used to expand educational opportunities, promote cooperation and collaboration, stimulate creative thinking, and find answers to previously insoluble research problems. Combining interactive projects in a CD-ROM format with informative printed essays, this volume showcases innovations that are revolutionizing the craft of scholarship. More than that, it examines realistically how applicable the new technology is to learning. Contributors clarify some of the difficulties of using computers and address problems with the philosophy and culture of computers, including concerns about intellectual property protection and the potential for creating a technological underclass of electronically disadvantaged schools and universities.

The accompanying CD features multimedia entries such as an interactive project on owls that educates users about forest ecology; RiverWeb, an interactive archive of information on the history, culture, and science of the Mississippi River; and "Global Jukebox," which recreates the context in which the folklorist Alan Lomax made his pioneering field recordings. The CD includes links to many external sites on the World Wide Web. For those with limited Internet access, a collection of relevant sites is integrated into the CD. Minimum System Requirements 32 MB available RAM, CD-ROM drive (4x) Some articles and programs also require 256-color monitor and sound capability Macintosh®: System 8.5 Windows®: Intel Pentium® processor (or equivalent) running Windows® 95 or Windows NT® 4.0

Orville Vernon Burton is a professor of history and sociology at the University of Illinois at Urbana-Champaign and professor and senior research scientist at the National Center for Supercomputing Applications at the University of Illinois. He is the author of In My Father's House Are Many Mansions: Family and Community in Edgefield, South Carolina.},
	language = {en},
	publisher = {University of Illinois Press},
	collaborator = {Burton, Orville Vernon},
	year = {2002},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities}
}

@article{berry_critical_2013,
	series = {stunlaw-blog},
	title = {Critical {Digital} {Humanities}},
	url = {http://stunlaw.blogspot.com/2013/01/critical-digital-humanities.html},
	abstract = {Critical Digital Humanities is an approach to the study and use of the digital which is attentive to questions of power, domination, myth and exploitation, what has been called the "The Dark Side of the Digital Humanities" (Chun 2013; Grusin 2013; Jagoda 2013; Raley 2013).},
	language = {en},
	author = {Berry, David M.},
	month = nov,
	year = {2013},
	keywords = {meta\_Assessing, obj\_DigitalHumanities, meta\_Theorizing}
}

@techreport{jannidis_dariah-dkpro-wrapper_2016,
	address = {Göttingen},
	title = {{DARIAH}-{DKPro}-{Wrapper} {Output} {Format} ({DOF}) {Specification}},
	url = {urn:nbn:de:gbv:7-dariah-2016-6-2},
	abstract = {The DARIAH-DKPro-Wrapper Output Format (DOF) is a tab-separated file format, designed to be easily accessible by various analysis tools and scripting languages. It is based on a modular linguistic processing pipeline, which includes a range of analysis capabilities and has been fitted to book-length documents. Both processing pipeline and output format have been developed in cooperation by
the Department of Literary Computing, Würzburg (“Lehrstuhl Für Computerphilologie Und Neuere
Deutsche Literaturgeschichte. Universität Würzburg”
2016) and the Ubiquitous Knowledge Processing Lab, Darmstadt (“Ubiquitous Knowledge Processing Lab. Technische Universität Darmstadt” 2016) as
part of DARIAH-DE, Digital Research Infrastructure for the Arts and Humanities (“DARIAH-DE” 2016].},
	language = {en},
	number = {20},
	institution = {DARIAH-DE},
	author = {Jannidis, Fotis and Pernes, Stefan and Pielström, Steffen and Reger, Isabella and Reimers, Nils and Vitt, Thorsten},
	year = {2016},
	note = {00000},
	keywords = {obj\_DigitalHumanities, obj\_Humanities},
	pages = {14}
}

@techreport{beer_datenlizenzen_2014,
	address = {Göttingen},
	title = {Datenlizenzen für geisteswissenschaftliche {Forschungsdaten} - {Rechtliche} {Bedingungen} und {Handlungsbedarf}},
	url = {http://webdoc.sub.gwdg.de/pub/mon/dariah-de/dwp-2014-6.pdf},
	abstract = {Der vorliegende Beitrag schildert den Diskussionsstand zu Lizenzen für geisteswissenschaftliche Forschungsdaten, gibt einen Überblick über deren urheberrechtliche Grundlagen, zeigt Fallbeispiele aus den
Altertumswissenschaften und schließt mit einer Evaluation gängiger Werkzeuge
zur Lizenzerstellung.},
	language = {de},
	number = {6},
	institution = {DARIAH-DE},
	author = {Beer, Nikolaos and Herold, Kristin and Heinrich, Maurice and Kolbmann, Wibke and Kollatz, Thomas and Romanello, Matteo and Rose, Sebastian and Schäfer, Felix Falco and Walkowski, Niels-Oliver},
	year = {2014},
	note = {00000},
	keywords = {obj\_DigitalHumanities, obj\_Research, obj\_Data, obj\_Humanities},
	pages = {49}
}

@book{terras_defining_2013,
	address = {Williston},
	title = {Defining {Digital} {Humanities} - {A} {Reader}},
	url = {http://www.ashgate.com/isbn/9781409469636},
	abstract = {Digital Humanities is becoming an increasingly popular focus of academic endeavour. There are now hundreds of Digital Humanities centres worldwide and the subject is taught at both postgraduate and undergraduate level. Yet the term ‘Digital Humanities’ is much debated. This reader brings together, for the first time, in one core volume the essential readings that have emerged in Digital Humanities. We provide a historical overview of how the term ‘Humanities Computing’ developed into the term ‘Digital Humanities’, and highlight core readings which explore the meaning, scope, and implementation of the field. To contextualize and frame each included reading, the editors and authors provide a commentary on the original piece. There is also an annotated bibliography of other material not included in the text to provide an essential list of reading in the discipline. This text will be required reading for scholars and students who want to discover the history of Digital Humanities through its core writings, and for those who wish to understand the many possibilities that exist when trying to define Digital Humanities.},
	language = {en},
	publisher = {Ashgate},
	editor = {Terras, Melissa and Nyhan, Julianne and Vanhoutte, Edward},
	year = {2013},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities}
}

@techreport{bock_einsatz_2016,
	address = {Göttingen},
	title = {Der {Einsatz} quantitativer {Textanalyse} in den {Geisteswissenschaften}: {Bericht} über den {Stand} der {Forschung}},
	url = {http://webdoc.sub.gwdg.de/pub/mon/dariah-de/dwp-2016-18.pdf},
	abstract = {Der Beitrag beschreibt den Forschungsstand zum Einsatz quantitativer Verfahren der Textanalyse in den Geisteswissenschaften. Dabei werden für zentrale Verfahren der stilistischen Analyse und themati-
schen Erschließung von Textbeständen jeweils die grundlegenden Konzepte und Intuitionen zu ihrer Wirkungsweise, ihre Anfänge und der heutige Entwicklungsstand beschrieben.},
	language = {de},
	number = {18},
	institution = {DARIAH-DE},
	author = {Bock, Sina and Du, Keli and Huber, Michael and Pernes, Stefan and Pielström, Steffen},
	year = {2016},
	note = {00000},
	keywords = {obj\_DigitalHumanities, obj\_Methods, obj\_Text, Object: Texts, obj\_Humanities},
	pages = {19}
}

@techreport{sahle_dh_2013,
	address = {Göttingen},
	title = {{DH} {Studieren}! {Auf} dem {Weg} zu einem {Kern}- und {Referenzcurriculum} der {Digital} {Humanities}},
	url = {http://webdoc.sub.gwdg.de/pub/mon/dariah-de/dwp-2013-1.pdf},
	abstract = {Der Status der Digital Humanities als eigenständiger wissenschaftlicher Bereich ist seit
langem in der Diskussion. Ein wichtiger Aspekt in der fortschreitenden Verfestigung als
Disziplin oder Fach ist die Vermittlung von DH in der Lehre. Dies geschieht bereits auf allen Ebenen der Ausbildung: von einzelnen Kursen und Modulen, über angestimmte Angebote, Zertifikate und Summer Schools, bis hin zu BA-, MA- und Promotionsstudiengängen.
Für den Erfolg dieser Lehrprogramme, für die Verwertbarkeit der Abschlüsse für die Studierenden und die Attraktivität der Absolventen auf dem Arbeitsmarkt ist ihre
Erkennbarkeit als DH-Ausbildungen mit ganz bestimmten Lehrinhalten und Zielkompetenzen von besonderer Bedeutung. Kern- und Referenzcurricula für die verschiedenen
Spielarten von DH-Programmen wären ein geeignetes Mittel, um die Kohärenz der
Ausbildungen zu verbessern und sichtbar zu machen. Auf dem Weg dorthin werden
zunächst eine empirische Sichtung der bestehenden Angebote, ein analytisches Raster zu ihrer Untersuchung, ein Modell der verschiedenen Grundtypen, eine erste Zusammenstellung der typischen Lehrinhalte und Zielkompetenzen sowie Überlegungen zum
Aufbau neuer Studienprogramme benötigt. Das vorliegende Papier versucht in diesem Sinne die Grundlagen für gemeinsame Kern- und Referenzcurricula zu legen, die dann innerhalb der Fachgemeinschaft der DH-Ausbildung weiter diskutiert werden müssen.
Der vorliegende Bericht enthält außerdem im Anhang ein Konzeptpapier von Manfred Thaller: “Vorüberlegungen zu einem Referenzcurriculum, das Zwiebelschalenmodell und
der Nürnberger Informatikkern”.},
	language = {de},
	number = {1},
	institution = {DARIAH-DE},
	author = {Sahle, Patrick},
	year = {2013},
	note = {00002},
	keywords = {obj\_DigitalHumanities, obj\_Humanities},
	pages = {39}
}

@book{hahn_dh-handbuch_2015,
	address = {Göttingen},
	title = {{DH}-{Handbuch} /{Version} 1.0},
	copyright = {https://creativecommons.org/licenses/by/4.0/deed.de},
	isbn = {978-3-7375-6818-0},
	url = {https://handbuch.tib.eu/w/DH-Handbuch},
	abstract = {Ziel dieses Buchs ist, einen konzentrierten Überblick über das Feld der Digital Humanities (DH) anzubieten. Für Einsteiger und mögliche AntragstellerInnen stellen sich häufig die folgenden Fragen:

    Was sind die Digital Humanities?
    Was sind relevante Forschungsfragen?
    Mithilfe welcher Tools lassen sich fachspezifische, aber auch fächerübergreifende Fragen beantworten?
    Was müssen Geisteswissenschaftler beim Umgang mit Daten beachten?
    Wie sehen erfolgreiche Projekte in den Digital Humanities aus? 

Neben Lösungswegen und Ressourcen zu typischen Fragen werden auch Projekte und Werkzeuge detailliert vorgestellt, um vorhandene Kenntnisse aufzufrischen und neue Aspekte der Digital Humanities kennenzulernen. Die Nähe zur fachwissenschaftlichen Praxis steht dabei im Vordergrund. Wir hoffen, mit diesem Handbuch auch Einsteigern die Digital Humanities nahebringen zu können und die Neugierde auf digitale Methoden und deren Möglichkeiten für die geisteswissenschaftliche Forschung zu wecken.},
	language = {de},
	publisher = {DARIAH-DE},
	author = {Hahn, Helene and Kalman, Tibor and Kolbmann, Wibke and Kollatz, Thomas and Neuschäfer, Steffen and Pielström, Steffen and Puhl, Johanna and Stiller, Juliane and Tonne, Danah},
	year = {2015},
	note = {pdf: http://bit.do/DH-Handbuch},
	keywords = {obj\_DigitalHumanities, act\_Communicating}
}

@techreport{maier_erstellung_2015,
	address = {Göttingen},
	title = {Die {Erstellung} eines {TEI}-{Metadatenschemas} für die {Auszeichnung} von {Texten} des {Klassischen} {Maya}},
	url = {urn:nbn:de:gbv:7-dariah-2015-1-6},
	abstract = {Im Rahmen des 2014 angelaufenen Projektes, Textdatenbank und Wörterbuch des Klassischen Maya (TWKM) der Universität Bonn, das die Erforschung und vollständige Entzifferung der Schrift und Sprache des Klassischen Maya zum Ziel hat, werden alle überlieferten Schriftenträger des Klassischen Maya erschlossen und systematisch dokumentiert. Hierfür wurden in einem Teil-Projekt die Inschriftentexte mittels des Datenformats der Text Encoding Initiative (TEI) erfasst: Wesentliche Informationen, die durch die Metadaten ausgezeichnet werden mussten, waren die unterschiedlichen Formen und Ausgestaltungen der Inschriftentexte, sowie die Zeitschrift selbst. Das TEI-Metadatenschema dieses Teilprojekts stellte eine Arbeitsgrundlage dar, die im weiteren Projektverlauf angepasst und ergänzt werden konnte.},
	language = {de},
	number = {8},
	institution = {DARIAH-DE},
	author = {Maier, Petra},
	year = {2015},
	note = {00000},
	keywords = {obj\_DigitalHumanities, obj\_Text, obj\_Humanities, obj\_Metadata},
	pages = {27}
}

@book{piatti_geographie_2008,
	address = {Göttingen},
	title = {Die {Geographie} der {Literatur} : {Schauplätze}, {Handlungsräume}, {Raumphantasien}},
	isbn = {978-3-8353-0329-4},
	shorttitle = {Die {Geographie} der {Literatur}},
	url = {http://www.wallstein-verlag.de/9783835303294-barbara-piatti-die-geographie-der-literatur.html},
	abstract = {Konzepte einer künftigen Literaturgeographie, die literarische Schauplätze zum Ausgangspunkt der Textanalysen macht.

Wo spielt Literatur? Die vermeintlich simple Frage eröffnet ein erst in Ansätzen etabliertes Forschungsgebiet mit neuen methodischen Zugängen unter dem Stichwort »Literaturgeographie«.
Jede literarische Handlung ist irgendwo lokalisiert, wobei die Skala von gänzlich imaginären bis zu realistisch gezeichneten Schauplätzen mit hohem Wiedererkennungswert reicht. Die Literaturgeographie rückt die vielfältigen Bezugnahmen von Räumen der Fiktion auf den Realraum hin ins Zentrum der Aufmerksamkeit: Literatur weist eine spezifische Geographie auf, die ganz eigenen Regeln folgt. Denn fiktionale Räume sind niemals nur mimetische Abbilder der Realität, auch wenn sie sich auf existierende Landschaften und Städte beziehen. Vielmehr müssen die poetologischen Verfahren von Verfremdung, Überblendung, Neubenennung, die Kombinationsmöglichkeiten von realen Orten mit fiktiven Elementen in Visualisierungskonzepte und Deutungen der Textanalysen einfließen.
Diese Theorie findet zunächst Anwendung auf eine an literarischen Schauplätzen überreiche Modellregion: auf den Vierwaldstättersee und das Gotthardmassiv in der Zentralschweiz. Im Anschluss wird der methodische Horizont für einen Literaturatlas aufgespannt - und das Potenzial literatur­geographischer Konzepte im Hinblick auf eine vergleichende europäische Literaturgeschichte aufgezeigt.
17 beigefügte vierfarbige Faltkarten ermöglichen die differenzierte Gegenüberstellung von fiktionalen und realen Landschaften.},
	language = {de},
	publisher = {Wallstein},
	author = {Piatti, Barbara},
	year = {2008},
	keywords = {act\_Visualizing, obj\_Literature}
}

@book{cohen_digital_2005,
	address = {Philadelphia, PA},
	title = {Digital {History}: {A} {Guide} to {Gathering}, {Preserving}, and {Presenting} the {Past} on the {Web}},
	url = {http://chnm.gmu.edu/digitalhistory/},
	abstract = {his book provides a plainspoken and thorough introduction to the web for historians—teachers and students, archivists and museum curators, professors as well as amateur enthusiasts—who wish to produce online historical work, or to build upon and improve the projects they have already started in this important new medium. It begins with an overview of the different genres of history websites, surveying a range of digital history work that has been created since the beginning of the web. The book then takes the reader step-by-step through planning a project, understanding the technologies involved and how to choose the appropriate ones, designing a site that is both easy-to-use and scholarly, digitizing materials in a way that makes them web-friendly while preserving their historical integrity, and how to reach and respond to an intended audience effectively. It also explores the repercussions of copyright law and fair use for scholars in a digital age, and examines more cutting-edge web techniques involving interactivity, such as sites that use the medium to solicit and collect historical artifacts. Finally, the book provides basic guidance on insuring that the digital history the reader creates will not disappear in a few years.},
	language = {en},
	urldate = {2009-11-19},
	publisher = {University of Pennsylvania Press},
	author = {Cohen, Daniel J. and Rosenzweig, Roy},
	year = {2005},
	keywords = {meta\_GiveOverview, act\_Publishing, obj\_Documents, act\_Archiving, goal\_Capture}
}

@book{burdick_digital_2012,
	address = {Cambridge, MA},
	title = {Digital {Humanities}},
	isbn = {978-0-262-01847-0},
	url = {http://www.amazon.com/Digital_Humanities-Anne-Burdick/dp/0262018470},
	abstract = {Digital\_Humanities is a compact, game-changing report on the state of contemporary knowledge production. Answering the question, "What is digital humanities?," it provides an in-depth examination of an emerging field. This collaboratively authored and visually compelling volume explores methodologies and techniques unfamiliar to traditional modes of humanistic inquiry--including geospatial analysis, data mining, corpus linguistics, visualization, and simulation--to show their relevance for contemporary culture.

Included are chapters on the basics, on emerging methods and genres, and on the social life of the digital humanities, along with "case studies," "provocations," and "advisories." These persuasively crafted interventions offer a descriptive toolkit for anyone involved in the design, production, oversight, and review of digital projects. The authors argue that the digital humanities offers a revitalization of the liberal arts tradition in the electronically inflected, design-driven, multimedia language of the twenty-first century.Written by five leading practitioner-theorists whose varied backgrounds embody the intellectual and creative diversity of the field, Digital\_Humanities is a vision statement for the future, an invitation to engage, and a critical tool for understanding the shape of new scholarship.},
	language = {en},
	publisher = {MIT Press},
	editor = {Burdick, Anne and Lunenfeld, Peter and Drucker, Johanna and Presner, Todd and Schnapp, Jeffrey},
	year = {2012},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities}
}

@article{adams_digital_2012,
	title = {Digital {Humanities} - {Where} to {Start}},
	volume = {73},
	issn = {0099-0086, 2150-6698},
	url = {http://crln.acrl.org/content/73/9/536},
	language = {en},
	number = {9},
	urldate = {2012-10-03},
	journal = {College \& Research Libraries News},
	author = {Adams, Jennifer L. and Gunn, Kevin B.},
	month = oct,
	year = {2012},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities},
	pages = {536--569}
}

@book{warwick_digital_2012,
	address = {London},
	edition = {1.},
	title = {Digital {Humanities} in {Practice}},
	isbn = {978-1-85604-766-1},
	url = {http://www.amazon.de/Digital-Humanities-Practice-Claire-Warwick/dp/1856047660},
	abstract = {This title offers a cutting-edge and comprehensive introduction to this vibrant and increasingly important global field drawing together a broad spectrum of disciplines. Each chapter interweaves the expert commentary of leading academics, analysis of current research and practice and several exciting international case studies, exploring the possibilities and challenges that occur when culture and digital technologies intersect. It covers key topics that include: social media and crowd sourcing; digital images and digitisation; 3D scanning and museums; studying users and readers; electronic text and corpora; archaeology and GIS; open access and online teaching of digital humanities; and, books, texts and digital editing. This is an essential practical guide for academics, researchers, librarians and professionals involved in the digital humanities. It will also be core reading for all humanities students and those taking courses in the digital humanities in particular.},
	language = {en},
	publisher = {Facet Publishing},
	author = {Warwick, Claire and Terras, Melissa and Nyhan, Julianne},
	year = {2012},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities, X-CHECK}
}

@article{suarez_digital_2014,
	title = {Digital {Humanities} in {Spanish}? (¿{Humanidades} digitales en español?, 2010)},
	url = {http://dayofdh2014.matrix.msu.edu/redhd/2014/04/08/digital-humanities-in-spanish/},
	language = {en},
	journal = {RedHD en Traducción},
	author = {Suárez, Juan Luis},
	translator = {Ortega, Élika},
	year = {2014},
	keywords = {meta\_Assessing, obj\_DigitalHumanities}
}

@techreport{maier_digital_2016,
	address = {Göttingen},
	title = {Digital {Humanities} und {Bibliothek} als {Kooperationspartner}},
	url = {urn:nbn:de:gbv:7-dariah-2016-5-6},
	abstract = {Wissenschaftliche Bibliotheken haben traditionell die Aufgabe, die Wissenschaft hinsichtlich der In-
formationsversorgung und -beschaffung zu unterstützen. Durch die digitalen Entwicklungen und der
Ausdifferenzierung der sogenannten Digital Humanities (DH) hat sich das Verständnis dessen, was
Information ist, gewandelt: Das Arbeiten mit digitalen Daten in der Wissenschaft gehört heute zum
Alltag. Hierdurch sind Bibliotheken gefordert, ihr Selbstverständnis und das Aufgabenprofil anzupassen.
In der bibliothekarischen Fachwelt werden seit längerem genau dieses Selbstverständnis sowie das
eigene Berufsbild stark und vor allem kontrovers diskutiert. Diese Diskussion wird aufgegriffen, und
herausgearbeitet, welche Faktoren als Grundlage für die praktische Unterstützung der Digital Humanities
durch wissenschaftliche Bibliotheken geschaffen werden müssen.},
	language = {de},
	number = {19},
	institution = {DARIAH-DE},
	author = {Maier, Petra},
	year = {2016},
	note = {00000},
	keywords = {obj\_DigitalHumanities, obj\_Research, obj\_Humanities, obj\_Metadata},
	pages = {29}
}

@article{sahle_digital_2015,
	title = {Digital {Humanities}? {Gibt}’s doch gar nicht!},
	url = {http://www.zfdg.de/sb001_004},
	abstract = {Die Digital Humanities sind entweder ein Forschungsfeld oder eine Disziplin, möglicherweise auch beides. Sie verfügen jedenfalls über eine gut ausgebaute Infrastruktur der Organisation, Information und Kommunikation und blicken in Bezug auf ihre vielfältigen Forschungsansätze teilweise auf lange Traditionen zurück. Als Bindeglied zwischen den Geisteswissenschaften und der Informatik scheint das Feld heute nicht nur für diese beiden Bereiche, sondern auch für die Organe der Forschungsförderung von besonders hoher Attraktivität zu sein. Neben der Wissenschaft haben selbst die Massenmedien in den letzten Jahren die Digital Humanities entdeckt. Die hohe Anziehungskraft des Feldes hat erfreulich integrative Tendenzen. Allerdings birgt dieser DH-›Hype‹ auch Gefahren. Diese reichen von der bloßen Aneignung des Etiketts über explizite Abwehrhaltungen bis hin zu Ignoranz und Verleugnung: »DH? Das gibt es doch gar nicht!«},
	urldate = {2015-09-16},
	journal = {ZfdG},
	author = {Sahle, Patrick},
	year = {2015},
	keywords = {obj\_DigitalHumanities, activity: Assess}
}

@book{jannidis_digital_2017,
	title = {Digital {Humanities}. {Eine} {Einführung}},
	shorttitle = {Digital {Humanities}},
	url = {https://www.springer.com/de/book/9783476026224},
	abstract = {Computerbasierte Verfahren greifen in viele Bereiche der Geistes- und Kulturwissenschaften ein und spielen eine zunehmende Rolle in der universitären Bildung. Dieser Band bietet eine fundierte Einführung in die grundlegenden Konzepte, Methoden und Werkzeuge der Digital Humanities. Sie präsentiert Grundlagen wie Digitalisierung, Aufbau von Datensammlungen, Datenmodellierung und XML. Darüber hinaus behandelt sie Anwendungsgebiete wie Digitale Edition, Information Retrieval, Netzwerkanalyse, Geographische Informationssysteme, Simulation ebenso weiterführende Aspekte wie die Rolle der Bibliotheken, Archive und Museen sowie rechtliche und ethische Fragen.},
	language = {De},
	publisher = {Metzler},
	author = {Jannidis, Fotis and Kohle, Hubertus and Rehbein, Malte},
	year = {2017}
}

@book{hoover_digital_2014,
	address = {London},
	title = {Digital {Literary} {Studies}: {Corpus} {Approaches} to {Poetry}, {Prose}, and {Drama}},
	isbn = {978-0-415-35230-7},
	url = {http://www.amazon.de/Digital-Literary-Studies-Approaches-Linguistics/dp/0415352304},
	abstract = {Digital Literary Studies presents a broad and varied picture of the promise and potential of methods and approaches that are crucially dependent upon the digital nature of the literary texts it studies and the texts and collections of texts with which they are compared. It focuses on style, diction, characterization, and interpretation of single works and across larger groups of texts, using both huge natural language corpora and smaller, more specialized collections of texts created for specific tasks, and applies statistical techniques used in the narrower confines of authorship attribution to broader stylistic questions. It addresses important issues in each of the three major literary genres, and intentionally applies different techniques and concepts to poetry, prose, and drama. It aims to present a provocative and suggestive sample intended to encourage the application of these and other methods to literary studies. Hoover, Culpeper, and O'Halloran push the methods, techniques, and concepts in new directions, apply them to new groups of texts or to new questions, modify their nature or method of application, and combine them in innovative ways},
	language = {en},
	publisher = {Routledge},
	author = {Hoover, David L. and Culpeper, Jonathan and O'Halloran, Kieran},
	year = {2014},
	keywords = {meta\_GiveOverview, obj\_Text}
}

@book{hayles_digital_2012,
	address = {Chicago},
	title = {Digital {Media} and contemporary technogenesis},
	isbn = {978-0-226-32142-4},
	url = {http://press.uchicago.edu/ucp/books/book/chicago/H/bo5437533.html},
	abstract = {Hayles examines the evolution of the field from the traditional humanities and how the digital humanities are changing academic scholarship, research, teaching, and publication. She goes on to depict the neurological consequences of working in digital media, where skimming and scanning, or “hyper reading,” and analysis through machine algorithms are forms of reading as valid as close reading once was. Hayles contends that we must recognize all three types of reading and understand the limitations and possibilities of each.},
	language = {en},
	publisher = {University of Chicago Press},
	author = {Hayles, N. Katherine},
	year = {2012},
	keywords = {obj\_DigitalHumanities, meta\_Theorizing, obj\_Research, act\_Conceptualizing, goal\_Interpretation, obj\_Humanities}
}

@book{rogers_digital_2013,
	address = {Cambridge, MA},
	title = {Digital methods},
	isbn = {978-0-262-01883-8},
	url = {http://www.amazon.com/Digital-Methods-Richard-Rogers/dp/0262018837},
	abstract = {In Digital Methods, Richard Rogers proposes a methodological outlook for social and cultural scholarly research on the Web that seeks to move Internet research beyond the study of online culture. It is not a toolkit for Internet research, or operating instructions for a software package; it deals with broader questions. How can we study social media to learn something about society rather than about social media use? How can hyperlinks reveal not just the value of a Web site but the politics of association? Rogers proposes repurposing Web-native techniques for research into cultural change and societal conditions. We can learn to reapply such "methods of the medium" as crawling and crowd sourcing, PageRank and similar algorithms, tag clouds and other visualizations; we can learn how they handle hits, likes, tags, date stamps, and other Web-native objects. By "thinking along" with devices and the objects they handle, digital research methods can follow the evolving methods of the medium. Rogers uses this new methodological outlook to examine the findings of inquiries into 9/11 search results, the recognition of climate change skeptics by climate-change-related Web sites, the events surrounding the Srebrenica massacre according to Dutch, Serbian, Bosnian, and Croatian Wikipedias, presidential candidates' social media "friends," and the censorship of the Iranian Web. With Digital Methods, Rogers introduces a new vision and method for Internet research and at the same time applies them to the Web's objects of study, from tiny particles (hyperlinks) to large masses (social media).},
	language = {en},
	publisher = {MIT Press},
	author = {Rogers, Richard},
	year = {2013},
	keywords = {bigdata{\textasciitilde}}
}

@book{haber_digital_2011,
	address = {München},
	title = {Digital {Past}. {Geschichtswissenschaften} im digitalen {Zeitalter}},
	isbn = {978-3-486-70704-5},
	url = {http://www.amazon.de/Digital-Past-Geschichtswissenschaft-digitalen-Zeitalter/dp/3486707043},
	abstract = {Digitale Ressourcen, multimediale Methoden und das Werkzeug Computer bestimmen scheinbar selbstverständlich die tägliche Arbeit des Historikers, der Historikerin. Peter Haber geht der Frage nach, wie die Digitalisierung der Welt die Wissenschaft von der Vergangenheit verändert. Wie wird Geschichte im 21. Jahrhundert geschrieben? Wie wird unser Wissen neu geordnet? Welche Macht hat Google? Eines ist offensichtlich: die Medienrevolutionen des Web und des Web 2.0 wirken massiv auf das Fach Geschichte und auf die Geschichtswissenschaft ein. Haber macht die Veränderungen der letzten Jahrzehnte greifbar und wagt einen Blick auf neue Perspektiven für die Wissenschaft von der Vergangenheit. "Peter Habers 'Digital Past' sollte Pflichtlektüre für alle sein, die sich für die digitale Geschichtswissenschaft in der Vergangenheit, Gegenwart und Zukunft interessieren." Mills Kelly, Assoziierter Direktor des Center for History and New Media an der George Mason University},
	language = {de},
	publisher = {Oldenbourg Wissenschaftsverlag},
	author = {Haber, Peter},
	year = {2011},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities}
}

@techreport{walkowski_digital_2016,
	address = {Göttingen},
	title = {Digital publications beyond digital communication},
	url = {urn:nbn:de:gbv:7-dariah-2016-3-4},
	abstract = {Since more than twenty years different stakeholders involved in scholarly publishing have tried to fundamentally rethink the shape of publications in a digital environment. In contrast to these abundant activities with their highly experimental character the same stakeholders continuously regret that the dominant form of digital publications is still the PDF. In the research literature, the situation of digital publications is often compared with the era of the printing press. While this comparison might be
helpful to create awareness about the dimension in which changes are taking place it certainly also blurs significant differences. In the situation that was described before it is even more important to identify the peculiarities of the process of change. Fortunately, the story of digital scholarly publications is long enough to tell it in a way in which these peculiarities become more transparent. The study at hand is an attempt to do so. At its end, it becomes clear that the major issue in digital publishing is not so much connected to the questions how publications look digitally but what we may perceive as a
publication in a digital environment.},
	language = {en},
	number = {17},
	institution = {DARIAH-DE},
	author = {Walkowski, Niels-Oliver},
	year = {2016},
	note = {00000},
	keywords = {obj\_DigitalHumanities, act\_Publishing, obj\_Humanities, act\_Communicating},
	pages = {23}
}

@book{bodard_digital_2010,
	address = {Farnham, Surrey, England},
	series = {Digital research in the arts and humanities},
	title = {Digital {Research} in the {Study} of {Classical} {Antiquity}},
	isbn = {978-0-7546-7773-4},
	url = {http://www.ashgate.com/isbn/9780754677734},
	abstract = {This book explores the challenges and opportunities presented to Classical scholarship by digital practice and resources. Drawing on the expertise of a community of scholars who use innovative methods and technologies, it shows that traditionally rigorous scholarship is as central to digital research as it is to mainstream Classical Studies. The chapters in this edited collection cover many subjects, including text and data markup, data management, network analysis, pedagogical theory and the Social and Semantic Web, illustrating the range of methods that enrich the many facets of the study of the ancient world. This volume exemplifies the collaborative and interdisciplinary nature that is at the heart of Classical Studies.},
	language = {en},
	publisher = {Ashgate},
	collaborator = {Bodard, Gabriel and Mahony, Simon},
	year = {2010},
	keywords = {meta\_GiveOverview, X-CHECK, obj\_Artefacts, obj\_AnyObject}
}

@book{edmond_digital_2020,
	title = {Digital {Technology} and the {Practices} of {Humanities} {Research}},
	isbn = {978-1-78374-839-6 978-1-78374-840-2 978-1-78374-841-9 978-1-78374-842-6 978-1-78374-843-3 978-1-78374-844-0},
	url = {https://www.openbookpublishers.com/product/1108},
	language = {en},
	urldate = {2020-02-04},
	publisher = {Open Book Publishers},
	editor = {Edmond, Jennifer},
	month = feb,
	year = {2020},
	doi = {10.11647/obp.0192},
	keywords = {meta\_GiveOverview, act\_Publishing, obj\_Research, goal\_Dissemination},
	file = {Edmond_2020_Digital Technology and the Practices of Humanities Research.pdf:/home/lisnux/Zotero/storage/5R7T5PN8/Edmond_2020_Digital Technology and the Practices of Humanities Research.pdf:application/pdf}
}

@book{gasteiner_digitale_2010,
	address = {Wien},
	title = {Digitale {Arbeitstechniken} für die {Geistes}- und {Kulturwissenschaften}},
	isbn = {978-3-8252-3157-6},
	url = {http://www.utb-shop.de/digitale-arbeitstechniken.html},
	abstract = {Möglichkeiten und Anwendungsgebiete digitaler Arbeitstechniken Das Buch vermittelt Kenntnisse und Kompetenzen für fortgeschrittene Studierende, Dozierende sowie Forschende in den Geisteswissenschaften im Umgang mit Neuen Medien, Open Access und Digitalen Publikationspraktiken. Es orientiert über Rechte und Pflichten im Umgang mit digitalen Texten und Bildern, behandelt die unterschiedlichen Situationen in den deutschsprachigen Ländern und weist weiterführende Literatur und Quellen zu diesen Themen nach.},
	language = {de},
	publisher = {UTB},
	editor = {Gasteiner, Martin and Haber, Peter},
	year = {2010},
	keywords = {meta\_GiveOverview, meta\_Assessing, act\_Publishing, t\_Encoding, X-CHECK, obj\_Methods}
}

@book{kohle_digitale_2013,
	address = {Glückstadt},
	title = {Digitale {Bildwissenschaft}},
	copyright = {info:eu-repo/semantics/openAccess},
	url = {http://archiv.ub.uni-heidelberg.de/artdok/2185/},
	abstract = {Das Digitale hat seinen Einzug in die Kulturwissenschaften gehalten. Aber was kann es zur Deutung von Bildern, gar von Kunstwerken beitragen? 
Eine Antwort darauf versucht das Buch „Digitale Bildwissenschaft“. Sein Autor behauptet nicht zuallererst, dass man mit dem Computer viel schneller wissenschaftliche Fragen abhandeln kann, sondern dass sich vor allem Methodiken des Analysierens, Publizierens und Bewertens ändern. 
Für klassische Geisteswissenschaftler sind solche Wandlungen nicht leicht zu verdauen. Wird eine digitale Bildgeschichte überhaupt noch Geschichtsschreibung sein?},
	language = {de},
	urldate = {2014-05-07},
	publisher = {Hülsbusch},
	author = {Kohle, Hubertus},
	year = {2013},
	keywords = {meta\_GiveOverview}
}

@book{schmale_digitale_2010,
	address = {Wien},
	title = {Digitale {Geschichtswissenschaft}},
	isbn = {978-3-205-78553-8},
	url = {http://www.boehlau-verlag.com/978-3-205-78553-8.html},
	abstract = {Digitale Geschichtswissenschaft ist mehr als nur der Einsatz digitaler Techniken und Medien, sie ist mehr als EDV, Web oder Internet. Sie eröffnet neue Forschungs- und Darstellungsmöglichkeiten, sie antwortet auf die liquiden Eigenschaften unserer gegenwärtigen Netzwerkzivilisation. Digitale Geschichtswissenschaft ersetzt dabei nicht die traditionelle monographische Geschichtswissenschaft, sondern verhilft dieser zu einer Renaissance im Feld der großen historischen Erzählung, die nicht obsolet ist. Digitale Geschichtswissenschaft gewinnt ihr Profil aus den vielfältigen Möglichkeiten der semantischen Techniken und der Forschungsvernetzung, sie stellt auch dem Nichthistoriker Techniken des Geschichtelernens von hoher Qualität zur Verfügung. Geschichte passiert auch im Netz - Geschichtswissenschaft wird wieder spannend.},
	language = {de},
	publisher = {Böhlau},
	author = {Schmale, Wolfgang},
	year = {2010},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities, X-CHECK}
}

@misc{kuczera_digitale_2014,
	title = {Digitale {Perspektiven} mediävistischer {Quellenrecherche}},
	url = {http://mittelalter.hypotheses.org/3492},
	language = {de},
	urldate = {2014-05-08},
	journal = {Mittelalter},
	author = {Kuczera, Andreas},
	month = apr,
	year = {2014},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities}
}

@techreport{puhl_diskussion_2015,
	address = {Göttingen},
	title = {Diskussion und {Definition} eines {Research} {Data} {LifeCycle} für die digitalen {Geisteswissenschaften}},
	url = {urn:nbn:de:gbv:7-dariah-2015-4-4},
	abstract = {Das vorliegende Dokument beschreibt den aktuellen Diskussionsstand über ein Referenzmodell eines Forschungsdatenzyklus in den digitalen Geisteswissenschaften. 
Nach einer Bedarfserläuterung
werden Begriffe, Funktionen und Abläufe eines solchen Datenzyklus näher analysiert und definiert.
Das Dokument schließt mit einem   Referenzmodell sowie einem Ausblick auf weiteren Evaluierungsbedarf und daraus resultierende Pläne in dem Forschungsprojekt DARIAH-DE.},
	language = {de},
	number = {11},
	institution = {DARIAH-DE},
	author = {Puhl, Johanna and Andorfer, Peter and Höckendorff, Mareike and Schmunk, Stefan and Stiller, Juliane and Thoden, Klaus},
	year = {2015},
	note = {00002},
	keywords = {obj\_DigitalHumanities, obj\_Research, obj\_Humanities, obj\_Metadata},
	pages = {53}
}

@book{moretti_distant_2013,
	address = {London},
	title = {Distant reading},
	isbn = {978-1-78168-084-1},
	url = {http://www.amazon.de/Distant-Reading-Franco-Moretti/dp/1781680841},
	abstract = {How does a literary historian end up thinking in terms of z-scores, principal component analysis, and clustering coefficient?

In the ten essays collected in this volume, Franco Moretti reconstructs the intellectual trajectory of his philosophy of ‘distant reading’. From the evolutionary model of ‘Modern European Literature’, through the geo-cultural dominant of ‘Conjectures on World Literature’ and ‘Planet Hollywood’ to the quantitative findings of ‘Style, inc.’ and the abstract patterns of ‘Network Theory, Plot Analysis’, the book follows two decades of critical explorations that have come to define – well beyond the wildest expectations of its author – a growing field of unorthodox literary studies.},
	language = {en},
	publisher = {Verso},
	author = {Moretti, Franco},
	year = {2013},
	keywords = {AnalyzeStatistically}
}

@incollection{burnard_du_2012,
	title = {Du literary and linguistic computing aux digital humanities : retour sur 40 ans de relations entre sciences humaines et informatique},
	copyright = {Licence Creative Commons Attribution - Pas d’Utilisation Commerciale - Pas de Modification 3.0 France},
	isbn = {978-2-8218-1324-3},
	shorttitle = {Du literary and linguistic computing aux digital humanities},
	url = {http://press.openedition.org/242},
	abstract = {IntroductionJe me suis fixé le but ambitieux de proposer un point de vue sur quarante ans d’histoire, sous la forme d’une synthèse, plutôt que de vous présenter des faits les uns à la suite des autres. J’ai essayé d’extraire de mon expérience quelques principes généraux.Je commencerai en témoignant du fait que nous vivons de façon irréversible un âge numérique. J’expliquerai cela en présentant les trois périodes successives qui ont marqué le développement des relations entre sciences humaines et informatique, ainsi que les principes qui les ont portées. Enfin, pour parler de façon concrète, je présenterai le cas spécifique de mon expérience à Oxford, ce qui permettra d’approfondir l’état courant des recherches dans les sciences humaines et sociales (SHS) informatisées.Il y a toujours eu une vraie tension entre les deux méthodes opposées dont on peut se servir dans les sciences humaines et sociales et l’informatique ; une confrontation entre le texte et les données. Certains diront qu’on peut se servir d’un ordinateur pour faire du data processing, pour gérer des données : des chiffres, des faits, des observations, des objets, des tendances statistiques. Le texte, en revanche, est composé d’autres choses : de mots, d’une langue, de paroles, qui ont une existence tout à fait indépendante de leur représentation dans un format numérique. Les données numériques, elles, n’existent que dans leur expression informatisée. C’est précisément là que réside la tension entre les deux. Pour [...]},
	language = {fr},
	urldate = {2012-09-24},
	booktitle = {{OpenEdition} {Press}},
	publisher = {OpenEdition Press},
	author = {Burnard, Lou},
	month = sep,
	year = {2012},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities},
	pages = {45--58}
}

@book{hockey_electronic_2000,
	address = {Oxford},
	title = {Electronic {Texts} in the {Humanities}},
	isbn = {978-0-19-871194-0},
	url = {http://ukcatalogue.oup.com/product/9780198711940.do},
	abstract = {The first overview of electronic texts in the humanities
    Shows Internet users what tools and techniques can be used to analyse and manipulate electronic texts

With word processing and the Internet, computing is much more part and parcel of the everyday life of the humanities scholar, but computers can do much more than assist with writing or Internet searching. This book introduces a range of tools and techniques for manipulating and analysing electronic texts in the humanities. It shows how electronic texts can be used for the literary analysis, linguistic analysis, authorship attribution, and the preparation and publication of electronic scholarly editions. It assesses the ways in which research in corpus and computational linguistics can feed into better electronic tools for humanities research. The tools and techniques discussed in this book will feed into better Internet tools and pave the way for the electronic scholar of the twenty-first century.},
	language = {en},
	publisher = {Oxford University Press},
	author = {Hockey, Susan},
	year = {2000},
	keywords = {AnalyzeQualitatively, AnalyzeStatistically, meta\_GiveOverview, X-CHECK}
}

@techreport{buddenbohm_erfolgskriterien_2014,
	address = {Göttingen},
	title = {Erfolgskriterien für den {Aufbau} und nachhaltigen {Betrieb} {Virtueller} {Forschungsumgebungen}},
	url = {urn:nbn:de:gbv:7-dariah-2014-5-4},
	abstract = {Virtuelle Forschungsumgebungen (VREs) haben sich in vielen Wissenschaftsdisziplinen zu
wichtigen Bestandteilen moderner Forschungsprozesse entwickelt. Dieser gewachsenen Bedeutung müssen die  Betreiber von VREs durch funktionierende und effiziente
Verfahren für Aufbau, Betrieb und die Qualitätssicherung gerecht werden. Zur
Unterstützung dieser Prozesse wird ein Lebensphasen-Modell für VREs vorgeschlagen, welches insbesondere   auf die erfolgskritischen Punkte für den Übergang in den nachhaltigen Betrieb einer VRE eingeht. Im weiteren wird ein Satz von Erfolgskriterien
diskutiert, welches es allen an der VRE beteiligten Akteuren (Betreiber, Förderer, Nutzer)ermöglicht, die für ihren Anwendungsfall relevanten Aspekte bereits im Vorfeld der Erstellung einer neuen VRE zu identifizieren. Angesichts der Heterogenität von VREs wird
dieses Kriterienset im individuellen Fall durch disziplinär spezifische Kriterien ergänzt.
Abgeschlossen werden diese Überlegungen durch Handreichungen zu den
Kostenstrukturen und möglichen Finanzierungsmodellen für VREs.},
	language = {de},
	number = {7},
	institution = {DARIAH-DE},
	author = {Buddenbohm, Stefan and Enke, Harry and Hofmann, Matthias and Klar, Jochen and Neuroth, Heike and Schwiegelshohn, Uwe},
	year = {2014},
	note = {00002},
	keywords = {obj\_DigitalHumanities, obj\_Humanities},
	pages = {37}
}

@techreport{rodriguez_experiments_2014,
	address = {Göttingen},
	title = {Experiments for the design of a help desk system for the {EHRI} project - an {Information} {Retrieval} approach},
	url = {http://nbn-resolving.de/urn:nbn:de:gbv:7-dariah-2014-3-2},
	abstract = {This paper describes the experiments realized for the conception of a helpdesk system for the EHRI project. The system will help researchers to find Collection Holding Institutions (CHI) suitable to support them to resolve questions about Holocaust relevant documentary sources. In this
experiments we model descriptions of archival material in a vector space model, and we show that this model is able to give an answer to real user queries.},
	language = {en},
	number = {5},
	institution = {DARIAH-DE},
	author = {Rodriguez, Kepa J.},
	year = {2014},
	note = {00000},
	keywords = {obj\_DigitalHumanities, obj\_Humanities, obj\_Metadata, t\_InformationRetrieval},
	pages = {14}
}

@inproceedings{kaageback_extractive_2014,
	title = {Extractive summarization using continuous vector space models},
	booktitle = {Proceedings of the 2nd {Workshop} on {Continuous} {Vector} {Space} {Models} and their {Compositionality} ({CVSC})},
	author = {K{\textbackslash}aagebäck, Mikael and Mogren, Olof and Tahmasebi, Nina and Dubhashi, Devdatt},
	year = {2014},
	pages = {31--39}
}

@incollection{mounier_faire_2012,
	title = {Faire des humanités numériques},
	copyright = {© OpenEdition Press, 2012 Conditions d'utilisation : http://www.openedition.org/6540},
	url = {http://books.openedition.org/oep/226},
	abstract = {Qu’est-ce que les humanités numériques ? Apparue en 2006, l’expression connaît depuis un véritable succès. Mais au-delà du slogan à la mode, quelle est la réalité des pratiques qu’il désigne ? Si tout le monde s’accorde sur une définition minimale à l’intersection des technologies numériques et des sciences humaines et sociales, les vues divergent lorsqu’on entre dans le vif du sujet. Les humanités numériques représentent-elles une véritable révolution des pratiques de recherche et des paradigmes intellectuels qui les fondent ou, plus simplement, une optimisation des méthodes existantes ? Constituent-elles un champ suffisamment structuré pour justifier une réforme des modes de financement de la recherche, des cursus de formation, des critères d’évaluation ? L’archive numérique offre-t-elle à la recherche suffisamment de garanties ? Quelle place la recherche « dirigée par les données » laisse-t-elle à l’interprétation ? Telles sont quelques-unes des questions abordées par ce deuxième opus de la collection « Read/Write Book ». Ces dix-huit textes essentiels, rédigés ou traduits en français par des chercheurs de différentes nationalités, proposent une introduction aux humanités numériques accessible à tous ceux qui souhaitent en savoir plus sur ce domaine de recherche en constante évolution.},
	language = {fr},
	urldate = {2013-10-19},
	booktitle = {Read/{Write} {Book} 2 : {Une} introduction aux humanités numériques},
	publisher = {OpenEdition Press},
	author = {Berra, Aurélien},
	editor = {Mounier, Pierre},
	year = {2012}
}

@techreport{gnadt_faktoren_2017,
	address = {Göttingen},
	title = {Faktoren und {Kriterien} für den {Impact} von {DH}-{Tools} und {Infrastrukturen}},
	url = {http://webdoc.sub.gwdg.de/pub/mon/dariah-de/dwp-2017-21.pdf},
	abstract = {Der Impact von digitalen Forschungsinfrastrukturen und Tools in der geistes- und kulturwissenschaftli-
chen Forschung ist von zentralem Interesse, wenn es darum geht die Vorteile solcher Entwicklungen zum
einen den ForscherInnen selbst, zum anderen aber auch den Förderern zu vermitteln. In diesem Artikel
werden Impactfaktoren und Erfolgskriterien in einem Bewertungskatalog gesammelt und aus Sicht verschiedener Stakeholder, FachwissenschaftlerInnen, DiensteanbieterInnen, DiensteentwicklerInnen und Förderer beurteilt. Dazu werden durchgeführte Umfragen ausgewertet, Kriterien und Maßzahlen aus der Literatur zusammengetragen und die daraus resultierenden Faktoren und Kennzahlen nach eindeutigen Impact-Bereichen katalogisiert. Berücksichtigt werden neben den gängigen quantitativen
Kennzahlen auch qualitative Aspekte, die den Besonderheiten der Geistes- und Kulturwissenschaften Rechnung tragen.},
	language = {de},
	number = {21},
	institution = {DARIAH-DE},
	author = {Gnadt, Timo and Schmitt, Viola E. and Stiller, Juliane and Thoden, Klaus},
	year = {2017},
	note = {00000},
	keywords = {obj\_Tools, obj\_DigitalHumanities, obj\_Infrastructures, obj\_Humanities},
	pages = {49}
}

@techreport{klimpel_forschen_2015,
	address = {Göttingen},
	title = {Forschen in der digitalen {Welt}. {Juristische} {Handreichung} für die {Geisteswissenschaften}},
	url = {http://webdoc.sub.gwdg.de/pub/mon/dariah-de/dwp-2015-12.pdf},
	abstract = {Unter dem Schlagwort Digital Humanities finden nicht nur Techniken der quantitativen Analyse vermehrt   Eingang in den Methodenkanon der Geistes- und Kulturwissenschaften, auch ein Umdenken in der Verwaltung von Zugriffs- und Nutzungsrechten wird   erforderlich. Ob die Einführung von Standards sowohl für Zugangsberechtigung zu Forschungsergebnissen als auch für die Verwendung von Forschungsdaten in Form von freien Lizenzen hierfür ein geeignetes Mittel darstellen, soll u. a. mit diesem Dokument eruiert werden. Nach der Einführung woran Rechte entstehen, widmen sich weitere Kapitel dem wissenschaftlichen Arbeiten auf Basis fremder Inhalte, dem wissenschaftlichen Arbeiten als Quelle eigener Rechte, den Rechten der Forschungsinstitution oder Universität, Open Access, sowie der grenzüberschreitenden Forschung und Haftungsfragen.},
	language = {de},
	number = {12},
	institution = {DARIAH-DE},
	author = {Klimpel, Paul and Weitzmann, John H.},
	year = {2015},
	note = {00000},
	keywords = {obj\_DigitalHumanities, obj\_Humanities, Open Access},
	pages = {36}
}

@techreport{andorfer_forschen_2015,
	address = {Göttingen},
	title = {Forschen und {Forschungsdaten} in den {Geisteswissenschaften}. {Zwischenbericht} einer {Interviewreihe}},
	url = {urn:nbn:de:gbv:7-dariah-2015-3-8},
	abstract = {Im Rahmen der an der Herzog August Bibliothek Wolfenbüttel durchgeführten Interviewreihe:
„Forschen und Forschungsdaten in den Geisteswissenschaften“ wurden GeistswissenschaftlerInnen und StipendiatInnen zu ihrer Forschungspraxis und ihrem Umgang mit Forschungsdaten befragt. Vorliegender Bericht präsentiert erste Ergebnisse   (Stand Jänner 2015) der auf
http://digital-archiv.at:8081/exist/apps/DARIAH-Collection/pages/InterviewAuswertung.html
„in Echtzeit“ erfolgenden Auswertung.},
	language = {de},
	number = {10},
	institution = {DARIAH-DE},
	author = {Andorfer, Peter},
	year = {2015},
	note = {00001},
	keywords = {obj\_DigitalHumanities, obj\_Research, obj\_Humanities},
	pages = {18}
}

@techreport{andorfer_forschungsdaten_2015,
	address = {Göttingen},
	title = {Forschungsdaten in den (digitalen) {Geisteswissenschaften}. {Versuch} einer {Konkretisierung}},
	url = {urn:nbn:de:gbv:7-dariah-2015-7-2},
	abstract = {Die Bezeichnung „Forschungsdaten“ zählt zu den Schlüsselbegriffen der Digitalen
Geisteswissenschaften, nicht zuletzt vor den zunehmenden Bemühungen zur Errichtung und Etablierung von Datenzentren, -archiven und Repositorien zur Veröffentlichung und dauerhaften Sicherung von Forschungsdaten. Vorliegendes Papier versucht eine Definition des Begriffes
Forschungsdaten. Im Fokus steht dabei   das Bemühen, diese Bezeichnung im   Vokabular traditionellen geisteswissenschaftlichen Arbeitens   zu verorten. Daran anschließend geht es
außerdem darum, Forschungsdaten von semantisch ähnlichen Bezeichnungen wie Publikation oder Quellen abzugrenzen.},
	language = {de},
	number = {14},
	institution = {DARIAH-DE},
	author = {Andorfer, Peter},
	year = {2015},
	note = {00000},
	keywords = {obj\_DigitalHumanities, obj\_Research, obj\_Humanities},
	pages = {29}
}

@book{conolly_geographical_2008,
	address = {Cambridge},
	edition = {3rd printing.},
	title = {Geographical {Information} {Systems} in archaeology},
	isbn = {978-0-521-79330-8},
	url = {http://ebooks.cambridge.org/ebook.jsf?bid=CBO9780511807459},
	abstract = {Geographical Information Systems has moved from the domain of the computer specialist into the wider archaeological community, providing it with an exciting new research method. This clearly written but rigorous book provides a comprehensive guide to that use. Topics covered include: the theoretical context and the basics of GIS; data acquisition including database design; interpolation of elevation models; exploratory data analysis including spatial queries; statistical spatial analysis; map algebra; spatial operations including the calculation of slope and aspect, filtering and erosion modeling; methods for analysing regions; visibility analysis; network analysis including hydrological modeling; the production of high quality output for paper and electronic publication; and the use and production of metadata. Offering an extensive range of archaeological examples, it is an invaluable source of practical information for all archaeologists, whether engaged in cultural resource management or academic research. This is essential reading for both the novice and the advanced user.},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Conolly, James},
	year = {2008},
	doi = {http://dx.doi.org/10.1017/CBO9780511807459},
	keywords = {act\_Visualizing, meta\_GiveOverview, obj\_Artefacts, obj\_Architecture}
}

@book{demantowsky_geschichte_2014,
	address = {Berlin, Boston},
	title = {Geschichte lernen im digitalen {Wandel}},
	isbn = {978-3-486-85866-2},
	url = {http://www.degruyter.com/view/product/231648},
	language = {DT},
	urldate = {2014-12-16},
	publisher = {De Gruyter Oldenbourg},
	author = {Demantowsky, Marko and Pallaske, Christoph},
	year = {2014}
}

@book{brossaud_humanites_2007,
	address = {Paris},
	title = {Humanités numériques : {Volume} 1, {Nouvelles} technologies cognitives et épistémologie},
	isbn = {978-2-7462-1661-7},
	shorttitle = {Humanités numériques},
	language = {fr},
	publisher = {Hermes Science Publications},
	author = {Brossaud, Claire and Reber, Bernard},
	year = {2007},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities}
}

@incollection{mccarty_humanities_2003,
	address = {New York},
	title = {Humanities computing},
	language = {en},
	booktitle = {Encyclopedia of {Library} and {Information} {Science}},
	publisher = {Dekker},
	author = {McCarty, Willard},
	year = {2003},
	pages = {1224--1235}
}

@book{mccarty_humanities_2005,
	address = {Basingstoke \& New York},
	title = {Humanities computing},
	isbn = {978-1-4039-3504-5},
	url = {http://www.mccarty.org.uk/essays/McCarty,%20Humanities%20computing.pdf},
	language = {en},
	publisher = {Palgrave Macmillan},
	author = {McCarty, Willard},
	year = {2005},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities, *****, act\_Conceptualizing, obj\_Methods, act\_Modeling}
}

@article{svensson_humanities_2009,
	title = {Humanities {Computing} as {Digital} {Humanities}},
	volume = {3},
	url = {http://digitalhumanities.org/dhq/vol/3/3/000065/000065.html},
	abstract = {This article presents an examination of how digital humanities is currently conceived and described, and examines the discursive shift from humanities computing to digital humanities. It is argued that this renaming of humanities computing as digital humanities carries with it a set of epistemic commitments that are not necessarily compatible with a broad and inclusive notion of the digital humanities. In particular, the author suggests that tensions arise from the instrumental, textual and methodological focus of humanities computing as well as its relative lack of engagement with the "digital" as a study object. This article is the first in a series of four articles attempting to describe and analyze the field of digital humanities and digital humanities as a transformative practice.},
	language = {en},
	number = {3},
	urldate = {2010-01-13},
	journal = {Digital Humanities Quarterly},
	author = {Svensson, Patrik},
	year = {2009}
}

@inproceedings{mccarty_humanities_1999,
	title = {Humanities computing as interdiscipline},
	url = {http://www.iath.virginia.edu/hcs/mccarty.html},
	abstract = {In its title, "Is humanities computing an academic discipline?", this Seminar poses a question that when asked is all too often answered with little or no awareness of the assumptions behind it. Because they have remained largely unexamined, these assumptions have tended to misdirect our thinking about what we do as computing humanists and to interfere with our ability to speak cogently about it. My efforts here will concentrate on taking this question apart and examining its major assumptions to see if we cannot make better questions. In passing I will also flag several areas of disciplinary research that we need to bring within our scope in order to address those better questions.},
	language = {en},
	booktitle = {Seminar "{Is} {Humanities} {Computing} an {Academic} {Discipline}?"},
	publisher = {Institute for Advanced Technology in the Humanities (IATH), University of Virginia},
	author = {McCarty, Willard},
	year = {1999},
	keywords = {obj\_DigitalHumanities, act\_Conceptualizing}
}

@book{orlandi_informatica_2010,
	address = {Roma ; Bari},
	title = {Informatica testuale : teoria e prassi},
	isbn = {978-88-420-9379-4},
	shorttitle = {Informatica testuale},
	url = {http://www.ibs.it/code/9788842093794/orlandi-tito/informatica-testuale-teoria.html},
	abstract = {Tito Orlandi propone una sintesi chiara delle metodologie che governano l'applicazione degli strumenti informatici in ambito testuale, in particolar modo si occupa della cosiddetta testualità riflessa o mediata, ossia di quei testi concepiti per essere diffusi su supporto cartaceo e solo in un secondo momento essere riprodotti in un ambiente informatico, così da diventare anche testi digitali. L'informatica nel suo impatto con le applicazioni umanistiche ha avuto infatti conseguenze innovative e in costante mutamento. Il vasto mondo che ruota attorno alla funzione, produzione e ricezione del testo sta subendo una trasformazione radicale dovuta alle tecnologie, ma pochi sono in grado di apprezzarne compiutamente le origini e le ricadute sulla letteratura tutta. Il volume, insieme alla considerazione teorica del tema, offre una guida pratica alle procedure applicative sempre valide, nonostante i quotidiani cambiamenti nelle macchine informatiche.},
	language = {it},
	publisher = {Laterza},
	author = {Orlandi, Tito},
	year = {2010},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities, X-CHECK}
}

@book{orlandi_informatica_1990,
	address = {Roma},
	series = {Studi superiori},
	title = {Informatica umanistica},
	isbn = {978-88-430-0887-2},
	url = {http://books.google.com/books?id=XHrDPAAACAAJ},
	language = {it},
	publisher = {Nuova Italia scientifica},
	author = {Orlandi, Tito},
	year = {1990},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities}
}

@techreport{beer_interdisciplinary_2014,
	address = {Göttingen},
	title = {Interdisciplinary {Interoperability}},
	url = {http://webdoc.sub.gwdg.de/pub/mon/dariah-de/dwp-2014-3.pdf},
	abstract = {The exchange and reusability of data used for research in the humanities is one of the goals of DARIAH. To increase the interoperability of data sets between disciplines we present an overview and recommendations of measures to achieve this. We account for
the finding and fetching of data with legal aspects in mind. This is achieved through standardized methods of discovery and transfer via interfaces on the web. Furthermore, we consider syntactic and semantic interoperability of data for use in different fields of
study. Standardized metadata sets are one way to achieve this and we present some of them in this paper. The importance for scholars to find and be able to process data that is be relevant to their work is the main motivation of this document and for the aspects of the digital humanities covered within. We present options for each of the four aspects that we identified (APIs and Protocols, Standards, Identifiers and Licensing).},
	language = {en},
	number = {3},
	institution = {DARIAH-DE},
	author = {Beer, Nikolaos and Herold, Kristin and Kolbmann, Wibke and Kollatz, Thomas and Romanello, Matteo and Rose, Sebastian and Walkowski, Niels-Oliver},
	year = {2014},
	note = {00002},
	keywords = {obj\_DigitalHumanities, obj\_Humanities},
	pages = {46}
}

@techreport{lorenz_interdisziplinare_2015,
	address = {Göttingen},
	title = {Interdisziplinäre {E}-{Publikationen} – interdisziplinäre {Evaluation}? {Ein} {Blick} auf die {Bewertung} fächerübergreifender {Forschungsleistungen} am {Beispiel} der {Digital} {Humanities}},
	url = {urn:nbn:de:gbv:7-dariah-2015-2-2},
	abstract = {Die Divergenzen zwischen der naturwissenschaftlichen und der geisteswissenschaftlichen Publikationskultur bestehen auch in der elektronischen Publikationspraxis weiter. Mit der zunehmenden interdisziplinären Zusammenarbeit werden die Publikations- und Bewertungsstandards zugleich mehrheitlich als defizitär wahrgenommen. Als eine Bestandsaufnahme skizziert der Beitrag den Entwicklungsstand unterschiedlicher Evalutionsverfahren und gibt im Bereich der Digital Humanities einen Überblick über die verschiedenen internationalen und interdisziplinären Standardisierungsansätze.},
	number = {9},
	institution = {DARIAH-DE},
	author = {Lorenz, Anne Katrin},
	year = {2015},
	note = {00000},
	keywords = {obj\_DigitalHumanities, act\_Publishing, obj\_Humanities},
	pages = {11}
}

@book{adolphs_introducing_2006,
	address = {London \& New York},
	title = {Introducing electronic text analysis : a practical guide for language and literary studies},
	isbn = {978-0-415-32021-4},
	shorttitle = {Introducing electronic text analysis},
	url = {http://www.routledge.com/textbooks/0415320216},
	abstract = {Introducing Electronic Text Analysis is a practical and much needed introduction to corpora - bodies of linguistic data. Written specifically for students studying this topic for the first time, the book begins with a discussion of the underlying principles of electronic text analysis. It then examines how these corpora enhance our understanding of literary and non-literary works. In the first section the author introduces the concepts of concordance and lexical frequency, concepts which are then applied to a range of areas of language study. Key areas examined are the use of on-line corpora to complement traditional stylistic analysis, and the ways in which methods such as concordance and frequency counts can reveal a particular ideology within a text. Presenting an accessible and thorough understanding of the underlying principles of electronic text analysis, the book contains abundant illustrative examples and a glossary with definitions of main concepts. It will also be supported by a companion website with links to on-line corpora so that students can apply their knowledge to further study. The accompanying website to this book can be found at http://www.routledge.com/textbooks/0415320216},
	language = {eng},
	publisher = {Routledge},
	author = {Adolphs, Svenja},
	year = {2006},
	keywords = {meta\_GiveOverview, goal\_Analysis, act\_ContentAnalysis}
}

@article{orlandi_is_2002,
	title = {Is {Humanities} {Computing} a {Discipline}?},
	url = {http://computerphilologie.digital-humanities.de/jg02/orlandi.html},
	abstract = {At the beginning, the article mentions the results of an enquiry on how humanities computing is being introduced into the curricula of the European universities; and the most important topics recently discussed around the theme of a theory of humanities computing. It appears that most experts agree on the opinion that humanities computing is an independent discipline, and as such it should be introduced into the faculties of humanities. The article then explains how the foundation of the discipline should be understood, on the basis of computing theory and the methodology of the different humanities disciplines. Source of abstract: http://computerphilologie.tu-darmstadt.de/jg02/orlandi.html},
	language = {en},
	number = {4},
	journal = {Jahrbuch für Computerphilologie},
	author = {Orlandi, Tito},
	year = {2002},
	note = {Orlandi, Tito: Is Humanities Computing a Discipline? In: Jahrbuch für Computerphilologie 4 (2002), S. 51-58.},
	keywords = {meta\_Assessing, obj\_DigitalHumanities, act\_Conceptualizing},
	pages = {51--58}
}

@book{hennermann_kartographie_2006,
	address = {Darmstadt},
	title = {Kartographie und {GIS} : eine {Einführung}},
	isbn = {978-3-534-19692-0},
	shorttitle = {Kartographie und {GIS}},
	url = {http://www.wbg-wissenverbindet.de/shop/de/wbg/kartographie-und-gis-b243952-001},
	abstract = {Diese neuartige Einführung entspricht in ihrem Aufbau dem Grundkurs Kartographie und Datenverarbeitung, wie er für nahezu alle Geowissenschaftler verpflichtend angeboten wird. Das Buch erläutert kompakt aber umfassend, wie Karten Informationen speichern und wiedergeben. Darüber hinaus wird die technische Seite der Kartenherstellung, soweit sie für die Geowissenschaften erforderlich ist, gleich mitbehandelt. Anhand von Beispielen und Übungsaufgaben lernt der Leser, eigene Daten zu verarbeiten und Karten zu erstellen. Technische Basis ist die Software ESRI ArcGIS 9, die im universitären Bereich überall eingesetzt wird und als Testversion für Übungszwecke umsonst angefordert werden kann. Zusätzliche Übungsdateien können aus dem Internet geladen werden.},
	language = {de},
	publisher = {Wiss. Buchges.},
	author = {Hennermann, Karl},
	year = {2006},
	keywords = {act\_Visualizing, meta\_GiveOverview, obj\_Maps, obj\_Artefacts}
}

@incollection{mounier_les_2012,
	title = {Les digital humanities aujourd'hui : centres, réseaux, pratiques, enjeux},
	copyright = {© OpenEdition Press, 2012 Conditions d'utilisation : http://www.openedition.org/6540},
	url = {http://books.openedition.org/oep/226},
	abstract = {Qu’est-ce que les humanités numériques ? Apparue en 2006, l’expression connaît depuis un véritable succès. Mais au-delà du slogan à la mode, quelle est la réalité des pratiques qu’il désigne ? Si tout le monde s’accorde sur une définition minimale à l’intersection des technologies numériques et des sciences humaines et sociales, les vues divergent lorsqu’on entre dans le vif du sujet. Les humanités numériques représentent-elles une véritable révolution des pratiques de recherche et des paradigmes intellectuels qui les fondent ou, plus simplement, une optimisation des méthodes existantes ? Constituent-elles un champ suffisamment structuré pour justifier une réforme des modes de financement de la recherche, des cursus de formation, des critères d’évaluation ? L’archive numérique offre-t-elle à la recherche suffisamment de garanties ? Quelle place la recherche « dirigée par les données » laisse-t-elle à l’interprétation ? Telles sont quelques-unes des questions abordées par ce deuxième opus de la collection « Read/Write Book ». Ces dix-huit textes essentiels, rédigés ou traduits en français par des chercheurs de différentes nationalités, proposent une introduction aux humanités numériques accessible à tous ceux qui souhaitent en savoir plus sur ce domaine de recherche en constante évolution.},
	language = {fr},
	urldate = {2013-10-19},
	booktitle = {Read/{Write} {Book} 2 : {Une} introduction aux humanités numériques},
	publisher = {OpenEdition Press},
	author = {Welger-Barboza, Corine},
	editor = {Mounier, Pierre},
	year = {2012}
}

@inproceedings{genet_les_2011,
	title = {Les historiens et l'informatique: un métier à réinventer},
	isbn = {978-2-7283-0904-7},
	shorttitle = {Les historiens et l'informatique},
	language = {French, English, and Italian.},
	publisher = {École française de Rome},
	author = {Genêt, Jean-Philippe and Zorzi, Andrea},
	year = {2011},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities}
}

@book{mounier_les_nodate,
	title = {Les humanités numériques. {Une} histoire critique},
	isbn = {978-2-7351-2255-4},
	shorttitle = {Les humanités numériques},
	url = {http://www.editions-msh.fr/livre/?GCOI=27351100065270},
	abstract = {Que sont les humanités numériques ?

D'abord une rencontre, au lendemain de la Seconde Guerre mondiale. Celle d'un prêtre jésuite soucieux d’analyser la Somme théologique de Thomas d’Aquin avec les ordinateurs d’IBM. Cette collaboration donnera naissance à ce qu’on appellera plus tard les humanités numériques.

Porteuses de l’histoire des technologies, marquée par le développement des technosciences et du complexe militaro-industriel, les humanités numériques conduisent à s’interroger en retour sur ce qui fait la spécificité des humanités. L’union des technologies numériques et des humanités conduit-elle à remettre en cause ce qui les dinstingue traditionnellement ? Le numérique pousse-t-il, par les méthodes et modèles qu’il permet de développer dans ce champ de recherche, à placer les humanités sous la domination de modèles scientifiques qui leur sont étrangers ?

Quels dangers ces approches comportent-elles, en particulier lorsqu’une part croissante des productions culturelles et des interactions sociales est désormais placée sous l’emprise de sociétés commerciales globalisées qui font un usage massif du numérique ?

Dans cet ouvrage, Pierre Mounier nous livre une histoire critique des humanités numériques et propose de redéfinir à la lumière de ces analyses le contrat moral que les humanités peuvent établir avec la société.},
	language = {Fr},
	author = {Mounier, Pierre},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities}
}

@book{clivaz_lire_2012,
	address = {Lausanne},
	title = {Lire demain: des manuscrits antiques a l'ère digitale {Jérôme} , {François} {Vallotton}, {Joseph} {Verheyden} and {Benjamin} {Bertho}},
	isbn = {978-2-88074-958-3},
	shorttitle = {Lire demain},
	language = {fr},
	publisher = {Presses polytechniques et universitaires romandes},
	editor = {Clivaz, Claire and Meizozéô},
	year = {2012}
}

@incollection{siemens_literary_2004,
	address = {Oxford},
	edition = {Hardcover},
	series = {Blackwell {Companions} to {Literature} and {Culture}},
	title = {Literary {Analysis}},
	isbn = {978-1-4051-0321-3},
	url = {http://www.digitalhumanities.org/companion/},
	language = {en},
	urldate = {2010-05-17},
	booktitle = {A {Companion} to {Digital} {Humanities}},
	publisher = {Blackwell Publishing Professional},
	author = {Rommel, Thomas},
	editor = {Siemens, Ray and Unsworth, John and Schreibman, Susan},
	year = {2004},
	keywords = {meta\_GiveOverview, act\_Publishing, meta\_Theorizing, t\_Encoding, X-CHECK}
}

@book{price_literary_2012,
	address = {New York},
	title = {Literary {Studies} in the {Digital} {Age}: {A} {Methodological} {Primer}},
	language = {en},
	publisher = {MLA Publications},
	editor = {Price, Ken and Siemens, Ray},
	year = {2012},
	keywords = {meta\_GiveOverview, X-CHECK, obj\_Literature}
}

@book{liu_local_nodate,
	address = {Chicago},
	title = {Local {Transcendence}: {Essays} on {Postmodern} {Historicism} and the {Database}},
	isbn = {978-0-226-48696-3},
	url = {http://www.press.uchicago.edu/ucp/books/book/chicago/L/bo5867451.html},
	abstract = {Book of essays on the methodology of the new historicism and other modes of postmodern cultural criticism in the age of the network and the database.},
	language = {en},
	publisher = {University of Chicago Press},
	author = {Liu, Alan},
	keywords = {meta\_GiveOverview, meta\_Theorizing, goal\_Interpretation, goal\_Create, obj\_Data/Databases, t\_Narratology}
}

@techreport{stollwerk_machbarkeitsstudie_2016,
	address = {Göttingen},
	title = {Machbarkeitsstudie zu {Einsatzmöglichkeiten} von {OCR}-{Software} im {Bereich} '{Alter} {Drucke}' zur {Vorbereitung} einer vollständigen {Digitalisierung} deutscher {Druckerzeugnisse} zwischen 1500 und 1930},
	url = {http://webdoc.sub.gwdg.de/pub/mon/dariah-de/dwp-2016-16.pdf},
	abstract = {Forschungsvorhaben sind oft an  spezifische technische Anforderungen  gekoppelt. Diese Studie liefert einen Überblick zu relevanten Anwendungsbereichen der OCR, dem Bereich der praxisbezogenen   Massen-OCR für Bibliotheken sowie   vergleichbaren  Einrichtungen und dem
anwendungsnahen Teil der OCR-Forschung.
Für die systematische und vollständige Digitalisierung von Druckwerken wird in dieser Studie eine Übersicht zur Größe der Bestände bereitgestellt. Zudem wird überprüft, ob mit derzeit marktüblicher Software oder mit derzeit vorhandenen noch nicht marktüblichen Produkten, die
Funktionalität anhand der Anforderungen älterer Drucke angepasst werden kann,   ohne grundlegende Neuentwicklung dabei vorzusehen.
Die Identifizierung von Forschungsfeldern im Bereich der angewandten Informatik, von denen die
Ausdehnung der derzeit möglichen   OCR-Verfahren auf bisher nicht behandelbare Korpora zu erwarten ist, wird ebenfalls geleistet. Darüber hinaus werden Forschungsfelder benannt, die eine Ausdehnung auf heute  nicht routinemäßig mit OCR-Verfahren behandelbare Schriften  (wie  z.B.
Handschriften) abdecken. 
Im Rahmen von vier Fallstudien sind für diese Zwecke Korpora definiert worden, um im Hinblick auf die Optimierungsmöglichkeiten von OCR-Verfahren die Ergebnisse in  Fallstudien zu diskutieren, und um Empfehlungen und Ansätze zu konkretisieren.},
	language = {de},
	number = {16},
	institution = {DARIAH-DE},
	author = {Stollwerk, Christoph},
	year = {2016},
	note = {00000},
	keywords = {obj\_DigitalHumanities, obj\_Humanities},
	pages = {97}
}

@misc{mccarty_mapping_2002,
	title = {Mapping the field},
	url = {http://www.eadh.org/mapping-field},
	abstract = {Mapping, as we all know, is a powerful tool for representing a complex terrain so that we can apprehend it as a whole and quickly grasp the interrelation of its parts. A “roadmap” is of course for going places, but prior to that, it’s for seeing what the choices are and what might be involved in getting there. Prior again to that, as recent work in the history of mapping has argued, the making of a map takes possession of and literally deﬁnes the mapped terrain: features are named (or re-named), relationships shown, boundaries indicated, unknown parts labelled – and so marked for exploration.},
	language = {en},
	journal = {EADH},
	author = {McCarty, Willard and Short, Harold},
	year = {2002}
}

@incollection{jannidis_methoden_2010,
	address = {Stuttgart \& Weimar},
	title = {Methoden der computergestützten {Textanalyse}},
	url = {http://www.amazon.de/Methoden-literatur-kulturwissenschaftlichen-Textanalyse-Modellanalysen/dp/3476021629},
	abstract = {Wie werden Texte wissenschaftlich interpretiert? Kompakt und übersichtlich stellt das Lehrbuch die gängigen literaturtheoretischen Ansätze vor – darunter z. B. Strukturalismus, Dekonstruktion, Diskursanalyse und Systemtheorie. Der Schwerpunkt des übersichtlich gestalteten Bandes liegt auf der Anwendung: Ausführliche Beispielanalysen zeigen, wie mit den verschiedenen Theorien und den daraus abzuleitenden Methoden Texte – und auch Filme – analysiert werden. Unerlässliches Handwerkszeug für das literaturwissenschaftliche Studium.},
	language = {de},
	booktitle = {Methoden der literatur- und kulturwissenschaftlichen {Textanalyse}},
	publisher = {Metzler},
	author = {Jannidis, Fotis},
	editor = {Nünning, Ansgar and Nünning, Vera},
	year = {2010},
	pages = {109--132}
}

@article{anderson_methodological_2010,
	title = {Methodological commons: arts and humanities e-{Science} fundamentals},
	volume = {368},
	shorttitle = {Methodological commons},
	url = {http://rsta.royalsocietypublishing.org/content/368/1925/3779.abstract},
	doi = {10.1098/rsta.2010.0156},
	abstract = {The application of e-Science technologies to disciplines in the arts and humanities raises major questions as to how those technologies can be most usefully exploited, what tools and infrastructures are needed for that exploitation, and what new research approaches can be generated. This paper reviews a number of activities in the UK and Europe in the last 5 years which have sought to address these questions through processes of experimentation and targeted infrastructure development. In the UK, the AHeSSC (Arts and Humanities e-Science Support Centre) has played a coordinating role for seven projects funded by the Arts and Humanities e-Science Initiative. In Europe, DARIAH (Digital Research Infrastructure for the Arts and Humanities) has sought to develop a deeper understanding of research information and communication in the arts and humanities, and to inform the development of e-infrastructures accordingly. Both sets of activity have indicated a common requirement: to construct a framework which consistently describes the methods and functions of scholarly activity which underlie digital arts and humanities research, and the relationships between them. Such a ‘methodological commons’ has been formulated in the field of the digital humanities. This paper describes the application of this approach to arts and humanities e-Science, with reference to the early work of DARIAH and AHeSSC.},
	language = {en},
	number = {1925},
	urldate = {2011-07-06},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Anderson, Sheila and Blanke, Tobias and Dunn, Stuart},
	year = {2010},
	keywords = {act\_Conceptualizing, obj\_Methods},
	pages = {3779 --3796}
}

@book{tomasi_metodologie_2008,
	address = {Roma},
	series = {Manuali universitari},
	title = {Metodologie informatiche e discipline umanistiche},
	isbn = {978-88-430-4303-3},
	url = {http://books.google.it/books?id=yG_kPAAACAAJ},
	abstract = {Quale vincolo lega le discipline umanistiche all'informatica come scienza dell'informazione? Quale rapporto sussiste fra le "humanae litterae" e una disciplina che si occupa di forme di rappresentazione, trattamento ed elaborazione dell'informazione? Il connubio non coinvolge i soli aspetti strumentali di uso della macchina informatica. Obiettivo del volume è ragionare su aspetti e settori della ricerca umanistica che possono essere ripensati alla luce dei presupposti teorici, metodologici e tecnici della scienza dell'informazione: dal concetto di informazione alla sua formalizzazione, dalle reti ai sistemi di basi di dati, dai linguaggi di markup agli ipertesti e ai sistemi multimediali. Ma è soprattutto sull'elaborazione dell'informazione che si concentra la "computabilità umanistica": dal trattamento automatico del linguaggio naturale al Web semantico, dai metadati alle biblioteche digitali. Fonte dell'abstract: http://books.google.it/books?id=yG\_kPAAACAAJ},
	language = {it},
	publisher = {Carocci},
	author = {Tomasi, Francesca},
	year = {2008},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities}
}

@book{crawford_modern_2009,
	address = {Farnham, England ; Burlington, VT},
	series = {Digital research in the arts and humanities},
	title = {Modern methods for musicology: prospects, proposals, and realities},
	isbn = {978-0-7546-7302-6},
	shorttitle = {Modern methods for musicology},
	url = {http://books.google.de/books?hl=de&lr=&id=uani7DPZn24C&oi=fnd&pg=PA1&dq=Modern+methods+for+musicology:+prospects,+proposals,+and+realities&ots=5h1n8W039J&sig=ixgqhhjgJVfAbTg2HxCCXJd1Frk#v=onepage&q=Modern%20methods%20for%20musicology%3A%20prospects%2C%20proposals%2C%20and%20realities&f=false},
	abstract = {Written by leading experts, this volume provides a picture of the realities of current ICT use in musicology as well as prospects and proposals for how it could be fruitfully used in the future. Through its coverage of topics spanning content-based sound searching/retrieval, sound and content analysis, markup and text encoding, audio resource sharing, and music recognition, this book highlights the breadth and inter-disciplinary nature of the subject matter and provides a valuable resource to technologists, musicologists, musicians and music educators. It facilitates the identification of worthwhile goals to be achieved using technology and effective interdisciplinary collaboration.},
	language = {en},
	publisher = {Ashgate},
	editor = {Crawford, Tim and Gibson, Lorna},
	year = {2009},
	keywords = {X-CHECK, obj\_Music}
}

@article{collar_networks_2015,
	title = {Networks in {Archaeology}: {Phenomena}, {Abstraction}, {Representation}},
	volume = {22},
	issn = {1072-5369, 1573-7764},
	shorttitle = {Networks in {Archaeology}},
	url = {http://link.springer.com/article/10.1007/s10816-014-9235-6},
	doi = {10.1007/s10816-014-9235-6},
	abstract = {The application of method and theory from network science to archaeology has dramatically increased over the last decade. In this article, we document this growth over time, discuss several of the important concepts that are used in the application of network approaches to archaeology, and introduce the other articles in this special issue on networks in archaeology. We argue that the suitability and contribution of network science techniques within particular archaeological research contexts can be usefully explored by scrutinizing the past phenomena under study, how these are abstracted into concepts, and how these in turn are represented as network data. For this reason, each of the articles in this special issue is discussed in terms of the phenomena that they seek to address, the abstraction in terms of concepts that they use to study connectivity, and the representations of network data that they employ in their analyses. The approaches currently being used are diverse and interdisciplinary, which we think are evidence of a healthy exploratory stage in the application of network science in archaeology. To facilitate further innovation, application, and collaboration, we also provide a glossary of terms that are currently being used in network science and especially those in the applications to archaeological case studies.},
	language = {en},
	number = {1},
	urldate = {2015-08-05},
	journal = {Journal of Archaeological Method and Theory},
	author = {Collar, Anna and Coward, Fiona and Brughmans, Tom and Mills, Barbara J.},
	month = jan,
	year = {2015},
	keywords = {act\_NetworkAnalysis, act\_Theorizing, obj\_ANTHROPOLOGY, obj\_Archaeology, obj\_Network science, obj\_Relational archaeology},
	pages = {1--32},
	file = {Collar et al_2015_Networks in Archaeology.pdf:/home/lisnux/Zotero/storage/W5WXXSRQ/Collar et al_2015_Networks in Archaeology.pdf:application/pdf}
}

@techreport{meiners_participatory_2015,
	address = {Göttingen},
	title = {Participatory {Design} bei der {Erstellung} einer {Virtuellen} {Forschungsumgebung} für die {Geschichtswissenschaft}},
	url = {urn:nbn:de:gbv:7-dariah-2015-6-6},
	abstract = {Participatory Design ist ein Ansatz aus der Software-Entwicklung, bei dem Benutzer und Entwickler zusammenarbeiten, um sicherzustellen, dass das zu entwickelnde Produkt
den Anforderungen der Zielgruppe entspricht. Ziel ist es, vom Wissen der jeweils anderen Gruppe zu profitieren: Entwickler kennen die Technologien, während auf der anderen Seite die Nutzer über Anwendungswissen und Anforderungen aus dem jeweiligen   Arbeits- oder Forschungsfeld verfügen   und den späteren Anwendungskontext genau beschreiben können.
Im vorliegenden Dokument wird die Anwendung des Participatory Designs im Rahmen der Entwicklung einer Virtuellen Forschungsumgebung (VRE) für die
Geschichtswissenschaft gezeigt und die dabei gemachten Erfahrungen vorgestellt.},
	language = {de},
	number = {13},
	institution = {DARIAH-DE},
	author = {Meiners, Hanna-Lena and Buddenbohm, Stefan and Thiel, Carsten},
	year = {2015},
	note = {00000},
	keywords = {obj\_DigitalHumanities, obj\_Research, obj\_Infrastructures, obj\_Humanities},
	pages = {33}
}

@incollection{meister_projekt_2005,
	address = {München},
	title = {Projekt {Computerphilologie}. Über {Geschichte}, {Verfahren} und {Theorie} rechnergestützter {Literaturwissenschaft}},
	isbn = {978-3-7705-4179-9},
	shorttitle = {Digitalität und {Literalität}},
	url = {http://www1.uni-hamburg.de/DigiLit/meister/computerphilologie_druck.html},
	language = {de},
	booktitle = {Digitalität und {Literalität} : zur {Zukunft} der {Literatur}},
	publisher = {Fink},
	author = {Meister, Jan Christoph},
	year = {2005},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities, obj\_Literature}
}

@techreport{schoch_quantitative_2013,
	address = {Göttingen},
	title = {Quantitative {Text} {Analysis} for {Literary} {History} – {Report} on a {DARIAH}-{DE} {Expert} {Workshop}},
	url = {http://webdoc.sub.gwdg.de/pub/mon/dariah-de/dwp-2013-2.pdf},
	abstract = {The workshop on Quantitative Text Analysis for Literary History was the first in a series of DARIAH-DE expert workshops and took place from November 22 to 23 at the University of Würzburg, Germany. It brought together experts in the computational analysis of collections of literary texts from France, Germany, Poland and the US. This report provides some context on the DARIAH expert workshops and inroduces the specific goals of the workshop reported on here. Then, it summarizes the major issues raised by the participants in their initial statements and debated in the ensuing discussions. Finally, it describes the key results from the workshop, which include advances int he areas of data, tools and methods for quantitative text analysis.},
	language = {en},
	number = {2},
	institution = {DARIAH-DE},
	author = {Schöch, Christof and Jannidis, Fotis},
	year = {2013},
	note = {00001},
	keywords = {obj\_DigitalHumanities, goal\_Analysis, obj\_Humanities},
	pages = {13}
}

@book{mounier_readwrite_2012,
	title = {Read/{Write} {Book} 2 : {Une} introduction aux humanités numériques},
	copyright = {© OpenEdition Press, 2012 Conditions d'utilisation : http://www.openedition.org/6540},
	shorttitle = {Read/{Write} {Book} 2},
	url = {http://books.openedition.org/oep/226},
	abstract = {Qu’est-ce que les humanités numériques ? Apparue en 2006, l’expression connaît depuis un véritable succès. Mais au-delà du slogan à la mode, quelle est la réalité des pratiques qu’il désigne ? Si tout le monde s’accorde sur une définition minimale à l’intersection des technologies numériques et des sciences humaines et sociales, les vues divergent lorsqu’on entre dans le vif du sujet. Les humanités numériques représentent-elles une véritable révolution des pratiques de recherche et des paradigmes intellectuels qui les fondent ou, plus simplement, une optimisation des méthodes existantes ? Constituent-elles un champ suffisamment structuré pour justifier une réforme des modes de financement de la recherche, des cursus de formation, des critères d’évaluation ? L’archive numérique offre-t-elle à la recherche suffisamment de garanties ? Quelle place la recherche « dirigée par les données » laisse-t-elle à l’interprétation ? Telles sont quelques-unes des questions abordées par ce deuxième opus de la collection « Read/Write Book ». Ces dix-huit textes essentiels, rédigés ou traduits en français par des chercheurs de différentes nationalités, proposent une introduction aux humanités numériques accessible à tous ceux qui souhaitent en savoir plus sur ce domaine de recherche en constante évolution.},
	language = {fr},
	urldate = {2013-10-19},
	publisher = {OpenEdition Press},
	editor = {Mounier, Pierre},
	year = {2012}
}

@book{bode_reading_2012,
	address = {New York},
	title = {Reading by numbers: recalibrating the literary field},
	isbn = {978-0-85728-454-9},
	shorttitle = {Reading by numbers},
	url = {http://www.anthempress.com/reading-by-numbers},
	abstract = {‘Reading by Numbers: Recalibrating the Literary Field’ proposes and demonstrates a new digital approach to literary history. Drawing on bibliographical information on the Australian novel in the AustLit database, the book addresses debates and issues in literary studies through a method that combines book history’s pragmatic approach to literary data with the digital humanities’ idea of computer modelling as an experimental and iterative practice. As well as showcasing this method, the case studies in ‘Reading by Numbers’ provide a revised history of the Australian novel, focusing on the nineteenth century and the decades since the end of the Second World War, and engaging with a range of themes including literary and cultural value, authorship, gender, genre and the transnational circulation of fiction. The book’s findings challenge established arguments in Australian literary studies, book history, feminism and gender studies, while presenting innovative ways of understanding literature, publishing, authorship and reading, and the relationships between them. More broadly, by demonstrating critical ways in which the growing number of digital archives in the humanities can be mined, modelled and visualised, ‘Reading by Numbers’ offers new directions and scope for digital humanities research.},
	language = {en},
	publisher = {Anthem Press},
	author = {Bode, Katherine},
	year = {2012},
	keywords = {AnalyzeStatistically, X-CHECK, bigdata, goal\_Analysis}
}

@misc{canet_reflexiones_2014,
	type = {text},
	title = {Reflexiones sobre las humanidades digitales},
	copyright = {http://creativecommons.org/licenses/by/3.0/es/},
	url = {http://www.janusdigital.es/anexos/contribucion.htm;jsessionid=4F686E22E5A31F197F803425C7932E9A?id=4},
	abstract = {Revista Digital JANUS - Estudios sobre el Siglo de Oro.
A partir de mi experiencia personal a lo largo de 30 años en edición digital de revistas y libros electrónicos; creación, preservación y gestión de bibliotecas y archivos digitales; investigación basada en ordenadores y aplicaciones informáticas para estudios literarios, lingüísticos, culturales e históricos; análisis de textos, corpus, marcación de corpus, bases de datos, etc., llego a la conclusión de que las Humanidades digitales han avanzado mediante el autoaprendizaje y también han sido posibles grandes proyectos a través del trabajo en equipo y la interdisciplinariedad, junto con el aprendizaje continuo. Lo que me lleva a reflexionar si debería haber estudios específicos en Humanidades digitales y, si así fuera, cuáles deberían ser. Hago un breve repaso a diferentes másteres existentes en la actualidad en EE.UU., Inglaterra y España, resaltando el alto contenido en TIC, y finalmente doy mi propuesta sobre las posibles alternativas futuras de las Humanidades digitales.},
	language = {es},
	urldate = {2014-05-04},
	author = {Canet, José Luis},
	year = {2014},
	keywords = {meta\_Assessing, obj\_DigitalHumanities}
}

@book{jones_roberto_2016,
	address = {London},
	title = {Roberto {Busa}, {S}. {J}., and the {Emergence} of {Humanities} {Computing}. {The} {Priest} and the {Punched} {Cards}},
	isbn = {978-1-138-18677-4},
	urldate = {2017-10-20},
	publisher = {Routledge},
	author = {Jones, Steven E.},
	year = {2016},
	keywords = {obj\_DigitalHumanities, act\_Conceptualizing, act\_RelationalAnalysis}
}

@book{fiormonte_scrittura_2003,
	address = {Turino},
	title = {Scrittura e filologia nell'era digitale},
	url = {http://www.bollatiboringhieri.it/scheda.php?codice=9788833957135},
	abstract = {Questo libro è un’introduzione completa e accessibile al fenomeno della testualità digitale rivolta a studenti e ricercatori dell’area umanistica. Il volume è strutturato in quattro parti più un’appendice didattica disponibile in rete e costantemente aggiornata (www.bollatiboringhieri.it). Nella prima parte viene discusso il rapporto fra la comunicazione scritta e i suoi supporti. Lo scopo è mostrare come l’impatto di ciascun sistema di media – informatica inclusa – dia luogo a risultati complessi e contradditori intrecciandosi con fenomeni sociali, culturali, economici. Nella seconda parte l’autore analizza le principali forme della testualità digitale, dalle sue origini (la videoscrittura, l’ipertesto) ai giorni nostri (la comunicazione interattiva, i giochi di ruolo, la retorica del Web). Vengono poi affrontati i nodi teorici e pratici del rapporto fra critica testuale e digitalizzazione del documento. Prendendo spunto dallo studio delle varianti d’autore viene presentato Varianti digitali, un archivio on-line per la didattica e lo studio del processo di scrittura. Accompagna e conclude il volume una rassegna delle più importanti risorse on-line e off-line per la filologia digitale.},
	language = {it},
	publisher = {Bollati Boringhieri},
	author = {Fiormonte, Domenico},
	year = {2003},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities}
}

@book{manovich_software_2013,
	address = {New York},
	title = {Software takes command},
	isbn = {978-1-62356-745-3 978-1-62356-817-7},
	url = {http://manovich.net/index.php/projects/software-takes-command},
	abstract = {Software has replaced a diverse array of physical, mechanical, and electronic technologies used before 21st century to create, store, distribute and interact with cultural artifacts. It has become our interface to the world, to others, to our memory and our imagination - a universal language through which the world speaks, and a universal engine on which the world runs. What electricity and combustion engine were to the early 20th century, software is to the early 21st century. Offering the the first theoretical and historical account of software for media authoring and its effects on the practice and the very concept of 'media,' the author of The Language of New Media (2001) develops his own theory for this rapidly-growing, always-changing field. What was the thinking and motivations of people who in the 1960 and 1970s created concepts and practical techniques that underlie contemporary media software such as Photoshop, Illustrator, Maya, Final Cut and After Effects? How do their interfaces and tools shape the visual aesthetics of contemporary media and design? What happens to the idea of a 'medium' after previously media-specific tools have been simulated and extended in software? Is it still meaningful to talk about different mediums at all? Lev Manovich answers these questions and supports his theoretical arguments by detailed analysis of key media applications such as Photoshop and After Effects, popular web services such as Google Earth, and the projects in motion graphics, interactive environments, graphic design and architecture. Software Takes Command is a must for all practicing designers and media artists and scholars concerned with contemporary media.},
	language = {en},
	publisher = {Bloomsbury},
	author = {Manovich, Lev},
	year = {2013}
}

@misc{connor_software_2013,
	title = {Software {Takes} {Command}: {An} {Interview} with {Lev} {Manovich}},
	url = {http://rhizome.org/editorial/2013/jul/10/lev-manovich-interview/},
	abstract = {Lev Manovich is a leading theorist of cultural objects produced with digital technology, perhaps best known for The Language of New Media (MIT Press, 2001). I interviewed him about his most recent book, Software Takes Command (Bloomsbury Academic, July 2014).},
	language = {en},
	journal = {Rhizome},
	author = {Connor, Michael},
	month = oct,
	year = {2013},
	keywords = {meta\_Theorizing, obj\_Databases, obj\_Software}
}

@book{frabetti_software_nodate,
	series = {Media {Philosophy}},
	title = {Software {Theory}. {A} {Cultural} and {Philosophical} {Study}.},
	isbn = {978-1-78348-197-2},
	url = {http://www.rowmaninternational.com/books/software-theory},
	abstract = {This book engages directly in close readings of technical texts and computer code in order to show how software works. It offers an analysis of the cultural, political, and philosophical implications of software technologies that demonstrates the significance of software for the relationship between technology, philosophy, culture, and society.},
	language = {en},
	publisher = {Rowman \& Littlefield},
	author = {Frabetti, Frederica},
	keywords = {obj\_Code, activity: Assess}
}

@article{potter_statistical_1991,
	title = {Statistical {Analysis} of {Literature}. {A} {Retrospective} on {Computers} and the {Humanities}, 1966-1990},
	url = {http://link.springer.com/article/10.1007%2FBF00141190},
	abstract = {This retrospective on statistical analysis of literature in the first twenty-four years of Computers and the Humanities divides the essays under review into four groups: the philosophical, the statistical analyses of language, the statistical analyses of literary texts, and the statistical analyses of themes. It begins with the question: must valid statistical analysis of any literary text be based on a complete linguistic description of the language of the text? It summarizes and evaluates over forty essays, giving details on works discussed, sample sizes used, statistical methods applied, and quotations from the researchers. The essay ends with a polemical summary of what has been done and what the future holds. It emphasizes the importance of extended pre-computational stages of learning about language and discourse analysis; reading previous research, building on and challenging theory; and the use of carefully crafted, small databases to test specific questions.},
	language = {en},
	number = {25},
	journal = {Computers and the Humanities},
	author = {Potter, Rosanne G.},
	year = {1991},
	keywords = {AnalyzeStatistically, meta\_GiveOverview, t\_Stylometry, obj\_Literature, act\_StylisticAnalysis},
	pages = {401--429}
}

@article{best_surface_nodate,
	series = {Representations, {University} of {California} {Press}},
	title = {Surface {Reading}: {An} {Introduction}},
	volume = {108},
	url = {http://www.jstor.org/stable/10.1525/rep.2009.108.1.1},
	doi = {DOI:10.1525/rep.2009.108.1.1},
	abstract = {In the text-based disciplines, psychoanalysis and Marxism have had a major influence on how we read, and this has been expressed most consistently in the practice of symptomatic reading, a mode of interpretation that assumes that a text's truest meaning lies in what it does not say, describes textual surfaces as superfluous, and seeks to unmask hidden meanings. For symptomatic readers, texts possess meanings that are veiled, latent, all but absent if it were not for their irrepressible and recurring symptoms. Noting the recent trend away from ideological demystification, this essay proposes various modes of "surface reading" that together strive to accurately depict the truth to which a text bears witness. Surface reading broadens the scope of critique to include the kinds of interpretive activity that seek to understand the complexity of literary surfaces---surfaces that have been rendered invisible by symptomatic reading.},
	number = {1 (Fall 2009)},
	author = {Best, Stephen and Marcus, Sharon},
	keywords = {obj\_Text, act\_ContentAnalysis, act\_RelationalAnalysis, act\_Theorizing, act\_Query/Retrieve},
	pages = {1--21},
	file = {Best_Marcus_Surface Reading.pdf:/home/lisnux/Zotero/storage/JBQXYYQL/Best_Marcus_Surface Reading.pdf:application/pdf}
}

@article{tahmasebi_survey_2018,
	title = {Survey of computational approaches to diachronic conceptual change},
	journal = {arXiv preprint arXiv:1811.06278},
	author = {Tahmasebi, Nina and Borin, Lars and Jatowt, Adam},
	year = {2018}
}

@book{kelly_teaching_nodate,
	series = {Digital humanities},
	title = {Teaching history in the digital age},
	isbn = {978-0-472-11878-6},
	url = {http://www.press.umich.edu/3526836/teaching_history_in_the_digital_age},
	abstract = {A practical guide on how one professor employs the transformative changes of digital media in the research, writing, and teaching of history},
	language = {en},
	author = {Kelly, T. Mills},
	keywords = {obj\_DigitalHumanities, meta\_Teaching/Learning}
}

@book{vanhoutte_tei_2010,
	title = {{TEI} by {Example}},
	url = {http://tbe.kantl.be/TBE/TBE.htm},
	abstract = {TEI By Example offers a series of freely available online tutorials walking individuals through the different stages in marking up a document in TEI (Text Encoding Initiative).},
	language = {en},
	publisher = {Centre for Scholarly Editing and Document Studies (CTB) of the Royal Academy of Dutch Language and Literature, the Centre for Computing in the Humanities (CCH) of King's College London, and the Department of Information Studies of University College London.},
	editor = {Vanhoutte, Edward and Terras, Melissa and Van den Branden, Ron},
	year = {2010},
	keywords = {t\_Encoding, act\_Modeling, meta\_Teaching}
}

@incollection{siemens_text_2004,
	address = {Oxford},
	edition = {Hardcover},
	series = {Blackwell {Companions} to {Literature} and {Culture}},
	title = {Text {Encoding}},
	isbn = {978-1-4051-0321-3},
	url = {http://www.digitalhumanities.org/companion/view?docId=blackwell/9781405103213/9781405103213.xml&chunk.id=ss1-3-5&toc.depth=1&toc.id=ss1-3-5&brand=default},
	abstract = {Text encoding holds a special place in humanities computing. It is not only of considerable practical importance and commonly used, but it has proven to be an exciting and theoretically productive area of analysis and research. Text encoding in the humanities has also produced a considerable amount of interesting debate – which can be taken as an index of both its practical importance and its theoretical significance.},
	language = {en},
	urldate = {2010-05-17},
	booktitle = {A {Companion} to {Digital} {Humanities}},
	publisher = {Blackwell Publishing Professional},
	author = {Renear, Allen H.},
	editor = {Siemens, Ray and Unsworth, John and Schreibman, Susan},
	year = {2004},
	keywords = {meta\_GiveOverview, act\_Publishing, meta\_Theorizing, t\_Encoding, X-CHECK}
}

@article{kirschenbaum_txtual_2013,
	title = {The .txtual {Condition}: {Digital} {Humanities}, {Born}-{Digital} {Archives}, and the {Future} {Literary}},
	volume = {7},
	url = {http://digitalhumanities.org/dhq/vol/7/1/000151/000151.html},
	abstract = {In 1995 in the midst of the first widespread wave of digitization, the Modern Language Association issued a Statement on the Significance of Primary Records in order to assert the importance of retaining books and other physical artifacts even after they have been microfilmed or scanned for general consumption. "A primary record," the MLA told us then, "can appropriately be defined as a physical object produced or used at the particular past time that one is concerned with in a given instance" (27). Today, the conceit of a "primary record" can no longer be assumed to be coterminous with that of a "physical object." Electronic texts, files, feeds, and transmissions of all sorts are also now, indisputably, primary records. In the specific domain of the literary, a writer working today will not and cannot be studied in the future in the same way as writers of the past, because the basic material evidence of their authorial activity — manuscripts and drafts, working notes, correspondence, journals — is, like all textual production, increasingly migrating to the electronic realm. This essay therefore seeks to locate and triangulate the emergence of a .txtual condition — I am of course remediating Jerome McGann’s influential notion of a “textual condition” — amid our contemporary constructions of the "literary", along with the changing nature of literary archives, and lastly activities in the digital humanities as that enterprise is now construed. In particular, I will use the example of the Maryland Institute for Technology in the Humanities (MITH) at the University of Maryland as a means of illustrating the kinds of resources and expertise a working digital humanities center can bring to the table when confronted with the range of materials that archives and manuscript repositories will increasingly be receiving.},
	number = {7.1},
	journal = {Digital Humanities Quarterly},
	author = {Kirschenbaum, Matthew},
	year = {2013},
	keywords = {meta\_Theorizing, obj\_Artefacts, act\_Archiving}
}

@book{amy_e_earhart_and_andrew_jewell_american_2011,
	title = {The {American} {Literature} {Scholar} in the {Digital} {Age}},
	url = {http://hdl.handle.net/2027/spo.9362034.0001.001},
	abstract = {Observing the title and concerns of this collection, many may wonder why we have chosen to focus on the American literature scholar; certainly the concerns of digital humanities are relevant across literary specializations. In fact, as other digital humanities scholarship demonstrates, the humanities as a boundary is itself suspect: it is not uncommon to see collaborations between a literary scholar, a computer scientist, and a librarian in digital humanities work. The artificial distinctions that have replicated the discipline divisions have become less relevant to those working in digital humanities, who often group around subject matter, not training. Add to this the increasing breakdown of national boundaries in literary studies, and perhaps it seems antiquated or anathema to reproduce American as a term with which to saddle a supposedly cutting-edge collection of essays.},
	language = {en},
	author = {Amy E. Earhart {and} Andrew Jewell, Editors},
	year = {2011},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities, obj\_Literature}
}

@book{kenny_computation_1982,
	title = {The computation of style: an introduction to statistics for students of literature and humanities},
	isbn = {978-0-08-024282-8},
	shorttitle = {The computation of style},
	url = {http://www.sciencedirect.com/science/book/9780080242811},
	abstract = {Each year more and more scholars are becoming aware of the importance of the statistical study of literary texts. The present book is the first elementary introduction in English for those wishing to use statistical techniques in the study of literature. Unlike other introductions to statistics, it specifically emphasizes those techniques most useful in literary contexts and gives examples of their application from literary and linguistic material. The text is aimed at those with the minimum of mathematical background and gives exercises for the student and relevant statistical tables.},
	language = {en},
	publisher = {Pergamon Press},
	author = {Kenny, Anthony},
	year = {1982},
	keywords = {AnalyzeStatistically}
}

@article{berry_computational_2011,
	title = {The {Computational} {Turn}: {Thinking} {About} the {Digital} {Humanities}},
	volume = {12},
	url = {http://www.culturemachine.net/index.php/cm/article/viewArticle/440},
	language = {en},
	journal = {Culture Machine},
	author = {Berry, David M.},
	year = {2011},
	keywords = {meta\_Assessing, obj\_DigitalHumanities, act\_Conceptualizing}
}

@misc{robertson_differences_2014,
	title = {The {Differences} between {Digital} {History} and {Digital} {Humanities}},
	url = {http://drstephenrobertson.com/2014/05/23/the-differences-between-digital-history-and-digital-humanities/},
	abstract = {For the last nine months I've spent much of my time exploring digital history. Part of becoming director of RRCHNM involved familiarizing myself with areas of work about which I had only passing kn...},
	language = {en},
	urldate = {2014-05-24},
	journal = {Dr Stephen Robertson},
	author = {Robertson, Stephen},
	year = {2014},
	keywords = {meta\_GiveOverview, obj\_DigitalHumanities}
}

@book{hall_digital_2011,
	series = {Culture {Machine}},
	title = {The {Digital} {Humanities}: {Beyond} {Computing}},
	url = {http://www.culturemachine.net/index.php/cm/issue/view/23},
	abstract = {Just as interesting as what computer science has to offer the humanities is the question of what the humanities have to offer computer science; what the humanities themselves can bring to the understanding of computing and the shaping of the digital. Do the humanities really need to draw quite so heavily on computer science to develop a sense of their identity and role in an era of digital media technology? Along with a computational turn in the humanities, might we not also benefit from more of a humanities turn in our understanding of the computational and the digital? 


Not just to what extent is it possible for the emerging digital humanities to go beyond the disciplinary objects, affiliations, assumptions and methodological practices of computing science and engineering, science and technology, or even science in general; but to what extent is it possible for the emerging digital humanities to go beyond the human-ities too?},
	language = {en},
	publisher = {Gary Hall et al.},
	editor = {Hall, Gary},
	year = {2011},
	keywords = {obj\_DigitalHumanities, meta\_Theorizing}
}

@article{vis_critical_2013-1,
	title = {A critical reflection on {Big} {Data}: {Considering} {APIs}, researchers and tools as data makers},
	volume = {18},
	copyright = {Authors submitting a paper to First Monday automatically agree to confer a limited license to First Monday if and when the manuscript is accepted for publication. This license allows First Monday to publish a manuscript in a given issue. Authors have a choice of: 1. Dedicating the article to the public domain. This allows anyone to make any use of the article at any time, including commercial use. A good way to do this is to use the Creative Commons Public Domain Dedication Web form; see  http://creativecommons.org/license/publicdomain-2?lang=en . 2. Retaining some rights while allowing some use. For example, authors may decide to disallow commercial use without permission. Authors may also decide whether to allow users to make modifications (e.g. translations, adaptations) without permission. A good way to make these choices is to use a Creative Commons license. * Go to  http://creativecommons.org/license/ . * Choose and select a license. * What to do next — you can then e–mail the license html code to yourself. Do this, and then forward that e–mail to First Monday’s editors. Put your name in the subject line of the e–mail with your name and article title in the e–mail. Background information about Creative Commons licenses can be found at  http://creativecommons.org/about/licenses/ . 3. Retaining full rights, including translation and reproduction rights. Authors may use the statement: © Author 2008 All Rights Reserved. Authors may choose to use their own wording to reserve copyright. If you choose to retain full copyright, please add your copyright statement to the end of the article. Authors submitting a paper to First Monday do so in the understanding that Internet publishing is both an opportunity and challenge. In this environment, authors and publishers do not always have the means to protect against unauthorized copying or editing of copyright–protected works.},
	issn = {13960466},
	shorttitle = {A critical reflection on {Big} {Data}},
	url = {http://firstmonday.org/ojs/index.php/fm/article/view/4878},
	doi = {10.5210/fm.v18i10.4878},
	abstract = {This paper looks at how data is ‘made’, by whom and how. Rather than assuming data already exists ‘out there’, waiting to simply be recovered and turned into findings, the paper examines how data is co–produced through dynamic research intersections. A particular focus is the intersections between the application programming interface (API), the researcher collecting the data as well as the tools used to process it. In light of this, this paper offers three new ways to define and think about Big Data and proposes a series of practical suggestions for making data.},
	language = {en},
	number = {10},
	urldate = {2014-01-28},
	journal = {First Monday},
	author = {Vis, Farida},
	month = oct,
	year = {2013},
	keywords = {bigdata}
}

@inproceedings{hakinson_interchange_2010-1,
	address = {Utrecht, Netherlands},
	title = {An {Interchange} {Format} for {Optical} {Music} {Recognition} {Applications}},
	url = {http://ismir2010.ismir.net/proceedings/ismir2010-11.pdf},
	abstract = {Page
appearance and
layout for
music notation is a
cri
tical component of the overall musical information
contained in a document
.
To capture and transfe
r this
information, we outline an interchange format for OMR
applications, the OMR Interchange Package (OIP)
format,
which is
designed to allow layout information
and page images to be
preserved and
transferred along
with semantic musical content. We ident
ify a number of
uses for this format that can enhance digital
representations of music, and introduce a novel
idea for
distributed optical music recognition
system
based on this
format},
	language = {en},
	booktitle = {In {Proceedings} of the 11th {International} {Society} for {Music} {Information} {Retrieval} {Conference} ({ISMIR} 2010)},
	author = {Hakinson, A. and Pugin, Laurent and Fujinaga, I.},
	year = {2010},
	keywords = {act\_DataRecognition, obj\_SheetMusic},
	pages = {51--56}
}

@book{tennison_beginning_2005,
	address = {Berkeley, CA : New York},
	title = {Beginning {XSLT} 2.0: from novice to professional},
	isbn = {978-1-59059-324-0},
	shorttitle = {Beginning {XSLT} 2.0},
	abstract = {This followup to Jeni Tennison's Beginning XSLT has been updated to accomodate the revised XSLT standard. Part one of this book introduces XML and XSLT at a comfortable pace, and gradually demonstrates techniques for generating HTML (plus other formats), from XML. In part two, Tennison applies theory to real-life XSLT capabilities - including generating graphics.

Each chapter includes step-by-step examples, plus review questions at the end, to help you grasp the discussed features.},
	language = {en},
	publisher = {Apress ; Distributed to the book trade in the United States by Springer-Verlag},
	author = {Tennison, Jeni},
	year = {2005},
	keywords = {act\_Modeling}
}

@article{aurast_big_2016,
	title = {Big {Data} und {Smart} {Data} in den {Geisteswissenschaften}},
	volume = {40},
	doi = {DOI: https://doi.org/10.1515/bfp-2016-0033},
	abstract = {With networked large data collections and suitable quantitative methods new research questions become possible. Within the DARIAH-DE cluster 5 new philologico-critical and historical comparing perspectives enter the modelling of large data collections. Consequently new, tailored analytical tools can be developed. This includes the chance to test in empirical ways theoretical presuppositions and the extension and advancement of the modes of the production of scientific knowledge. The article describes two analytical tools developed within cluster 5: the Dariah-DKPro-Wrapper, a software package for the automated linguistical analysis, and Cosmotool, a search- and analysis tool for the exploration of transboundary lives by means of structured and unstructured data collections.},
	number = {2},
	journal = {Bibliothek Forschung und Praxis},
	author = {Aurast, Anna and Gradl, Tobias and Pernes, Stefan and Pielström, Steffen},
	year = {2016},
	keywords = {obj\_Tools, goal\_Dissemination, goal\_Analysis, act\_ContentAnalysis, obj\_Data, act\_RelationalAnalysis, meta\_Teaching, act\_Collaborating},
	file = {Aurast et al_2016_Big Data und Smart Data in den Geisteswissenschaften.pdf:/home/lisnux/Zotero/storage/Z5PZZSM7/Aurast et al_2016_Big Data und Smart Data in den Geisteswissenschaften.pdf:application/pdf}
}

@incollection{ouyang_complex_2009,
	address = {Montreal, QC, Canada},
	title = {Complex {Layout} {Analysis} of {Medieval} {Music} {Manuscripts} for {Information} {Extraction} and {Optical} {Recognition}},
	language = {en},
	booktitle = {In {Proceedings} of the 2009 {International} {Computer} {Music} {Conference} ({ICMC} 2009)},
	author = {Ouyang, Y. and Burgoyne, J. A. and Pugin, Laurent and Fujinaga, I.},
	year = {2009},
	keywords = {act\_DataRecognition, obj\_SheetMusic},
	pages = {101--104}
}

@misc{cover_conceptual_nodate,
	title = {Conceptual {Modeling} and {Markup} {Languages}},
	url = {http://xml.coverpages.org/conceptualModeling.html},
	abstract = {This document contains information relevant to 'Conceptual Modeling and Markup Languages' and is part of the Cover Pages resource. The Cover Pages is a comprehensive Web-accessible reference collection supporting the SGML/XML family of (meta) markup language standards and their application. The principal objective in this public access knowledgebase is to promote and enable the use of open, interoperable standards-based solutions which protect digital information and enhance the integrity of communication. Some of the standards covered include SGML (Standard Generalized Markup Language), XML (Extensible Markup Language), W3C XML Schema, Other XML Schema Languages (RELAX NG, Schematron, DSDL), XSL (Extensible Stylesheet Language), XSLT (XSL Transformations), XPath (XML Path Language), XLink (XML Linking), XML Query, XHTML (Extensible HyperText Markup Language), DOM (Document Object Model), XPointer (XML Pointer Language), HyTime, RDF, Topic Maps, DSSSL, CSS (Cascading Style Sheets), SPDL, SVG (Scalable Vector Graphics), CGM, ISO-HTML, etc. A secondary objective in The Cover Pages is to provide reference material on enabling technologies compatible with descriptive markup language standards and applications: object modeling, semantic nets, ontologies, authority lists, document production systems, and conceptual modeling. The reference collection contains over 9000 documents covering more than 700 topics on markup language technologies.},
	language = {en},
	urldate = {2011-11-01},
	author = {Cover, Robin},
	keywords = {obj\_AnyObject, act\_Modeling}
}

@article{rapp_memex-idee_2013,
	title = {Die {Memex}-{Idee}. {Vannevar} {Bush} und die maschinelle {Erweiterung} des {Denkens}},
	volume = {4},
	issn = {1863-8937},
	url = {http://www.z-i-g.de/vorschau.cfm?akt=1},
	language = {de},
	author = {Rapp, Andrea and Bender, Michael},
	year = {2013},
	keywords = {obj\_Text, obj\_Infrastructures, obj\_Documents, meta\_Collaborating, obj\_Knowledge},
	pages = {53--64}
}

@book{hedges_digital_2014,
	address = {[S.l.]},
	title = {Digital asset management handbook.},
	isbn = {978-1-85604-935-1},
	url = {http://www.amazon.de/Digital-Asset-Management-Theory-Practice/dp/1856049353},
	abstract = {This practical handbook provides information professionals with everything they need to know to effectively manage digitial content and information. The book addresses digital asset management (DAM) from a practitioner's point of view but also introduces readers to the theoretical background to the subject. It will thus equip readers with a range of essential strategic, technical and practical skills required to direct digital asset management activities within their area of business, while also providing them a well-rounded and critical understanding of the issues across domains. The Digital Asset Management Handbook includes an evolving case study that serves to illustrate the topics and issues addressed in each chapter, as well as a sequence of practical exercises using freely available DAM software. Readership: Information professionals who work (or aim to work) in the digital content industries and managers of digital assets of various forms. Cultural and memory institutions, digital archives, and any areas of science, government and business organisation where there is a need to curate digital assets. Students taking LIS graduate courses worldwide.},
	language = {en},
	publisher = {Facet Publishing},
	author = {Hedges, Mark},
	year = {2014},
	keywords = {goal\_Dissemination, obj\_AnyObject, goal\_Storage}
}

@inproceedings{irvine_digitizing_2012,
	address = {Montréal, Canada},
	title = {Digitizing 18th-{Century} {French} {Literature}: {Comparing} transcription methods for a critical edition text},
	url = {http://www.aclweb.org/anthology/W12-2509},
	abstract = {We compare four methods for transcribing early printed texts. Our comparison is through a case-study of digitizing an eighteenth- century French novel for a new critical edition: the 1784 Lettres ta??tiennes by Jose?phine de Monbart. We provide a detailed error analy- sis of transcription by optical character recog- nition (OCR), non-expert humans, and expert humans and weigh each technique based on accuracy, speed, cost and the need for schol- arly overhead. Our findings are relevant to 18th-century French scholars as well as the entire community of scholars working to pre- serve, present, and revitalize interest in litera- ture published before the digital age.},
	language = {en},
	booktitle = {Proceedings of the {NAACL}-{HLT} 2012 {Workshop} on {Computational} {Linguistics} for {Literature}},
	publisher = {Association for Computational Linguistics},
	author = {Irvine, Ann and Marcellesi, Laure and Zomorodian, Afra},
	month = jun,
	year = {2012},
	keywords = {act\_DataRecognition},
	pages = {64--68}
}

@inproceedings{burgoyne_enhanced_2008,
	address = {Philadelphia, PA},
	title = {Enhanced {Bleedthrough} {Correction} for {Early} {Music} {Documents} with {Recto}-{Verso} {Registration}},
	url = {http://www.aruspix.net/publications/bourgoyne08enhanced.pdf},
	abstract = {Ink bleedthrough is common problem in early music documents.
Even when such bleedthrough does not pose problems
for human perception, it can inhibit the performance
of optical music recognition (OMR). One way to reduce
the amount of bleedthrough is to take into account what is
printed on the reverse of the page. In order to do so, the
reverse of the page must be registered to match the front of
the page on a pixel-by-pixel basis. This paper describes our
approach to registering scanned early music scores as well
as our modifications to two robust binarization approaches
to take into account bleedthrough and the information available
from the registration process. We determined that although
the information from registration itself often makes
little difference in recognition performance, other modifications
to binarization algorithms for correcting bleedthrough
can yield dramatic increases in OMR results.},
	language = {en},
	booktitle = {Proceedings of the 9th {International} {Conference} on {Music} {Information} {Retrieval} ({ISMIR} 2008)},
	author = {Burgoyne, John Ashley and Devaney, Johanna and Pugin, Laurent and Fujinaga, Ichiro},
	year = {2008},
	keywords = {act\_DataRecognition, obj\_SheetMusic},
	pages = {407--12}
}

@incollection{pugin_gamera_2008,
	address = {Philadelphia, PA},
	title = {Gamera {Versus} {Aruspix}: {Two} {Optical} {Music} {Recognition} {Approaches}},
	url = {http://www.aruspix.net/publications/pugin08aruspix.pdf},
	abstract = {Optical music recognition (OMR) applications are predominantly
designed for common music notation and as such,
are inherently incapable of adapting to specialized notation
forms within early music. Two OMR systems, namely Gamut
(a Gamera application) and Aruspix, have been proposed
for early music. In this paper, we present a novel
comparison of the two systems, which use markedly different
approaches to solve the same problem, and pay close
attention to the performance and learning rates of both applications.
In order to obtain a complete comparison of Gamut
and Aruspix, we evaluated the core recognition systems and
the pitch determination processes separately. With our experiments,
we were able to highlight the advantages of both
approaches as well as causes of problems and possibilities
for future improvements.},
	language = {en},
	booktitle = {Proceedings of the 9th {International} {Conference} on {Music} {Information} {Retrieval} ({ISMIR} 2008)},
	author = {Pugin, Laurent and Hockman, J. and Burgoyne, J. A. and Fujinaga, I.},
	year = {2008},
	keywords = {act\_DataRecognition, obj\_SheetMusic},
	pages = {419--424}
}

@inproceedings{pugin_goal-directed_2007,
	address = {Vancouver, Canada},
	title = {Goal-{Directed} {Evaluation} for the {Improvement} of {Optical} {Music} {Recognition} on {Early} {Music} {Prints}},
	url = {http://www.aruspix.net/publications/pugin07goal-directed.pdf},
	abstract = {Optical music recognition (OMR) systems are promising
tools for the creation of searchable digital music libraries.
Using an adaptive OMR system for early music prints based
on hidden Markov models, we leverage an edit-distance evaluation
metric to improve recognition accuracy. Baseline results
are computed with new labeled training and test sets
drawn from a diverse group of prints. We present two experiments
based on this evaluation technique. The first resulted
in a significant improvement to the feature extraction function
for these images. The second is a goal-directed comparison
of several popular adaptive binarization algorithms,
which are often evaluated only subjectively. Accuracy increased
by as much as 55\% for some pages, and the experiments
suggest several avenues for further research.},
	language = {en},
	booktitle = {In {Proceedings} of the {ACM}-{IEEE} {Joint} {Conference} on {Digital} {Libraries} ({JCDL} 2007)},
	author = {Pugin, Laurent and Burgoyne, John Ashley and Fujinaga, Ichiro},
	year = {2007},
	keywords = {act\_DataRecognition, obj\_SheetMusic},
	pages = {303--04}
}

@incollection{durusau_how_2006,
	address = {New York},
	title = {How and {Why} to {Formalize} {Your} {Markup}},
	isbn = {978-0-87352-970-9},
	url = {http://www.tei-c.org/About/Archive_new/ETE/Preview/durusau.xml},
	abstract = {Why markup decisions should be formalized
Formalizing markup: a process of discovery
Formalizing markup: a process of training
Formalizing markup: a process of validation
Formalizing markup: a process of review
Conclusion},
	language = {en},
	booktitle = {Electronic textual editing},
	publisher = {Modern Language Association of America},
	author = {Durusau, Patrick},
	editor = {Burnard, Lou},
	year = {2006},
	keywords = {act\_Modeling}
}

@article{beynon_human_2006,
	title = {Human {Computing}—{Modelling} with {Meaning}},
	volume = {21},
	issn = {0268-1145, 1477-4615},
	url = {http://llc.oxfordjournals.org/content/21/2/141.abstract},
	doi = {10.1093/llc/fql015},
	abstract = {This article is based on a session given by the authors at the ACH/ALLC conference at the University of Victoria in June 2005. It discusses the prospects for partnership between the humanities and computing from the alternative perspective afforded by Empirical Modelling (EM). Perceived dualities that separate the two cultures of science and art are identified as the primary impediment to this partnership. A vision for ‘human computing’ that promises to dissolve these dualities is outlined. The key characteristics and potential for EM for the humanities are illustrated with reference to a modelling exercise on the theme of Schubert's Erlkönig. This highlights how each of the six varieties of modelling identified by McCarty can be represented within an EM model. The implications of EM are discussed with reference to McCarty's account of the key role for modelling in the humanities, in relation to James's ‘philosophic attitude’ of Radical Empiricism and to ideas from phenomenological sources.},
	language = {en},
	number = {2},
	urldate = {2011-11-01},
	journal = {Literary and Linguistic Computing},
	author = {Beynon, Meurig and Russ, Steve and McCarty, Willard},
	month = apr,
	year = {2006},
	keywords = {obj\_AnyObject, act\_Modeling},
	pages = {141--157}
}

@inproceedings{pugin_map_2007,
	address = {Vienna, Austria},
	title = {{MAP} {Adaptation} to {Improve} {Optical} {Music} {Recognition} of {Early} {Music} {Documents} {Using} {Hidden} {Markov} {Models}},
	url = {http://www.aruspix.net/publications/pugin07map.pdf},
	abstract = {Despite steady improvement in optical music recognition
(OMR), early documents remain challenging because of
the high variability in their contents. In this paper, we
present an original approach using maximum a posteriori
(MAP) adaptation to improve an OMR tool for early
typographic prints dynamically based on hidden Markov
models. Taking advantage of the fact that during the normal
usage of any OMR tool, errors will be corrected, and
thus ground-truth produced, the system can be adapted in
real-time. We experimented with five 16th-century music
prints using 250 pages of music and two procedures in
applying MAP adaptation. With only a handful of pages,
both recall and precision rates improved even when the
baseline was above 95 percent.},
	language = {en},
	booktitle = {In {Proceedings} of the 8th {International} {Conference} on {Music} {Information} {Retrieval} ({ISMIR} 2007)},
	author = {Pugin, Laurent and Burgoyne, J. A. and Fujinaga, I.},
	year = {2007},
	keywords = {act\_DataRecognition, obj\_SheetMusic},
	pages = {513--16}
}

@article{tanner_measuring_2009,
	title = {Measuring {Mass} {Text} {Digitization} {Quality} and {Usefulness}: {Lessons} {Learned} from {Assessing} the {OCR} {Accuracy} of the {British} {Library}'s 19th {Century} {Online} {Newspaper} {Archive}},
	volume = {15},
	issn = {1082-9873},
	shorttitle = {Measuring {Mass} {Text} {Digitization} {Quality} and {Usefulness}},
	url = {http://www.dlib.org/dlib/july09/munoz/07munoz.html},
	doi = {10.1045/july2009-munoz},
	abstract = {This article will discuss how to measure the accuracy of Optical Character Recognition (OCR) output in a way that is relevant to the needs of the end users of digital resources. A case study measuring the OCR accuracy of the British Library's 19th Century Newspapers Database provides a clear example of the benefits to be gained from measuring not just character accuracy but also word and significant word accuracy. As OCR primarily facilitates searching, indexing and other means of structuring the user experience of online newspaper archives, measuring the word and significant word accuracy of the OCR output is very revealing of a resource's likely performance for these functions. Having such data is therefore extremely helpful for planning and quality assurance assessment. After briefly discussing the role of OCR in the text capture process and how OCR works, we give a detailed description of the methodology, statistical data gathering techniques and analysis used in this study. Our conclusions point the way forward with suggested actions to assist other mass digitization projects in applying these techniques.},
	language = {en},
	number = {7/8},
	urldate = {2014-09-05},
	journal = {D-Lib Magazine},
	author = {Tanner, Simon and Muñoz, Trevor and Ros, Pich Hemy},
	year = {2009},
	keywords = {act\_DataRecognition, bigdata{\textasciitilde}, obj\_Text, goal\_Capture}
}

@article{haaf_measuring_2013,
	title = {Measuring the {Correctness} of {Double}-{Keying}: {Error} {Classification} and {Quality} {Control} in a {Large} {Corpus} of {TEI}-{Annotated} {Historical} {Text}},
	copyright = {TEI Consortium 2013 (Creative Commons Attribution-NoDerivs 3.0 Unported License)},
	issn = {2162-5603},
	shorttitle = {Measuring the {Correctness} of {Double}-{Keying}},
	url = {http://jtei.revues.org/739},
	doi = {10.4000/jtei.739},
	abstract = {Among mass digitization methods, double-keying is considered to be the one with the lowest error rate. This method requires two independent transcriptions of a text by two different operators. It is particularly well suited to historical texts, which often exhibit deficiencies like poor master copies or other difficulties such as spelling variation or complex text structures.            Providers of data entry services using the double-keying method generally advertise very high accuracy rates (around 99.95\% to 99.98\%). These advertised percentages are generally estimated on the basis of small samples, and little if anything is said about either the actual amount of text or the text genres which have been proofread, about error types, proofreaders, etc. In order to obtain significant data on this problem it is necessary to analyze a large amount of text representing a balanced sample of different text types, to distinguish the structural XML/TEI level from the typographical level, and to differentiate between various types of errors which may originate from different sources and may not be equally severe.         This paper presents an extensive and complex approach to the analysis and correction of double-keying errors which has been applied by the DFG-funded project "Deutsches Textarchiv" (German Text Archive, hereafter DTA) in order to evaluate and preferably to increase the transcription and annotation accuracy of double-keyed DTA texts. Statistical analyses of the results gained from proofreading a large quantity of text are presented, which verify the common accuracy rates for the double-keying method.},
	language = {en},
	number = {Issue 4},
	urldate = {2013-04-01},
	journal = {Journal of the Text Encoding Initiative},
	author = {Haaf, Susanne and Wiegand, Frank and Geyken, Alexander},
	collaborator = {Jannidis, Fotis and Rehbein, Malte and Romary, Laurent},
	month = mar,
	year = {2013},
	keywords = {act\_DataRecognition, bigdata{\textasciitilde}, obj\_Text},
	file = {Haaf et al_2013_Measuring the Correctness of Double-Keying.pdf:/home/lisnux/Zotero/storage/827NU4TD/Haaf et al_2013_Measuring the Correctness of Double-Keying.pdf:application/pdf}
}

@incollection{mccarty_modeling_2004,
	address = {Oxford},
	edition = {Online Edition},
	title = {Modeling: {A} {Study} in {Words} and {Meanings}},
	url = {http://www.digitalhumanities.org/companion/},
	abstract = {The question of modeling arises naturally for humanities computing from the prior question of what its practitioners across the disciplines have in common. What are they all doing with their computers that we might find in their diverse activities indications of a coherent or cohesible practice? How do we make the best, most productive sense of what we observe? There are, of course, many answers: practice varies from person to person, from project to project, and ways of construing it perhaps vary even more. In this chapter I argue for modeling as a model of such a practice. I have three confluent goals: to identify humanities computing with an intellectual ground shared by the older disciplines, so that we may say how and to what extent our field is of as well as in the humanities, how it draws from and adds to them; at the same time to reflect experience with computers "in the wild"; and to aim at the most challenging problems, and so the most intellectually rewarding future now imaginable.},
	language = {en},
	booktitle = {A {Companion} to {Digital} {Humanities}},
	publisher = {Blackwell},
	author = {McCarty, Willard},
	editor = {Schreibman, Susan and Siemens, Ray and Unsworth, John},
	year = {2004},
	keywords = {meta\_Theorizing, goal\_Interpretation, goal\_Analysis, obj\_AnyObject, act\_Modeling},
	pages = {(chapter 19)}
}

@article{piotrowski_natural_2012,
	title = {Natural {Language} {Processing} for {Historical} {Texts}},
	volume = {5},
	issn = {1947-4040, 1947-4059},
	url = {http://www.morganclaypool.com/doi/abs/10.2200/S00436ED1V01Y201207HLT017},
	doi = {10.2200/S00436ED1V01Y201207HLT017},
	abstract = {More and more historical texts are becoming available in digital form. Digitization of paper documents is motivated by the aim of preserving cultural heritage and making it more accessible, both to laypeople and scholars. As digital images cannot be searched for text, digitization projects increasingly strive to create digital text, which can be searched and otherwise automatically processed, in addition to facsimiles. Indeed, the emerging field of digital humanities heavily relies on the availability of digital text for its studies.

Together with the increasing availability of historical texts in digital form, there is a growing interest in applying natural language processing (NLP) methods and tools to historical texts. However, the specific linguistic properties of historical texts -- the lack of standardized orthography, in particular -- pose special challenges for NLP.

This book aims to give an introduction to NLP for historical texts and an overview of the state of the art in this field. The book starts with an overview of methods for the acquisition of historical texts (scanning and OCR), discusses text encoding and annotation schemes, and presents examples of corpora of historical texts in a variety of languages. The book then discusses specific methods, such as creating part-of-speech taggers for historical languages or handling spelling variation. A final chapter analyzes the relationship between NLP and the digital humanities.

Certain recently emerging textual genres, such as SMS, social media, and chat messages, or newsgroup and forum postings share a number of properties with historical texts, for example, nonstandard orthography and grammar, and profuse use of abbreviations. The methods and techniques required for the effective processing of historical texts are thus also of interest for research in other domains.},
	language = {en},
	number = {2},
	urldate = {2013-01-19},
	journal = {Synthesis Lectures on Human Language Technologies},
	author = {Piotrowski, Michael},
	month = sep,
	year = {2012},
	keywords = {meta\_GiveOverview, goal\_Enrichment, goal\_Capture},
	pages = {1--157}
}

@book{dufrene_numerisation_2013,
	address = {Paris},
	title = {Numérisation du patrimoine: quelles médiations? quels accès? quelles cultures?},
	isbn = {978-2-7056-8742-7},
	shorttitle = {Numérisation du patrimoine},
	url = {http://www.amazon.de/Num%C3%A9risation-patrimoine-Quelles-m%C3%A9diations-cultures/dp/2705687424},
	language = {fr},
	publisher = {Hermann},
	author = {Dufrêne, Bernadette and Ihadjadene, Madjid and Bruckmann, Denis},
	year = {2013}
}

@inproceedings{pugin_optical_2006,
	address = {Victoria, Canada},
	title = {Optical {Music} {Recognition} of {Early} {Typographic} {Prints} using {Hidden} {Markov} {Models}},
	url = {http://www.aruspix.net/publications/pugin06optical.pdf},
	abstract = {Music printed with movable type (typographic music) from
the 16th and 17th centuries contains specific graphic features.
In this paper, we present a technique and associated
experiments for performing optical music recognition
on such music prints using HiddenMarkovModels (HMM).
Our original approach avoids the difficult and unreliable removal
of staff lines usually required before processing. The
modeling of symbols on the staff is based on low-level simple
features. We show that, using our technique, these features
are robust enough to obtain good recognition rates even
with poor quality images scanned from microfilm of originals.
The music content retrieved by the optical recognition
process can be put to significant use in, for example, the
creation of searchable digital music libraries.},
	language = {en},
	author = {Pugin, Laurent},
	year = {2006},
	keywords = {act\_DataRecognition, obj\_SheetMusic}
}

@article{rebelo_optical_2012,
	series = {1},
	title = {Optical music recognition: state-of-the-art and open issues},
	url = {http://download.springer.com/static/pdf/527/art%253A10.1007%252Fs13735-012-0004-6.pdf?auth66=1386596381_fe5f560bc7d93233c0ad75e5b7607b50&ext=.pdf},
	doi = {10.1007/s13735-012-0004-6},
	abstract = {For centuries, music has been shared and remembered by two traditions: aural transmission and in the form of written documents normally called musical scores. Many of these scores exist in the form of unpublished manuscripts and hence they are in danger of being lost through the normal ravages of time. To preserve the music some form of typesetting or, ideally, a computer system that can automatically decode the symbolic images and create new scores is required. Programs analogous to optical character recognition systems called optical music recognition (OMR) systems have been under intensive development for many years. However, the results to date are far from ideal. Each of the proposed methods emphasizes different properties and therefore makes it difficult to effectively evaluate its competitive advantages. This article provides an overview of the literature concerning the automatic analysis of images of printed and handwritten musical scores. For self-containment and for the benefit of the reader, an introduction to OMR processing systems precedes the literature overview. The following study presents a reference scheme for any researcher wanting to compare new OMR algorithms against well-known ones.},
	language = {en},
	author = {Rebelo, Ana and Fujinaga, Ichiro and Paszkiewicz, Filipe and Marcal, Andre R. S. and Guedes, Carlos and Cardoso, Jamie S.},
	year = {2012},
	keywords = {meta\_GiveOverview, act\_DataRecognition, obj\_SheetMusic},
	pages = {173--190},
	file = {Rebelo et al_2012_Optical music recognition.pdf:/home/lisnux/Zotero/storage/5TPZT7LE/Rebelo et al_2012_Optical music recognition.pdf:application/pdf}
}

@inproceedings{pugin_reducing_2007,
	address = {Budapest, Hungary},
	title = {Reducing {Costs} for {Digitising} {Early} {Music} with {Dynamic} {Adaptation}},
	url = {http://www.aruspix.net/publications/pugin07reducing.pdf},
	abstract = {Optical music recognition (OMR) enables librarians to digi-
tise early music sources on a large scale. The cost of expert human
labour to correct automatic recognit
ion errors dominates the cost of such
projects. To reduce the number of recognition errors in the OMR pro-
cess, we present an innovative approach to adapt the system dynamically,
taking advantage of the human editing work that is part of any digitisa-
tion project. The corrected data are used to perform MAP adaptation,
a machine-learning technique used previously in speech recognition and
optical character recognition (OCR). Our experiments show that this
technique can reduce editing costs by more than half.},
	language = {en},
	booktitle = {In {Proceedings} of the {European} {Conference} on {Digital} {Libraries} ({ECDL} 2007)},
	author = {Pugin, Laurent and Burgoyne, J. A. and Fujinaga, I.},
	year = {2007},
	keywords = {obj\_SheetMusic, goal\_Capture},
	pages = {471--74}
}

@article{kirschenbaum_txtual_2013-1,
	title = {The .txtual {Condition}: {Digital} {Humanities}, {Born}-{Digital} {Archives}, and the {Future} {Literary}},
	volume = {7},
	url = {http://digitalhumanities.org/dhq/vol/7/1/000151/000151.html},
	abstract = {In 1995 in the midst of the first widespread wave of digitization, the Modern Language Association issued a Statement on the Significance of Primary Records in order to assert the importance of retaining books and other physical artifacts even after they have been microfilmed or scanned for general consumption. "A primary record," the MLA told us then, "can appropriately be defined as a physical object produced or used at the particular past time that one is concerned with in a given instance" (27). Today, the conceit of a "primary record" can no longer be assumed to be coterminous with that of a "physical object." Electronic texts, files, feeds, and transmissions of all sorts are also now, indisputably, primary records. In the specific domain of the literary, a writer working today will not and cannot be studied in the future in the same way as writers of the past, because the basic material evidence of their authorial activity — manuscripts and drafts, working notes, correspondence, journals — is, like all textual production, increasingly migrating to the electronic realm. This essay therefore seeks to locate and triangulate the emergence of a .txtual condition — I am of course remediating Jerome McGann’s influential notion of a “textual condition” — amid our contemporary constructions of the "literary", along with the changing nature of literary archives, and lastly activities in the digital humanities as that enterprise is now construed. In particular, I will use the example of the Maryland Institute for Technology in the Humanities (MITH) at the University of Maryland as a means of illustrating the kinds of resources and expertise a working digital humanities center can bring to the table when confronted with the range of materials that archives and manuscript repositories will increasingly be receiving.},
	number = {7.1},
	journal = {Digital Humanities Quarterly},
	author = {Kirschenbaum, Matthew},
	year = {2013},
	keywords = {meta\_Theorizing, obj\_Artefacts, act\_Archiving}
}

@inproceedings{lauritsen_toward_2009,
	address = {New York, NY, USA},
	series = {{ICAIL} '09},
	title = {Toward a general theory of document modeling},
	isbn = {978-1-60558-597-0},
	url = {http://doi.acm.org/10.1145/1568234.1568257},
	doi = {10.1145/1568234.1568257},
	abstract = {Most legal tasks involve document preparation and review. Drafting effective texts is central to lawyering, judging, legislating, and regulating. How best to support that work with intelligent tools is an ancient topic in AI-and-Law research. For those tools to work, they must have good quality knowledge content to work with. Many alternative theories and techniques for modeling documents have been developed for particular kinds of situations. This article sketches a basic general theory of legal document modeling, with a focus on the key role of argumentation.},
	language = {en},
	urldate = {2011-11-01},
	booktitle = {Proceedings of the 12th {International} {Conference} on {Artificial} {Intelligence} and {Law}},
	publisher = {ACM},
	author = {Lauritsen, Marc and Gordon, Thomas F.},
	year = {2009},
	keywords = {obj\_AnyObject, act\_Modeling},
	pages = {202--211}
}

@article{markham_undermining_2013,
	title = {Undermining ‘data’: {A} critical examination of a core term in scientific inquiry},
	volume = {18},
	copyright = {Authors submitting a paper to First Monday automatically agree to confer a limited license to First Monday if and when the manuscript is accepted for publication. This license allows First Monday to publish a manuscript in a given issue. Authors have a choice of: 1. Dedicating the article to the public domain. This allows anyone to make any use of the article at any time, including commercial use. A good way to do this is to use the Creative Commons Public Domain Dedication Web form; see  http://creativecommons.org/license/publicdomain-2?lang=en . 2. Retaining some rights while allowing some use. For example, authors may decide to disallow commercial use without permission. Authors may also decide whether to allow users to make modifications (e.g. translations, adaptations) without permission. A good way to make these choices is to use a Creative Commons license. * Go to  http://creativecommons.org/license/ . * Choose and select a license. * What to do next — you can then e–mail the license html code to yourself. Do this, and then forward that e–mail to First Monday’s editors. Put your name in the subject line of the e–mail with your name and article title in the e–mail. Background information about Creative Commons licenses can be found at  http://creativecommons.org/about/licenses/ . 3. Retaining full rights, including translation and reproduction rights. Authors may use the statement: © Author 2008 All Rights Reserved. Authors may choose to use their own wording to reserve copyright. If you choose to retain full copyright, please add your copyright statement to the end of the article. Authors submitting a paper to First Monday do so in the understanding that Internet publishing is both an opportunity and challenge. In this environment, authors and publishers do not always have the means to protect against unauthorized copying or editing of copyright–protected works.},
	issn = {13960466},
	shorttitle = {Undermining ‘data’},
	url = {http://firstmonday.org/ojs/index.php/fm/article/view/4868},
	doi = {10.5210/fm.v18i10.4868},
	abstract = {The term ‘data’ functions as a powerful frame for discourse about how knowledge is derived and privileges certain ways of knowing over others. Through its ambiguity, the term can foster a self–perpetuating sensibility that ‘data’ is incontrovertible, something to question the meaning or the veracity of, but not the existence of. This article critically examines the concept of ‘data’ within larger questions of research method and frameworks for scientific inquiry. The current dominance of the term ‘data’ and ‘big data’ in discussions of scientific inquiry as well as everyday advertising focuses our attention on only certain aspects of the research process. The author suggests deliberately decentering the term, to explore nuanced frames for describing the materials, processes, and goals of inquiry.},
	language = {en},
	number = {10},
	urldate = {2014-01-28},
	journal = {First Monday},
	author = {Markham, Annette N.},
	month = sep,
	year = {2013}
}

@book{tidwell_xslt_2008,
	address = {Sebstopol, CA},
	title = {{XSLT}},
	isbn = {978-0-596-52721-1},
	url = {http://it-ebooks.info/book/119/},
	abstract = {The second edition of XSLT incorporates new material for XSLT 2.0 and
expounds on the lessons learned over the last six years of XSLT 1.0 use. Whether you're looking for the latest and greatest in XSLT 1.0 techniques, or moving on to XSLT 2.0, this new edition of XSLT will address your needs. This book includes plenty of practical,
real-world examples to show you how to apply XSLT stylesheets to XML data using either version.},
	language = {en},
	publisher = {O'Reilly Media},
	author = {Tidwell, Doug},
	year = {2008},
	keywords = {t\_XML, act\_Modeling}
}

@book{mcgann_new_2014-2,
	address = {Boston, MA},
	title = {A {New} {Republic} of {Letters}. {Memory} and {Scholarship} in the {Age} of {Digital} {Reproduction}},
	url = {http://www.hup.harvard.edu/catalog.php?isbn=9780674728691},
	abstract = {Jerome McGann's manifesto argues that the history of texts and how they are preserved and accessed for interpretation are the overriding subjects of humanist study in the digital age. Theory and philosophy no longer suffice as an intellectual framework. But philology -- out of fashion for decades -- models these concerns with surprising fidelity.},
	language = {en},
	urldate = {2014-03-30},
	publisher = {Harvard Univ. Press},
	author = {McGann, Jerome},
	year = {2014},
	keywords = {meta\_GiveOverview, meta\_Assessing, obj\_DigitalHumanities, bigdata{\textasciitilde}, obj\_Text}
}

@inproceedings{zhang_novel_2009-1,
	address = {Stroudsburg, PA, USA},
	series = {People's {Web} '09},
	title = {A novel approach to automatic gazetteer generation using {Wikipedia}},
	isbn = {978-1-932432-55-8},
	url = {http://dl.acm.org/citation.cfm?id=1699765.1699766},
	abstract = {Gazetteers or entity dictionaries are important knowledge resources for solving a wide range of NLP problems, such as entity extraction. We introduce a novel method to automatically generate gazetteers from seed lists using an external knowledge resource, the Wikipedia. Unlike previous methods, our method exploits the rich content and various structural elements of Wikipedia, and does not rely on language- or domain-specific knowledge. Furthermore, applying the extended gazetteers to an entity extraction task in a scientific domain, we empirically observed a significant improvement in system accuracy when compared with those using seed gazetteers.},
	language = {en},
	urldate = {2013-05-06},
	booktitle = {Proceedings of the 2009 {Workshop} on {The} {People}'s {Web} {Meets} {NLP}: {Collaboratively} {Constructed} {Semantic} {Resources}},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Ziqi and Iria, José},
	year = {2009},
	keywords = {bigdata, t\_NamedEntityRecognition},
	pages = {1--9}
}

@article{nadeau_survey_2007-1,
	title = {A survey of named entity recognition and classification},
	volume = {30},
	url = {http://www.ingentaconnect.com/content/jbp/li/2007/00000030/00000001/art00002?token=005219458c2514faa7e2a46762c6b635d3b662a2553492b467c673f7b2f267738703375686f497c05b},
	doi = {10.1075/li.30.1.03nad},
	abstract = {This survey covers fifteen years of research in the Named Entity Recognition and Classification (NERC) field, from 1991 to 2006. We report observations about languages, named entity types, domains and textual genres studied in the literature. From the start, NERC systems have been developed using hand-made rules, but now machine learning techniques are widely used. These techniques are surveyed along with other critical aspects of NERC such as features and evaluation methods. Features are word-level, dictionary-level and corpus-level representations of words in a document. Evaluation techniques, ranging from intuitive exact match to very complex matching techniques with adjustable cost of errors, are an indisputable key to progress.},
	language = {en},
	number = {1},
	journal = {Lingvisticae Investigationes},
	author = {Nadeau, David and Sekine, Satoshi},
	year = {2007},
	keywords = {*****, t\_NamedEntityRecognition},
	pages = {3--26},
	file = {Nadeau_Sekine_2007_A survey of named entity recognition and classification.pdf:/home/lisnux/Zotero/storage/BG74CC2G/Nadeau_Sekine_2007_A survey of named entity recognition and classification.pdf:application/pdf}
}

@phdthesis{czmiel_adaquate_2003-1,
	type = {Thesis ({MA} level)},
	title = {Adäquate {Markupsysteme} für die digitale {Behandlung} altägyptischer {Texte}},
	url = {http://old.hki.uni-koeln.de/studium/MA/MA_czmiel.pdf},
	language = {de},
	school = {Köln},
	author = {Czmiel, Alexander},
	month = oct,
	year = {2003},
	keywords = {t\_Encoding, t\_XML, goal\_Enrichment}
}

@article{wolfe_annotations_2008-1,
	title = {Annotations and the collaborative digital library: {Effects} of an aligned annotation interface on student argumentation and reading strategies},
	volume = {3},
	issn = {1556-1607, 1556-1615},
	shorttitle = {Annotations and the collaborative digital library},
	url = {http://link.springer.com/article/10.1007/s11412-008-9040-x},
	doi = {10.1007/s11412-008-9040-x},
	abstract = {Recent research on annotation interfaces provides provocative evidence that anchored, annotation-based discussion environments may lead to better conversations about a text. However, annotation interfaces raise complicated tradeoffs regarding screen real estate and positioning. It is argued that solving this screen real estate problem requires limiting the number of annotations displayed to users. In order to understand which annotations have the most learning value for students, this paper presents two complementary studies examining the effects of annotations on students performing a reading-to-write task. The first study used think-aloud protocols and a within-subjects methodology, finding that annotations appeared to provoke students to reflect more critically upon the primary text. This effect was particularly strong when students encountered pairs of annotations presenting different viewpoints on the same section of text. Student interviews suggested that annotations were most helpful when they caused the reader to consider and weigh conflicting viewpoints. The second study used a between-subjects methodology and a more naturalistic task to provide complementary evidence that annotations encourage more reflective responses to a text. This study found that students who received annotated materials both perceived themselves and were perceived by instructors as less reliant on unreflective summary strategies than students who received the same content but in a different format. These findings indicate that the learning value of an annotation lies in its ability to provoke students to consider and weigh new perspectives on the primary text. When selected effectively, annotations provide a critical scaffolding that can support students’ critical thinking and argumentation activities. Collaborative digital libraries and applications for the Web 2.0 should be designed with this learning framework in mind.},
	number = {2},
	urldate = {2014-03-26},
	author = {Wolfe, Joanna},
	month = jun,
	year = {2008},
	note = {00036 bibtex: Wolfe2008Annotations 
biblatexdata[journaltitle=International Journal of Computer-Supported Collaborative Learning;langid=english;shortjournal=Computer Supported Learning]},
	keywords = {Activity: Annotate, Object: Texts},
	pages = {141--164}
}

@article{kalvesmaki_canonical_2014,
	title = {Canonical {References} in {Electronic} {Texts}: {Rationale} and {Best} {Practices}},
	volume = {8},
	shorttitle = {Canonical {References} in {Electronic} {Texts}},
	url = {http://www.digitalhumanities.org/dhq/vol/8/2/000181/000181.html},
	abstract = {Systems of canonical references, whereby segments of written works are sequentially labeled with numbers or letters to facilitate cross-referencing, are widely used but seldom studied, undeservedly so. Canonical numbers are complex interpretive mechanisms with a great deal of potential for anyone editing and using electronic texts. In this essay I consider the rationale for and nature of canonical reference systems, to recommend principles to consider when deploying them in digital projects. After briefly reviewing the history of canonical references I note how they have been used so far, emphasizing the advances made by Canonical Text Services (CTS). I argue that the practical and theoretical problems that remain unaddressed require engagement with descriptions of how textual scholarship works and how notional literary works relate to the artefacts that carry them (using Functional Requirements for Bibliographic Records, FRBR). By correlating a theory of canonical reference numbers with those two models — editorial workflow and creative works — I offer key principles that should be addressed when planning, writing, and using digital projects.},
	language = {en},
	number = {2},
	urldate = {2014-07-21},
	author = {Kalvesmaki, Joel},
	year = {2014},
	keywords = {obj\_Text, act\_Identifying, act\_Annotating, goal\_Enrichment}
}

@article{walsh_comic_2012,
	title = {Comic {Book} {Markup} {Language}: {An} {Introduction} and {Rationale}},
	volume = {6},
	shorttitle = {Comic {Book} {Markup} {Language}},
	url = {http://www.digitalhumanities.org/dhq/vol/6/1/000117/000117.html},
	abstract = {Comics, comic books, and graphic novels are increasingly the target of seriously scholarly attention in the humanities. Moreover, comic books are exceptionally complex documents, with intricate relationships between pictorial and textual elements and a wide variety of content types within a single comic book publication. The complexity of these documents, their combination of textual and pictorial elements, and the collaborative nature of their production shares much in common with other complex documents studied by humanists—illuminated manuscripts, artists’ books, illustrated poems like those of William Blake, letterpress productions like those of the Kelmscott Press, illustrated children’s books, and even Web pages and other born-digital media. Comic Book Markup Language, or CBML, is a TEI-based XML vocabulary for encoding and analyzing comic books, comics, graphic novels, and related documents. This article discusses the goals and motivations for developing CBML, reviews the various content types found in comic book publications, provides an overview and examples of the key features of the CBML XML vocabulary, explores some of the problems and challenges in the encoding and digital representation of comic books, and outlines plans for future work. The structural, textual, visual, and bibliographic complexity of comic books make them an excellent subject for the general study of complex documents, especially documents combining pictorial and textual elements.},
	language = {en},
	number = {1},
	urldate = {2014-03-04},
	author = {Walsh, John A.},
	year = {2012},
	keywords = {t\_Encoding, bigdata, goal\_Enrichment, obj\_Comics}
}

@article{schanze_computer-unterstutzte_1969,
	title = {Computer-unterstützte {Literaturwissenschaft}. {Probleme} und {Perspektiven} im {Zusammenhang} mit dem maschinell erstellten {Kleist}-{Index}},
	number = {79},
	journal = {Muttersprache},
	author = {Schanze, Helmut},
	year = {1969},
	keywords = {goal\_Analysis, obj\_Text, goal\_Enrichment},
	pages = {315--321}
}

@incollection{jannidis_computerphilologie_2007-1,
	address = {Stuttgart},
	title = {Computerphilologie},
	volume = {2 (Methoden und Theorien)},
	language = {de},
	booktitle = {Handbuch {Literaturwissenschaft}},
	publisher = {Metzler},
	author = {Jannidis, Fotis},
	editor = {Anz, Thomas},
	year = {2007},
	keywords = {AnalyzeStatistically, meta\_GiveOverview, obj\_Tools, obj\_DigitalHumanities, *****, act\_Publishing, t\_Encoding, X-CHECK, goal\_Enrichment, x\_astree},
	pages = {27--40}
}

@book{simsion_data_2004,
	address = {Amsterdam},
	edition = {3. ed.},
	title = {Data modeling essentials},
	isbn = {978-0-12-644551-0},
	url = {http://shop.oreilly.com/product/9780126445510.do},
	abstract = {Data Modeling Essentials, Third Edition provides expert tutelage for data modelers, business analysts and systems designers at all levels. Beginning with the basics, this book provides a thorough grounding in theory before guiding the reader through the various stages of applied data modeling and database design. Later chapters address advanced subjects, including business rules, data warehousing, enterprise-wide modeling and data management.

The third edition of this popular book retains its distinctive hallmarks of readability and usefulness, but has been given significantly expanded coverage and reorganized for greater reader comprehension. Authored by two leaders in the field, Data Modeling Essentials, Third Edition is the ideal reference for professionals and students looking for a real-world perspective.},
	language = {en},
	publisher = {Morgan Kaufmann},
	author = {Simsion, Graeme},
	year = {2004},
	keywords = {meta\_GiveOverview, goal\_Interpretation, goal\_Analysis, act\_Modeling}
}

@inproceedings{ratinov_design_2009,
	address = {Stroudsburg, PA, USA},
	series = {{CoNLL} '09},
	title = {Design challenges and misconceptions in named entity recognition},
	isbn = {978-1-932432-29-9},
	url = {http://dl.acm.org/citation.cfm?id=1596374.1596399},
	abstract = {We analyze some of the fundamental design challenges and misconceptions that underlie the development of an efficient and robust NER system. In particular, we address issues such as the representation of text chunks, the inference approach needed to combine local NER decisions, the sources of prior knowledge and how to use them within an NER system. In the process of comparing several solutions to these challenges we reach some surprising conclusions, as well as develop an NER system that achieves 90.8 F1 score on the CoNLL-2003 NER shared task, the best reported result for this dataset.},
	language = {en},
	urldate = {2013-05-06},
	booktitle = {Proceedings of the {Thirteenth} {Conference} on {Computational} {Natural} {Language} {Learning}},
	publisher = {Association for Computational Linguistics},
	author = {Ratinov, Lev and Roth, Dan},
	year = {2009},
	keywords = {*****, t\_NamedEntityRecognition},
	pages = {147--155}
}

@article{fiormonte_digital_2010,
	title = {Digital {Encoding} as a {Hermeneutic} and {Semiotic} {Act}: {The} {Case} of {Valerio} {Magrelli}},
	volume = {4},
	shorttitle = {Digital {Encoding} as a {Hermeneutic} and {Semiotic} {Act}},
	url = {http://www.digitalhumanities.org/dhq/vol/4/1/000082/000082.html},
	language = {en},
	number = {1},
	urldate = {2010-08-03},
	author = {Fiormonte, Domenico and Martiradonna, Valentina and Schmidt, Desmond},
	year = {2010},
	keywords = {meta\_Theorizing, t\_Encoding}
}

@incollection{price_digital_nodate,
	title = {Digital {Scholarly} {Editing}},
	isbn = {978-1-60329-145-3},
	url = {http://dlsanthology.commons.mla.org/digital-scholarly-editing/},
	language = {en},
	urldate = {2014-03-05},
	booktitle = {Literary {Studies} in the {Digital} {Age}},
	publisher = {Modern Language Association of America},
	author = {Schreibman, Susan},
	editor = {Price, Kenneth M. and Siemens, Ray},
	keywords = {obj\_Text, goal\_Enrichment}
}

@book{gasteiner_digitale_2010-1,
	address = {Wien},
	title = {Digitale {Arbeitstechniken} für die {Geistes}- und {Kulturwissenschaften}},
	isbn = {978-3-8252-3157-6},
	url = {http://www.utb-shop.de/digitale-arbeitstechniken.html},
	abstract = {Möglichkeiten und Anwendungsgebiete digitaler Arbeitstechniken Das Buch vermittelt Kenntnisse und Kompetenzen für fortgeschrittene Studierende, Dozierende sowie Forschende in den Geisteswissenschaften im Umgang mit Neuen Medien, Open Access und Digitalen Publikationspraktiken. Es orientiert über Rechte und Pflichten im Umgang mit digitalen Texten und Bildern, behandelt die unterschiedlichen Situationen in den deutschsprachigen Ländern und weist weiterführende Literatur und Quellen zu diesen Themen nach.},
	language = {de},
	publisher = {UTB},
	editor = {Gasteiner, Martin and Haber, Peter},
	year = {2010},
	keywords = {meta\_GiveOverview, meta\_Assessing, act\_Publishing, t\_Encoding, X-CHECK, obj\_Methods}
}

@book{sahle_digitale_2013,
	address = {Norderstedt},
	title = {Digitale {Editionsformen}. {Zum} {Umgang} mit der Überlieferung unter den {Bedingungen} des {Medienwandels}. {Teil} 2: {Befunde}, {Theorie} und {Methodik}.},
	volume = {8},
	isbn = {978-3-8482-5252-7},
	shorttitle = {Digitale {Editionsformen}. {Zum} {Umgang} mit der Überlieferung unter den {Bedingungen} des {Medienwandels}. {Teil} 2},
	url = {http://kups.ub.uni-koeln.de/5352/},
	abstract = {Die neuen Technologien und Medien sind auch für die wissenschaftliche Edition eine Herausforderung. Die Entwicklung unterschiedlicher digitaler Ausgabeformen in den letzten Jahren kann zunächst als evolutionäre Abfolge verschiedener technischer Paradigmen beschrieben werden, die jeweils auch inhaltliche und methodische Konsequenzen hatten. Auf dieser Grundlage lassen sich Bausteine für eine neue verallgemeinernde Theorie der - nun digitalen - Edition umreißen, die ihren Kern u.a. im Konzept der Transmedialisierung findet. Damit ist nach der spezifischen Formung von Methode und Praxis in analogen und digitalen Medien eine Neufassung der Zielstellungen der Edition auf einer eher medienneutralen, konzeptionellen Ebene zu erreichen. Die veränderten Bedingungen und unsere zunehmend digitale Umwelt führen dazu, dass fast alle Bereiche der Edition einem Wandel unterworfen werden. Auch einige dieser Aspekte werden in diesem Band genauer beleuchtet.},
	language = {de},
	urldate = {2014-02-09},
	publisher = {BoD},
	author = {Sahle, Patrick},
	year = {2013}
}

@phdthesis{schrader_xml-datenformat_2007,
	address = {Tübingen},
	type = {{MA} level},
	title = {Ein {XML}-{Datenformat} zur {Repräsentation} kritischer {Musikedition} unter besonderer {Berücksichtigung} von {Neumennotation}},
	url = {http://www.dimused.uni-tuebingen.de/downloads/studienarbeit.pdf},
	language = {de},
	school = {Tübingen},
	author = {Schräder, Gregor},
	collaborator = {Morent, Stefan},
	year = {2007},
	keywords = {t\_Encoding, t\_XML, obj\_Music}
}

@book{burnard_electronic_2006,
	address = {New York},
	title = {Electronic {Textual} {Editing}},
	isbn = {978-0-87352-970-9},
	url = {http://www.tei-c.org/About/Archive_new/ETE/Preview/},
	language = {en},
	publisher = {MLA},
	editor = {Burnard, Lou and O’Brien O’Keeffe, Katherine and Unsworth, John},
	year = {2006},
	keywords = {meta\_GiveOverview, act\_Publishing, t\_Encoding}
}

@inproceedings{kazama_exploiting_2007,
	title = {Exploiting {Wikipedia} as {External} {Knowledge} for {Named} {Entity} {Recognition}},
	url = {http://www.aclweb.org/anthology-new/D/D07/D07-1073.pdf},
	abstract = {We explore the use of Wikipedia as external knowledge to improve named entity recognition (NER). Our method retrieves the corresponding Wikipedia entry for each candidate word sequence and extracts a category label from the first sentence of the entry, which can be thought of as a definition part. These category labels are used as features in a CRF-based NE tagger. We demonstrate using the CoNLL 2003 dataset that the Wikipedia category labels extracted by such a simple method actually improve the accuracy of NER.},
	language = {en},
	urldate = {2013-05-06},
	author = {Kazama, Jun'ichi and Torisawa, Kentaro},
	year = {2007},
	keywords = {t\_NamedEntityRecognition},
	pages = {698--707}
}

@book{shillingsburg_gutenberg_2006,
	address = {Cambridge},
	title = {From {Gutenberg} to {Google}. {Electronic} {Representations} of {Literary} {Texts}},
	isbn = {978-0-521-86498-5},
	url = {http://www.cambridge.org/us/academic/subjects/literature/printing-and-publishing-history/gutenberg-google-electronic-representations-literary-texts},
	abstract = {As technologies for electronic texts develop into ever more sophisticated engines for capturing different kinds of information, radical changes are underway in the way we write, transmit and read texts. In this thought-provoking work, Peter Shillingsburg considers the potentials and pitfalls, the enhancements and distortions, the achievements and inadequacies of electronic editions of literary texts. In tracing historical changes in the processes of composition, revision, production, distribution and reception, Shillingsburg reveals what is involved in the task of transferring texts from print to electronic media. He explores the potentials, some yet untapped, for electronic representations of printed works in ways that will make the electronic representation both more accurate and more rich than was ever possible with printed forms. However, he also keeps in mind the possible loss of the book as a material object and the negative consequences of technology.},
	language = {en},
	publisher = {Cambridge Univ. Press},
	author = {Shillingsburg, Peter},
	year = {2006},
	keywords = {act\_Publishing, meta\_Theorizing, t\_Encoding, obj\_Literature, obj\_Text, act\_Annotating, goal\_Enrichment, act\_Editing}
}

@article{mcgann_text_2006,
	title = {From {Text} to {Work}: {Digital} {Tools} and the {Emergence} of the {Social} {Text}},
	issn = {1467-1255},
	shorttitle = {From {Text} to {Work}},
	url = {http://id.erudit.org/iderudit/013153ar},
	abstract = {The essay is a study of how critical editions work, whether in paper-based forms or in electronic forms. The first section – more than half the essay – gives a close examination to J. C. C. Mays’s superb recent (Bollingen) edition of Coleridge’s poetry. This analysis establishes the terms for investigating the opportunities that digital technology supplies for scholars pursuing a close study of the socio-historical character of literary works. This investigation pivots around the seminal work of D. F. McKenzie, whose theory of the social-text edition argues for a more comprehensive kind of editorial method. This essay argues that the method can be best realized through digital resources. It concludes with a discussion of The Rossetti Archive as a “proof of concept” experiment to test the social-text approach to editorial method.},
	language = {en},
	number = {41},
	urldate = {2010-05-21},
	journal = {Romanticism on the Net},
	author = {McGann, Jerome},
	year = {2006},
	keywords = {act\_Visualizing, act\_Publishing, meta\_Theorizing, t\_Encoding, goal\_Enrichment}
}

@misc{noauthor_guidelines_2007,
	title = {Guidelines for {Editors} of {Scholarly} {Editions}},
	url = {http://www.mla.org/cse_guidelines},
	language = {en},
	urldate = {2010-05-17},
	journal = {Modern Language Association},
	month = sep,
	year = {2007},
	keywords = {act\_Visualizing, obj\_Tools, meta\_Assessing, t\_Encoding, goal\_Enrichment}
}

@article{beynon_human_2006-1,
	title = {Human {Computing}—{Modelling} with {Meaning}},
	volume = {21},
	issn = {0268-1145, 1477-4615},
	url = {http://llc.oxfordjournals.org/content/21/2/141.abstract},
	doi = {10.1093/llc/fql015},
	abstract = {This article is based on a session given by the authors at the ACH/ALLC conference at the University of Victoria in June 2005. It discusses the prospects for partnership between the humanities and computing from the alternative perspective afforded by Empirical Modelling (EM). Perceived dualities that separate the two cultures of science and art are identified as the primary impediment to this partnership. A vision for ‘human computing’ that promises to dissolve these dualities is outlined. The key characteristics and potential for EM for the humanities are illustrated with reference to a modelling exercise on the theme of Schubert's Erlkönig. This highlights how each of the six varieties of modelling identified by McCarty can be represented within an EM model. The implications of EM are discussed with reference to McCarty's account of the key role for modelling in the humanities, in relation to James's ‘philosophic attitude’ of Radical Empiricism and to ideas from phenomenological sources.},
	language = {en},
	number = {2},
	urldate = {2011-11-01},
	journal = {Literary and Linguistic Computing},
	author = {Beynon, Meurig and Russ, Steve and McCarty, Willard},
	month = apr,
	year = {2006},
	keywords = {obj\_AnyObject, act\_Modeling},
	pages = {141--157}
}

@article{ramel_interactive_2013,
	title = {Interactive layout analysis, content extraction, and transcription of historical printed books using {Pattern} {Redundancy} {Analysis}},
	volume = {28},
	issn = {0268-1145, 1477-4615},
	url = {http://llc.oxfordjournals.org/content/28/2/301},
	doi = {10.1093/llc/fqs077},
	abstract = {This article describes the work performed in the Pattern Redundancy Analysis for Document Image Indexing and Transcription research project. The project focused on layout analysis, text/graphics separation, optical character recognition (OCR), and text transcription processes dedicated to old and precious books. The originality of this work relies on the analysis and exploitation of pattern redundancy in documents to enable the efficient indexing and quick transcription of books and the identification of typographic materials. For these purposes, we have developed two software packages. The first, AGORA, performs page layout analysis, text/graphics separation, and pattern (letterform) extraction simultaneously. These patterns are then processed to group similar patterns together in single clusters so that different letterforms of a book can be extracted and analysed to compute redundancy rates. This process allows a significant reduction of the number of letterforms to be recognized. Once the clustering of letterforms is done, a user may assign a label to each cluster using the second software, RETRO. Labels are then automatically assigned to each corresponding character to perform the text transcription of the whole book. Thus, if 90\% of the letterforms are detected as redundant, only one character out of ten must be labelled by the user to transcribe the book. Moreover, this transcription method allows us to deal easily with the special characters that appear frequently in old books. It is also possible to use our clustering approach to extract and create new font packages from specific printing material (e.g. from rare books printed with particular types or woodblocks). These new font packages could be incorporated into the training step of optical fonts recognition methods to improve the recognition results of OCRs on rare or specific books. The identification of typographic materials could also be useful for the study of both the aesthetic (such as how the thickness and shape of printing types evolved from the 15th to the mid-16th century) and economic aspects of printing historically. Until the second half of the 16th century, for instance, printing types circulated among workshops, and printers frequently sold or lent types to their fellows.},
	language = {en},
	number = {2},
	urldate = {2013-06-12},
	journal = {Literary and Linguistic Computing},
	author = {Ramel, Jean-Yves and Sidère, Nicolas and Rayar, Frédéric},
	month = jun,
	year = {2013},
	keywords = {t\_Encoding, act\_Annotating, obj\_Documents, act\_StructuralAnalysis},
	pages = {301--314}
}

@book{galina_introduccion_2007,
	address = {Mexico City},
	series = {Colección {Biblioteca} del {Editor}},
	title = {Introducción a la edición digital},
	url = {http://www.anatomiadelaedicion.com/wordpress/wp-content/uploads/2010/01/manual-de-edicion-digital-1.pdf},
	language = {es},
	publisher = {Dirección General de Publicaciones y Fomento Editorial, UNAM},
	author = {Galina, Isabel and Ordoñez, C.},
	year = {2007},
	keywords = {obj\_Text, goal\_Enrichment, act\_Editing}
}

@inproceedings{tjong_kim_sang_introduction_2003,
	address = {Stroudsburg, PA, USA},
	series = {{CONLL} '03},
	title = {Introduction to the {CoNLL}-2003 shared task: language-independent named entity recognition},
	shorttitle = {Introduction to the {CoNLL}-2003 shared task},
	url = {http://dx.doi.org/10.3115/1119176.1119195},
	doi = {10.3115/1119176.1119195},
	abstract = {We describe the CoNLL-2003 shared task: language-independent named entity recognition. We give background information on the data sets (English and German) and the evaluation method, present a general overview of the systems that have taken part in the task and discuss their performance.},
	language = {en},
	urldate = {2013-05-06},
	booktitle = {Proceedings of the seventh conference on {Natural} language learning at {HLT}-{NAACL} 2003 - {Volume} 4},
	publisher = {Association for Computational Linguistics},
	author = {Tjong Kim Sang, Erik F. and De Meulder, Fien},
	year = {2003},
	keywords = {t\_NamedEntityRecognition},
	pages = {142--147},
	file = {Tjong Kim Sang_De Meulder_2003_Introduction to the CoNLL-2003 shared task.pdf:/home/lisnux/Zotero/storage/FSE75UDV/Tjong Kim Sang_De Meulder_2003_Introduction to the CoNLL-2003 shared task.pdf:application/pdf}
}

@article{stapelfeldt_islandora_2013,
	title = {Islandora and {TEI}: {Current} and {Emerging} {Applications}/{Approaches}},
	copyright = {TEI Consortium 2013 (Creative Commons Attribution-NoDerivs 3.0 Unported License)},
	issn = {2162-5603},
	shorttitle = {Islandora and {TEI}},
	url = {http://jtei.revues.org/790},
	doi = {10.4000/jtei.790},
	abstract = {Islandora is an open-source software framework developed since 2006 by the University of Prince Edward Island's Robertson Library. The Islandora framework is designed to ease the management of security and workflow for digital assets, and to help implementers create custom interfaces for display, search, and discovery. Turnkey options are provided via tools and modules ("solution packs") designed to support the work of a particular knowledge domain (such as chemistry), a particular content type (such as a digitized newspaper), or a particular task (such as TEI encoding). While it does not yet have native support for TEI, Islandora provides a promising basis on which digital humanities scholars could manage the creation, editing, validation, display, and comparison of TEI-encoded text. UPEI's IslandLives project, with its forthcoming solution pack, provides insight into how an Islandora version 6 installation can support OCR text extraction, automatic structural/semantic encoding of text, and web-based TEI editing and display functions for site administrators. This article introduces the Islandora framework and its suitability for TEI, describes the IslandLives approach in detail, and briefly discusses recent work and future directions for TEI work in Islandora. The authors hope that interested readers may help contribute to the expansion of TEI-related services and features available to be used with Islandora.},
	language = {en},
	number = {Issue 5},
	urldate = {2013-10-17},
	journal = {Journal of the Text Encoding Initiative},
	author = {Stapelfeldt, Kirsta and Moses, Donald},
	collaborator = {Blanke, Tobias and Romary, Laurent},
	month = jun,
	year = {2013},
	keywords = {goal\_Dissemination},
	file = {Stapelfeldt_Moses_2013_Islandora and TEI.pdf:/home/lisnux/Zotero/storage/NTEJ9WYF/Stapelfeldt_Moses_2013_Islandora and TEI.pdf:application/pdf}
}

@article{gorz_kognitive_2007,
	title = {Kognitive {Karten} des {Mittelalters}: {Digitale} {Erschließung} mittelalterlicher {Weltkarten}.},
	volume = {10},
	shorttitle = {Kognitive {Karten} des {Mittelalters}},
	url = {http://wwwdh.cs.fau.de/IMMD8/staff/Goerz/mappae2006jb.pdf},
	abstract = {Mittelalterliche Weltkarten sind in erster Linie als kognitive Karten zu verstehen. Um den
Bestand und das Ver
̈
anderungspotential des Text-Bild-Materials zu erfassen und zu erschlie-
ßen, wird im Rahmen eines interdisziplin
̈
aren Projekts in Erlangen ein systematischer verglei-
chender Stellenkatalog mittelalterlicher und fr
̈
uhneuzeitlicher Weltkarten erarbeitet. In einem
ersten Schritt wurde eine multimediale Datenbank aus hochaufgel
̈
osten digitalen Bildern cir-
ca 300 repr
̈
asentativer Weltkarten aufgebaut, die durch zugeordnete Metadaten erschlossen
sind. Die Datenbank wird erg
̈
anzt durch spezielle Software-Werkzeuge zur Bildbearbeitung
und zum Bildvergleich. Mit diesen Mitteln wird der Stellenkatalog erarbeitet, der alle Posi-
tionen umfassen soll, die auf den Mappaemundi des 13.–16. Jahrhunderts mit Bildern, Legen-
den und Bild-Text-Kombinationen verzeichnet sind. Als begriffliche Grundlage hierf
̈
ur dient
eine formale Ontologie, die zur Zeit in der beschreibungslogischen
”
Web Ontology Language“
OWL-DL formuliert wird; sie erweitert eine zun
̈
achst f
̈
ur den Behaim-Globus erstellte Konzept-
hierarchie und wird in das
”
Conceptual Reference Model“ der ICOM-CIDOC (International
Committee for Documentation of the International Council of Museums) eingebettet. Diese Art
der Formalisierung erm
̈
oglicht komplexe Anfragen, die weit
̈
uber die M
̈
oglichkeiten herk
̈
omm-
licher Datenbanken hinausgehen. Fernziel des Projekts ist die Ausarbeitung der in den Karten
dargestellten kognitiven Beziehungen und deren Wandel auf der Grundlage des Stellenkata-
logs.},
	language = {de},
	journal = {Historisches Forum},
	author = {Görz, Günther},
	year = {2007},
	keywords = {act\_Annotating, goal\_Enrichment, obj\_Maps}
}

@book{cresson_ledition_2012,
	address = {Paris},
	title = {L'Édition du manuscrit},
	abstract = {Cet ouvrage expose la relation tissée entre l'archive patrimoniale du conservateur, la valorisation scientifique assumée par le chercheur et la transmission des savoirs dont l'éditeur est garant. Il réunit des généticiens du texte, des conservateurs, des éditeurs, chacun faisant part de son expérience concernant la numérisation, l'appropriation des manuscrits et l'édition de genèse.},
	language = {fr},
	publisher = {L'Harmattan},
	editor = {Cresson, Aurèle},
	year = {2012},
	keywords = {act\_Publishing, t\_Encoding, obj\_Manuscripts}
}

@article{gabler_preeminence_2008,
	title = {La prééminence du document dans l’édition},
	copyright = {© Recherches \& Travaux},
	issn = {0151-1874},
	url = {http://recherchestravaux.revues.org/index85.html},
	abstract = {À la question posée jadis par Gunter Martens, spécialiste allemand de critique textuelle1 : « D’un point de vue éditorial, qu’est-ce qu’un texte2 ?», je répondrai aujourd’hui par la question suivante : « D’un point de vue éditorial, qu’est-ce qu’un document ? » Tant que la transmission et l’édition se trouvaient liées dans le même univers matériel d’encre et de papier, le problème ne se posait pas : le document était « transparent » parce qu’il était aussi matériel que tout ce qui l’entourait. M [...]},
	language = {fr},
	number = {72},
	urldate = {2010-05-24},
	journal = {Recherches \& Travaux},
	author = {Gabler, Hans-Walter},
	editor = {Leriche, Françoise and Meynard, Cécile},
	collaborator = {Coste, Claude and Vibert, Bertrand},
	month = jun,
	year = {2008},
	keywords = {act\_Visualizing, meta\_Theorizing, t\_Encoding, obj\_Documents, obj\_Manuscripts},
	pages = {39--51}
}

@book{hulle_manuscript_2008,
	address = {Gainesville},
	title = {Manuscript genetics, {Joyce}'s know-how, {Beckett}'s nohow},
	isbn = {978-0-8130-3200-9},
	language = {English},
	publisher = {University Press of Florida},
	author = {Hulle, Dirk van},
	year = {2008}
}

@misc{coombs_markup_1987,
	title = {Markup {Systems} and the {Future} of {Scholarly} {Text} {Processing}},
	url = {http://xml.coverpages.org/coombs.html},
	abstract = {In the last few years, scholarly text processing has entered a reactionary stage. Previously, developers were working toward systems that would support scholars in their roles as researchers and authors. Building on the ideas of Vannevar Bush, people such as Theodor H. Nelson and Andries van Dam (Drucker, van Dam, Yankelovitch) prototyped systems designed to parallel the associative thought processes of researching scholars. Similarly, Douglas C. Engelbart sought to augment human intellect by providing concept-manipulation aids (Engelbart and English, Engelbart et al.). Brian K. Reid developed Scribe, freeing authors from formatting concerns and providing them with integrated tools for bibliography and citation management. Although only a small percentage of scholars were exposed to these ideas, the movement was toward developing new strategies for research and composition.},
	language = {en},
	journal = {Cover Pages. Online resource for markup language technologies},
	author = {Coombs, James H. and Renear, Allen H. and DeRose, Steven J.},
	year = {1987},
	keywords = {meta\_Theorizing, t\_Encoding}
}

@article{bruning_multiple_2013,
	title = {Multiple {Encoding} in {Genetic} {Editions}: {The} {Case} of "{Faust}"},
	copyright = {TEI Consortium 2013 (Creative Commons Attribution-NoDerivs 3.0 Unported License)},
	issn = {2162-5603},
	shorttitle = {Multiple {Encoding} in {Genetic} {Editions}},
	url = {http://jtei.revues.org/697},
	abstract = {The aim of the present paper is to show how, and to what extent, the standards of critical genetic editions as applied to Goethe's Faust can be attained within a TEI framework. It proposes and argues for the introduction of two separate transcripts: documentary and textual. Despite the apparent disadvantages of multiple encoding, this approach recommends itself for practical reasons (e.g., avoidance of overlapping hierarchies), and it conveniently reflects the idea that any written document must be considered a material object on the one hand and a medium of textual transmission on the other. In the course of the paper, some aspects and problems of chapter 11 of version 2.0.0 of TEI P5 (the definition and use of the elements {\textless}line{\textgreater} and {\textless}mod{\textgreater} and related issues) will be discussed.},
	language = {en},
	number = {Issue 4},
	urldate = {2013-04-01},
	journal = {Journal of the Text Encoding Initiative},
	author = {Brüning, Gerrit and Henzel, Katrin and Pravida, Dietmar},
	collaborator = {Jannidis, Fotis and Rehbein, Malte and Romary, Laurent},
	month = mar,
	year = {2013},
	keywords = {t\_Encoding, obj\_Manuscripts}
}

@book{kepper_musikedition_2011,
	address = {Norderstedt},
	title = {Musikedition im {Zeichen} neuer {Medien} : historische {Entwicklung} und gegenwärtige {Perspektiven} musikalischer {Gesamtausgaben}},
	isbn = {978-3-8448-0076-0},
	shorttitle = {Musikedition im {Zeichen} neuer {Medien}},
	url = {http://ifb.bsz-bw.de/bsz36848601Xrez-1.pdf},
	abstract = {Die Keimzelle der Musikwissenschaft als geisteswissenschaftlicher Disziplin liegt in den Bemühungen des 19. Jahrhunderts, die Werke herausragender Komponisten zu konservieren und einer breiteren Öffentlichkeit zu erschließen. In diesem Umfeld erschien im Jahr 1851 der erste Band der Bach-Gesamtausgabe, herausgegeben von der Leipziger Bachgesellschaft. Alle nachfolgenden Musiker-Ausgaben entwickelten sich auf dieser Basis und reizten die Möglichkeiten des Buchmediums in zunehmenden Maße aus. Seit etwa zehn Jahren wird versucht, das Potential digitaler Medien für die Musikphilologie zu erschließen. Ausgehend von der Geschichte musikwissenschaftlicher Ausgaben und einer kritischen Reflektion des bisher Geleisteten, weist dieser Band mögliche neue Perspektiven für zukünftige, dem neuen Medium angemessene Editionsformen auf.},
	language = {de},
	publisher = {Books on Demand},
	author = {Kepper, Johannes},
	year = {2011},
	keywords = {meta\_Theorizing, t\_Encoding, obj\_SheetMusic, obj\_Music, goal\_Enrichment}
}

@book{wanske_musiknotation_1988,
	address = {Mainz [u.a.]},
	title = {Musiknotation : von der {Syntax} des {Notenstichs} zum {EDV}-gesteuerten {Notensatz}},
	isbn = {978-3-7957-2886-1},
	url = {http://www.amazon.de/Musiknotation-Syntax-Notenstichs-EDV-gesteuerten-Notensatz/dp/379572886X},
	language = {de},
	publisher = {Schott},
	author = {Wanske, Helene},
	year = {1988},
	keywords = {act\_Publishing, t\_Encoding, obj\_Music}
}

@article{stewart_charles_2003,
	title = {Charles {Brockden} {Brown}: {Quantitative} {Analysis} and {Literary} {Interpretation}},
	volume = {18},
	shorttitle = {Charles {Brockden} {Brown}},
	url = {http://llc.oxfordjournals.org/content/18/2/129.abstract},
	doi = {10.1093/llc/18.2.129},
	abstract = {This study is a test case in the use of stylometric techniques to provide an entrance into questions of literary criticism and interpretation. The study applies multivariate analysis to two texts of Charles Brockden Brown, sometimes considered the first professional writer in the United States. Both a scatter graph of a principal components analysis and a cluster analysis show that individual chapters from each of two novels (Wieland and Carwin) group together, except for three chapters of Wieland that cluster with the Carwin chapters. One chapter of Wieland that clusters with the Carwin chapters is narrated by the same character who narrates all of Carwin, thus providing statistical evidence that Brown has created a narrator with a distinctive voice. Accounting for the clustering of the other two chapters calls for a consideration of several of the more crucial and problematic interpretative issues in the novel, and suggests that quantitative analysis can indeed provide background and evidence for literary critical discussion and understanding.},
	language = {en},
	number = {2},
	urldate = {2011-06-03},
	journal = {Literary and Linguistic Computing},
	author = {Stewart, Larry L.},
	month = jun,
	year = {2003},
	keywords = {AnalyzeStatistically, goal\_Interpretation},
	pages = {129 --138},
	file = {Stewart_2003_Charles Brockden Brown.pdf:/home/lisnux/Zotero/storage/T3KBKTI4/Stewart_2003_Charles Brockden Brown.pdf:application/pdf}
}

@book{schier_cid_1995,
	title = {{CID}. {Computergestützte} {Interpretationen} von {Detektivromanen}},
	url = {http://www.gbv.de/dms/goettingen/184540100.pdf},
	abstract = {Das Forschungsprojekt CID widmet sich dreierlei Aufgaben: Fundiert durch Webers Theorie der analytischen Erzählung wird CID zum einen die Bauform und das erzähltechnische Regelwerk des Detektivromans und seiner «Untergattungen» untersuchen. Zum anderen geht es um die Entwicklung und Erprobung eines leistungsfähigen und komfortablen Instrumentariums zur computergestützten empirischen Analyse epischer Texte, das den Qualitätsanforderungen der sozialwissenschaftlichen Inhaltsanalyse entspricht. Mit Hilfe dieser Instrumente sollen schließlich wissenschaftlich relevante und repräsentative Ergebnisse für die Literaturwissenschaft ermittelt werden: Die Arbeitsfelder, in die CID aufgeteilt ist, haben verschiedene konstitutive Elemente epischer Texte im allgemeinen und des Detektivromans im besonderen zum Gegenstand.},
	language = {de},
	publisher = {Frankfurt am Main: Lang 1995},
	editor = {Schier, Dagmar and Giersch, Malchus},
	year = {1995},
	keywords = {AnalyzeQualitatively, goal\_Interpretation, obj\_Literature}
}

@article{mani_computational_2012,
	title = {Computational {Modeling} of {Narrative}},
	volume = {5},
	issn = {1947-4040, 1947-4059},
	url = {http://www.morganclaypool.com/doi/abs/10.2200/S00459ED1V01Y201212HLT018},
	doi = {10.2200/S00459ED1V01Y201212HLT018},
	abstract = {The field of narrative (or story) understanding and generation is one of the oldest in natural language processing (NLP) and artificial intelligence (AI), which is hardly surprising, since storytelling is such a fundamental and familiar intellectual and social activity. In recent years, the demands of interactive entertainment and interest in the creation of engaging narratives with life-like characters have provided a fresh impetus to this field. This book provides an overview of the principal problems, approaches, and challenges faced today in modeling the narrative structure of stories. The book introduces classical narratological concepts from literary theory and their mapping to computational approaches. It demonstrates how research in AI and NLP has modeled character goals, causality, and time using formalisms from planning, case-based reasoning, and temporal reasoning, and discusses fundamental limitations in such approaches. It proposes new representations for embedded narratives and fictional entities, for assessing the pace of a narrative, and offers an empirical theory of audience response. These notions are incorporated into an annotation scheme called NarrativeML. The book identifies key issues that need to be addressed, including annotation methods for long literary narratives, the representation of modality and habituality, and characterizing the goals of narrators. It also suggests a future characterized by advanced text mining of narrative structure from large-scale corpora and the development of a variety of useful authoring aids.

This is the first book to provide a systematic foundation that integrates together narratology, AI, and computational linguistics. It can serve as a narratology primer for computer scientists and an elucidation of computational narratology for literary theorists. It is written in a highly accessible manner and is intended for use by a broad scientific audience that includes linguists (computational and formal semanticists), AI researchers, cognitive scientists, computer scientists, game developers, and narrative theorists.},
	language = {en},
	number = {3},
	urldate = {2013-04-23},
	journal = {Synthesis Lectures on Human Language Technologies},
	author = {Mani, Inderjeet},
	month = dec,
	year = {2012},
	keywords = {goal\_Interpretation, goal\_Analysis, act\_Modeling},
	pages = {1--142}
}

@article{berry_critical_2013-1,
	series = {stunlaw-blog},
	title = {Critical {Digital} {Humanities}},
	url = {http://stunlaw.blogspot.com/2013/01/critical-digital-humanities.html},
	abstract = {Critical Digital Humanities is an approach to the study and use of the digital which is attentive to questions of power, domination, myth and exploitation, what has been called the "The Dark Side of the Digital Humanities" (Chun 2013; Grusin 2013; Jagoda 2013; Raley 2013).},
	language = {en},
	author = {Berry, David M.},
	month = nov,
	year = {2013},
	keywords = {meta\_Assessing, obj\_DigitalHumanities, meta\_Theorizing}
}

@article{samuels_deformance_1999,
	title = {Deformance and {Interpretation}},
	volume = {30},
	url = {http://www2.iath.virginia.edu/jjm2f/old/deform.html},
	abstract = {Works of imagination encourage interpreters, who respond in diverse and inventive ways. The variety of critical practices--indeed, the number of differing interpretations directed at the same works--can obscure the theoretical commonality that holds those practices together. We can draw an immediate distinction, however, between critical practices which do or do not aim to be interpretive: bibliographical studies and prosodic analysis, for example, typically discount their interpretive moves, if any are explicitly engaged.

The usual object of interpretation is "meaning," or some set of ideas that can be cast in thematic form. These meanings are sought in different ways: as though resident "in" the work, or evoked through "reader-response," or deconstructable through a process that would reinstall a structure of intelligibility at a higher, more critical level. The contemporary terminology will not obscure the long-standing character of such practices, which can be mixed in various ways. In all these cases, however, an essential relation is preserved between an artistic work and some structure of ideas, that is, some conceptual form that gets more or less fully articulated "for" the work. To understand a work of art, interpreters try to close with a structure of thought that represents its essential idea(s).

In this paper we want to propose--or recall--another way of engaging imaginative work. Perhaps as ancient as more normative practices, it has been less in vogue for some time. This alternative does not stand opposed to interpretive procedures as such, nor to the elaboration of conceptual equivalents for imaginative work. But it does try to set these modes of exegesis on a new footing. The alternative moves to break beyond conceptual analysis into the kinds of knowledge involved in performative operations--a practice of everyday imaginative life. We will argue that concept-based interpretation, reading along thematic lines, is itself best understood as a particular type of performative and rhetorical operation.},
	language = {en},
	number = {1},
	journal = {New Literary History},
	author = {Samuels, Lisa and McGann, Jerome},
	year = {1999},
	keywords = {AnalyzeQualitatively, AnalyzeStatistically, meta\_Theorizing, goal\_Interpretation},
	pages = {25--56}
}

@book{hayles_digital_2012-1,
	address = {Chicago},
	title = {Digital {Media} and contemporary technogenesis},
	isbn = {978-0-226-32142-4},
	url = {http://press.uchicago.edu/ucp/books/book/chicago/H/bo5437533.html},
	abstract = {Hayles examines the evolution of the field from the traditional humanities and how the digital humanities are changing academic scholarship, research, teaching, and publication. She goes on to depict the neurological consequences of working in digital media, where skimming and scanning, or “hyper reading,” and analysis through machine algorithms are forms of reading as valid as close reading once was. Hayles contends that we must recognize all three types of reading and understand the limitations and possibilities of each.},
	language = {en},
	publisher = {University of Chicago Press},
	author = {Hayles, N. Katherine},
	year = {2012},
	keywords = {obj\_DigitalHumanities, meta\_Theorizing, obj\_Research, act\_Conceptualizing, goal\_Interpretation, obj\_Humanities}
}

@article{jacomy_forceatlas2_2014,
	title = {{ForceAtlas2}, a {Continuous} {Graph} {Layout} {Algorithm} for {Handy} {Network} {Visualization} {Designed} for the {Gephi} {Software}},
	volume = {9},
	url = {http://dx.doi.org/10.1371/journal.pone.0098679},
	doi = {10.1371/journal.pone.0098679},
	abstract = {Gephi is a network visualization software used in various disciplines (social network analysis, biology, genomics…). One of its key features is the ability to display the spatialization process, aiming at transforming the network into a map, and ForceAtlas2 is its default layout algorithm. The latter is developed by the Gephi team as an all-around solution to Gephi users’ typical networks (scale-free, 10 to 10,000 nodes). We present here for the first time its functioning and settings. ForceAtlas2 is a force-directed layout close to other algorithms used for network spatialization. We do not claim a theoretical advance but an attempt to integrate different techniques such as the Barnes Hut simulation, degree-dependent repulsive force, and local and global adaptive temperatures. It is designed for the Gephi user experience (it is a continuous algorithm), and we explain which constraints it implies. The algorithm benefits from much feedback and is developed in order to provide many possibilities through its settings. We lay out its complete functioning for the users who need a precise understanding of its behaviour, from the formulas to graphic illustration of the result. We propose a benchmark for our compromise between performance and quality. We also explain why we integrated its various features and discuss our design choices.},
	language = {en},
	number = {6},
	urldate = {2014-06-18},
	journal = {PLoS ONE},
	author = {Jacomy, Mathieu and Venturini, Tommaso and Heymann, Sebastien and Bastian, Mathieu},
	month = jun,
	year = {2014},
	keywords = {act\_Visualizing, goal\_Interpretation, obj\_Data},
	pages = {98679},
	file = {Jacomy et al_2014_ForceAtlas2, a Continuous Graph Layout Algorithm for Handy Network.pdf:/home/lisnux/Zotero/storage/FB9ATRSA/Jacomy et al_2014_ForceAtlas2, a Continuous Graph Layout Algorithm for Handy Network.pdf:application/pdf}
}

@article{cleveland_graphical_1983,
	title = {Graphical {Perception}: {Theory}, {Experimentation}, and {Application} to the {Development} of {Graphical} {Methods}},
	volume = {79},
	doi = {10.1080/01621459.1984.10478080},
	abstract = {The subject of graphical methods for data analysis and for data presentation needs a scientific foundation. In this article we take a few steps in the direction of establishing such a foundation. Our approach is based on graphical perception—the visual decoding of information encoded on graphs—and it includes both theory and experimentation to test the theory. The theory deals with a small but important piece of the whole process of graphical perception. The first part is an identification of a set of elementary perceptual tasks that are carried out when people extract quantitative information from graphs. The second part is an ordering of the tasks on the basis of how accurately people perform them. Elements of the theory are tested by experimentation in which subjects record their judgments of the quantitative information on graphs. The experiments validate these elements but also suggest that the set of elementary tasks should be expanded. The theory provides a guideline for graph construction: Graphs should employ elementary tasks as high in the ordering as possible. This principle is applied to a variety of graphs, including bar charts, divided bar charts, pie charts, and statistical maps with shading. The conclusion is that radical surgery on these popular graphs is needed, and as replacements we offer alternative graphical forms—dot charts, dot charts with grouping, and framed-rectangle charts.},
	language = {en},
	number = {387},
	journal = {Journal of the American Statistical Association},
	author = {Cleveland, William S. and McGill, Robert},
	year = {1983},
	pages = {531--554}
}

@article{beynon_human_2006-2,
	title = {Human {Computing}—{Modelling} with {Meaning}},
	volume = {21},
	issn = {0268-1145, 1477-4615},
	url = {http://llc.oxfordjournals.org/content/21/2/141.abstract},
	doi = {10.1093/llc/fql015},
	abstract = {This article is based on a session given by the authors at the ACH/ALLC conference at the University of Victoria in June 2005. It discusses the prospects for partnership between the humanities and computing from the alternative perspective afforded by Empirical Modelling (EM). Perceived dualities that separate the two cultures of science and art are identified as the primary impediment to this partnership. A vision for ‘human computing’ that promises to dissolve these dualities is outlined. The key characteristics and potential for EM for the humanities are illustrated with reference to a modelling exercise on the theme of Schubert's Erlkönig. This highlights how each of the six varieties of modelling identified by McCarty can be represented within an EM model. The implications of EM are discussed with reference to McCarty's account of the key role for modelling in the humanities, in relation to James's ‘philosophic attitude’ of Radical Empiricism and to ideas from phenomenological sources.},
	language = {en},
	number = {2},
	urldate = {2011-11-01},
	journal = {Literary and Linguistic Computing},
	author = {Beynon, Meurig and Russ, Steve and McCarty, Willard},
	month = apr,
	year = {2006},
	keywords = {obj\_AnyObject, act\_Modeling},
	pages = {141--157}
}

@article{drucker_humanities_2011,
	title = {Humanities {Approaches} to {Graphical} {Display}},
	volume = {5},
	url = {http://www.digitalhumanities.org/dhq/vol/5/1/000091/000091.html},
	abstract = {As digital humanists have adopted visualization tools in their work, they have borrowed methods developed for the graphical display of information in the natural and social sciences. These tools carry with them assumptions of knowledge as observer-independent and certain, rather than observer co-dependent and interpretative. This paper argues that we need a humanities approach to the graphical expression of interpretation. To begin, the concept of data as a given has to be rethought through a humanistic lens and characterized as capta, taken and constructed. Next, the forms for graphical expression of capta need to be more nuanced to show ambiguity and complexity. Finally, the use of a humanistic approach, rooted in a co-dependent relation between observer and experience, needs to be expressed according to graphics built from interpretative models. In summary: all data have to be understood as capta and the conventions created to express observer-independent models of knowledge need to be radically reworked to express humanistic interpretation.},
	language = {en},
	number = {1},
	urldate = {2013-07-25},
	journal = {DHQ: Digital Humanities Quarterly},
	author = {Drucker, Johanna},
	year = {2011},
	keywords = {act\_Visualizing, *****, goal\_Interpretation}
}

@article{guichard_linternet_2013,
	title = {L’internet et les épistémologies des sciences humaines et sociales},
	copyright = {© Revue Sciences/Lettres},
	issn = {2271-6246},
	url = {http://rsl.revues.org/389},
	abstract = {Les ordinateurs et les réseaux nous rappellent à quel point notre pensée est instrumentée et nous font prendre conscience qu’elle l’a toujours été. Pour le dire autrement : la culture propre à l’informatique apparaît surtout technique. Mais elle n’est que la traduction contemporaine de l’ensemble des savoir-faire liés à la maîtrise de l’écriture. Nous (re)découvrons alors un lien étroit entre culture technique et culture des savants et des érudits, et les anthropologues ont montré la relation entre cette dernière et la culture au sens large : par effet de domination (le pouvoir de l’écrit) et parce que l’écriture invite à la réflexion sur les objets qu’elle manipule ou met en évidence. Il y a donc un lien direct entre culture technique propre à l’écriture et culture d’une société. Nous montrons alors comment l’écriture électronique et en réseau infléchit les problématiques et les épistémologies de disciplines communément regroupées sous l’étiquette « sciences humaines et sociales » (SHS) : nouvelles méthodes, potentialités combinatoires, questions posées par les usages du « numérique », etc., mais aussi savoir-faire élémentaires (écrire ou repérer un signe dans un texte). Certaines de ces problématiques commencent à être abordées par des personnes qui se revendiquent du mouvement des « humanités numériques ». Nous montrons que la faiblesse argumentative des représentants de ce mouvement est moins préoccupante pour les scientifiques que la facilité avec laquelle ils se font entendre : outre le dévoilement sociologique du monde universitaire actuel, toujours instructif, ce n’est pas tant l’essor des « humanités numériques » qui pose problème (parce qu’elles seraient mal définies ou joueraient d’un oxymore peu efficace épistémologiquement) que le silence de représentants des « SHS » quant à l’évolution des contours de chacune de leur discipline sous l’effet de l’écriture contemporaine. Pourtant, l’étude de cet effet, déjà balisée par des épistémologues, est prometteuse. Et elle permet de comprendre ce qui se « fabrique », de façon profane comme savante, en matière de culture numérique.},
	language = {fr},
	number = {2},
	urldate = {2014-03-06},
	journal = {Revue Sciences/Lettres},
	author = {Guichard, Éric},
	collaborator = {Poibeau, Thierry},
	month = oct,
	year = {2013},
	keywords = {obj\_DigitalHumanities, goal\_Interpretation}
}

@article{leroux_notion_2012,
	title = {La notion de modèle en philosophie des sciences},
	volume = {7},
	issn = {1712-8307, 1918-7475},
	url = {http://www.erudit.org/revue/npss/2012/v7/n2/1013054ar.html?vue=resume},
	doi = {10.7202/1013054ar},
	abstract = {We want to highlight the central role played by the notion of model in philosophy of science. Having first underlined the intricate tie existing between models and theories in science, we distinguish, in the philosophical literature, the notion of model as general conception from the structural notion of model currently encountered. We present in informal fashion and critically assess the main approaches adopted by epistemological analysis in its intent to characterize models generally associated with scientific theories. Lastly, and with a view to illustrating the centrality of the notion of model, we show how the current debate over scientific realism essentially hinges on contrasting ways to construe of models.},
	language = {fr},
	number = {2},
	urldate = {2014-09-02},
	journal = {Nouvelles perspectives en sciences sociales},
	author = {Leroux, Jean},
	year = {2012},
	keywords = {obj\_Research, goal\_Interpretation, act\_Modeling},
	pages = {49},
	file = {Leroux_2012_La notion de modèle en philosophie des sciences.pdf:/home/lisnux/Zotero/storage/Z8CQ4BBM/Leroux_2012_La notion de modèle en philosophie des sciences.pdf:application/pdf}
}

@article{gelman_lets_2002,
	title = {Lets practice what we preach: {Turning} tables into graphs},
	volume = {56},
	url = {http://www.stat.ncsu.edu/people/fuentes/courses/st810a/gelman.pdf},
	abstract = {Statisticians recommend graphical displays but often use tables to present their own research results. Could graphs do better? We study the question by going through the tables in a recent issue of the Journal of the American Statistical Association. We show how it is possible to improve the presentation using graphs that actually take up less space than the original tables. We find a particularly effective tool to be multiple repeated line plots, with comparisons of interest connected by lines and separate comparisons isolated on different plots.},
	language = {en},
	number = {2},
	journal = {The American Statistician},
	author = {Gelman, Andrew and Pascarica, Cristian and Dodhia, Rahul},
	year = {2002},
	pages = {121--130}
}

@book{liu_local_nodate-1,
	address = {Chicago},
	title = {Local {Transcendence}: {Essays} on {Postmodern} {Historicism} and the {Database}},
	isbn = {978-0-226-48696-3},
	url = {http://www.press.uchicago.edu/ucp/books/book/chicago/L/bo5867451.html},
	abstract = {Book of essays on the methodology of the new historicism and other modes of postmodern cultural criticism in the age of the network and the database.},
	language = {en},
	publisher = {University of Chicago Press},
	author = {Liu, Alan},
	keywords = {meta\_GiveOverview, meta\_Theorizing, goal\_Interpretation, goal\_Create, obj\_Data/Databases, t\_Narratology}
}

@article{king_making_2000,
	title = {Making the {Most} of {Statistical} {Analyses}: {Interpretation} and {Presentation}},
	volume = {44},
	url = {http://gking.harvard.edu/files/making.pdf},
	abstract = {Social Scientists rarely take full advantage of the information available in their statistical results. As a consequence, they miss opportunities to present quantities that are of greatest substantive interest for their research and express the appropriate degree of certainty about these quantities. In this article, we offer an approach, built on the technique of statistical simulation, to extract the currently overlooked information from any statistical method and to interpret and present it in a reader-friendly manner. Using this technique requires some expertise, which we try to provide herein, but its application should make the results of quantitative articles more informative and transparent. To illustrate our recommendations, we replicate the results of several published works, showing in each case how the authors’ own conclusions can be expressed more sharply and informatively, and, without changing any data or statistical assumptions, how our approach reveals important new information about the research questions at hand. We also offer very easy-to-use Clarify software that implements our suggestions},
	language = {en},
	number = {2},
	journal = {American Journal of Political Science},
	author = {King, Gary and Tomz, Michael},
	year = {2000},
	pages = {341--355}
}

@article{fish_mind_2012,
	title = {Mind {Your} {P}'s and {B}'s: {The} {Digital} {Humanities} and {Interpretation}},
	shorttitle = {Mind {Your} {P}'s and {B}'s},
	url = {http://opinionator.blogs.nytimes.com/2012/01/23/mind-your-ps-and-bs-the-digital-humanities-and-interpretation/},
	abstract = {Digital humanists have new tools at their disposal, but does that necessarily make for a more accurate reading of texts?},
	language = {en},
	urldate = {2012-01-24},
	journal = {Opinionator, New York Times},
	author = {Fish, Stanley},
	month = jan,
	year = {2012},
	keywords = {goal\_Interpretation}
}

@incollection{mccarty_modeling_2004-1,
	address = {Oxford},
	edition = {Online Edition},
	title = {Modeling: {A} {Study} in {Words} and {Meanings}},
	url = {http://www.digitalhumanities.org/companion/},
	abstract = {The question of modeling arises naturally for humanities computing from the prior question of what its practitioners across the disciplines have in common. What are they all doing with their computers that we might find in their diverse activities indications of a coherent or cohesible practice? How do we make the best, most productive sense of what we observe? There are, of course, many answers: practice varies from person to person, from project to project, and ways of construing it perhaps vary even more. In this chapter I argue for modeling as a model of such a practice. I have three confluent goals: to identify humanities computing with an intellectual ground shared by the older disciplines, so that we may say how and to what extent our field is of as well as in the humanities, how it draws from and adds to them; at the same time to reflect experience with computers "in the wild"; and to aim at the most challenging problems, and so the most intellectually rewarding future now imaginable.},
	language = {en},
	booktitle = {A {Companion} to {Digital} {Humanities}},
	publisher = {Blackwell},
	author = {McCarty, Willard},
	editor = {Schreibman, Susan and Siemens, Ray and Unsworth, John},
	year = {2004},
	keywords = {meta\_Theorizing, goal\_Interpretation, goal\_Analysis, obj\_AnyObject, act\_Modeling},
	pages = {(chapter 19)}
}

@article{noack_modularity_2009,
	title = {Modularity clustering is force-directed layout},
	volume = {79},
	issn = {1539-3755, 1550-2376},
	url = {http://link.aps.org/doi/10.1103/PhysRevE.79.026102},
	doi = {10.1103/PhysRevE.79.026102},
	abstract = {Two natural and widely used representations for the community structure of networks are clusterings, which partition the vertex set into disjoint subsets, and layouts, which assign the vertices to positions in a metric space. This paper unifies prominent characterizations of layout quality and clustering quality, by showing that energy models of pairwise attraction and repulsion subsume Newman and Girvan's modularity measure. Layouts with optimal energy are relaxations of, and are thus consistent with, clusterings with optimal modularity, which is of practical relevance because both representations are complementary and often used together.},
	language = {en},
	number = {2},
	urldate = {2013-04-05},
	journal = {Physical Review E},
	author = {Noack, Andreas},
	month = feb,
	year = {2009},
	keywords = {act\_Visualizing},
	file = {Noack_2009_Modularity clustering is force-directed layout.pdf:/home/lisnux/Zotero/storage/XP4IXDEL/Noack_2009_Modularity clustering is force-directed layout.pdf:application/pdf}
}

@incollection{zollner-weber_noctua_2007,
	address = {Tübingen},
	title = {Noctua literaria - {A} {System} for a {Formal} {Description} of {Literary} {Characters}},
	url = {http://pub.uni-bielefeld.de/luur/download?func=downloadFile&recordOId=2301795&fileOId=2301798},
	abstract = {Literary characters are just as old as the stories they appear in. They form the active part
of narration and are important to drive the plot further. Often, characters are remembered
longer than the story they belong to. One can assume that many readers are fascinated,
maybe even inspired by them.1 Several characters, or parts of them, survived centuries and
were transported from oral tales to written stories. Thus, characters like the Knights of the
Round Table, Doctor Faustus or creatures of fairy tales still appear in current stories or film
productions.},
	language = {en},
	booktitle = {Data {Structures} for {Linguistic} {Resources} and {Applications}},
	author = {Zöllner-Weber, Amélie},
	editor = {Rehm, Georg and Witt, Andreas and Lemnitzer, Lothar},
	year = {2007},
	keywords = {act\_Conceptualizing, goal\_Interpretation, obj\_Literature, goal\_Analysis, act\_Modeling},
	pages = {113--12}
}

@article{stiegler_relational_2012,
	title = {Relational {Ecology} and the {Digital} {Pharmakon}},
	volume = {13},
	url = {http://www.culturemachine.net/index.php/cm/article/view/464/501},
	abstract = {Alphabetical vocalic writing, which appeared between the 8th and 7th Century B.C., allowed the constitution of a singular attentional process which is the very basis of ancient Greek civilisation. They called it the logos. At the same time, an equally alphabetical, but consonant-based form of writing allowed the construction of the kingdom of Judea. When the two civilisations will meet through Paul of Tarsus, the West will be formed – and ceaselessly reformed, deformed and transformed as the process of psychic and collective individuation based on writing as the technique of the formation of attention. This includes what are known as the Scriptures, which will come into their own with the printing press, inaugurating the attentional revolution which was the Reformation.
In this way the elements of what Katherine Hayles has called ‘deep attention’ came together – an attentional form allowing its own replacement by another form that she calls ‘hyper-attention’ produced by the digital technologies of attention capture (Hayles, 2007).
If we want to analyse and understand the stakes of this transformation (insofar as this is possible), we must analyse what, as process of ‘grammatisation’, leads us from the appearance of the writing of grammata up to the digital apparatuses and the new attentional forms that they constitute. For these inaugurate a new process of psychic and collective individuation that emerges at the heart of what must be understood as a network society of planetary proportions.},
	language = {en},
	number = {Paying Attention},
	author = {Stiegler, Bernard},
	year = {2012},
	keywords = {obj\_DigitalHumanities, meta\_Theorizing, obj\_AnyObject, meta\_Teaching/Learning, meta\_Collaborating},
	pages = {11}
}

@article{miall_representing_1995,
	title = {Representing and {Interpreting} {Literature} by {Computer}},
	volume = {25},
	copyright = {Copyright © 1995 Modern Humanities Research Association},
	issn = {0306-2473},
	url = {http://www.jstor.org/stable/3508827},
	doi = {10.2307/3508827},
	abstract = {It is clear that the advent of computers has so far had almost no impact on the mainstream activities of producing, reading, or studying literary texts. This may be about to change. The prophecy that computing will transform the nature of literary studies is certainly one that we have heard before, but the widespread use of powerful personal computers in the last few years and the increasing role played by the internet, now makes such a forecast seem to carry more weight. Advocates of these technologies have recently begun to put a new and powerful argument: computer technology for modelling, representing, or creating texts is emerging that will allow us to bring these processes a major step nearer to the activities of actual readers; this in turn will revolutionize understanding of the nature of textuality itself. If this is true, the forthcoming shift in the domain of the literary will be on a tectonic scale, analogous to that brought about in the visual arts by the invention of photography and film.},
	language = {en},
	urldate = {2011-04-26},
	journal = {The Yearbook of English Studies},
	author = {Miall, David D.},
	month = jan,
	year = {1995},
	keywords = {act\_Visualizing, AnalyzeQualitatively, meta\_GiveOverview, t\_Encoding, goal\_Interpretation, obj\_Literature},
	pages = {199--212}
}

@book{bertin_semiology_2010,
	address = {Redlands, Calif},
	edition = {1st ed},
	title = {Semiology of graphics: diagrams, networks, maps},
	isbn = {978-1-58948-261-6},
	shorttitle = {Semiology of graphics},
	url = {http://esripress.esri.com/display/index.cfm?fuseaction=display&websiteID=190},
	abstract = {Originally published in French in 1967, "Semiology of Graphics" holds a significant place in the theory of information design. Founded on Jacques Bertin's practical experience as a cartographer, Part One of this work is an unprecedented attempt to synthesize principles of graphic communication with the logic of standard rules applied to writing and topography. Part Two brings Bertin's theory to life, presenting a close study of graphic techniques including shape, orientation, color, texture, volume, and size in an array of more than 1,000 maps and diagrams.},
	language = {en},
	publisher = {ESRI Press : Distributed by Ingram Publisher Services},
	author = {Bertin, Jacques},
	collaborator = {Berg, William J.},
	year = {2010}
}

@book{bader_simson_1991,
	address = {Tübingen},
	series = {Textwissenschaft, {Theologie}, {Hermeneutik}, {Linguistik}, {Literaturanalyse}, {Informatik}},
	title = {Simson bei {Delila}: computerlinguistische {Interpretation} des {Textes} {Ri} 13 - 16},
	volume = {3},
	isbn = {978-3-7720-1952-4},
	language = {de},
	publisher = {Francke},
	author = {Bader, Winfried},
	year = {1991},
	note = {PhD Thesis: Tübingen, 1989.},
	keywords = {goal\_Interpretation, goal\_Analysis}
}

@misc{connor_software_2013-1,
	title = {Software {Takes} {Command}: {An} {Interview} with {Lev} {Manovich}},
	url = {http://rhizome.org/editorial/2013/jul/10/lev-manovich-interview/},
	abstract = {Lev Manovich is a leading theorist of cultural objects produced with digital technology, perhaps best known for The Language of New Media (MIT Press, 2001). I interviewed him about his most recent book, Software Takes Command (Bloomsbury Academic, July 2014).},
	language = {en},
	journal = {Rhizome},
	author = {Connor, Michael},
	month = oct,
	year = {2013},
	keywords = {meta\_Theorizing, obj\_Databases, obj\_Software}
}

@inproceedings{cox_supporting_2000,
	address = {Philadelphia, Pennsylvania, United States},
	title = {Supporting collaborative interpretation in distributed {Groupware}},
	isbn = {978-1-58113-222-9},
	url = {http://portal.acm.org/citation.cfm?id=359000},
	doi = {10.1145/358916.359000},
	abstract = {Collaborative interpretationoccurs when a group interprets and transforms a diverse set of information fragments into a coherent set of meaningful descriptions. This activity is characterized byemergence, where the participants' shared understanding develops gradually as they interact with each other and the source material. Our goal is to support collaborative interpretation by small, distributed groups. To achieve this, we first observed how face-to-face groups perform collaborative interpretation in a particular work context. We then synthesized design principles from two relevant areas: the key behaviors of people engaged in activities where emergence occurs, and how distributed groups work together over visual surfaces. We built and evaluated a system that supports a specific collaborative interpretation task. This system provides a large workspace and several objects that encourages emergence in interpretation. People manipulatecardsthat contain the raw information fragments. They reduce complexity by placing duplicate cards intopiles. They suggest groupings as they manipulate the spatial layout of cards and piles. They enrich spatial layouts throughnotes, textandfreehand annotations. They record their understanding of their final groupings asreportscontaining coherent descriptions.},
	language = {en},
	urldate = {2009-05-03},
	publisher = {ACM},
	author = {Cox, Donald and Greenberg, Saul},
	year = {2000},
	keywords = {goal\_Interpretation, obj\_Infrastructures, goal\_Collaboration, obj\_People},
	pages = {289--298}
}

@incollection{puretskiy_survey_2010,
	title = {Survey of {Text} {Visualization} {Techniques}},
	copyright = {Copyright © 2010 John Wiley \& Sons, Ltd},
	isbn = {978-0-470-68964-6},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/9780470689646.ch6/summary},
	abstract = {This chapter contains sections titled: * Visualization in text analysis * Tag clouds * Authorship and change tracking * Data exploration and the search for novel patterns * Sentiment tracking * Visual analytics and FutureLens * Scenario discovery * Earlier prototype * Features of FutureLens * Scenario discovery example: bioterrorism * Scenario discovery example: drug trafficking * Future work * References},
	language = {en},
	urldate = {2013-05-09},
	booktitle = {Text {Mining}},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Puretskiy, Andrey A. and Shutt, Gregory L. and Berry, Michael W.},
	editor = {Berry, Michael W. and Kogan, Jacob},
	year = {2010},
	keywords = {act\_Visualizing},
	pages = {105--127}
}

@inproceedings{shneiderman_eyes_1996,
	title = {The {Eyes} {Have} {It}: {A} {Task} by {Data} {Type} {Taxonomy} for {Information} {Visualizations}},
	url = {http://www.cs.ubc.ca/~tmm/courses/old533/readings/shneiderman96eyes.pdf},
	doi = {10.1109/VL.1996.545307},
	abstract = {A useful starting point for designing advanced graphical user interfaces is the visual information seeking Mantra: overview first, zoom and filter, then details on demand. But this is only a starting point in trying to understand the rich and varied set of information visualizations that have been proposed in recent years. The paper offers a task by data type taxonomy with seven data types (one, two, three dimensional data, temporal and multi dimensional data, and tree and network data) and seven tasks (overview, zoom, filter, details-on-demand, relate, history, and extracts)},
	language = {en},
	booktitle = {Proceedings of {Visual} {Languages} 1996},
	author = {Shneiderman, Ben},
	year = {1996},
	pages = {336 -- 343},
	file = {Shneiderman_1996_The Eyes Have It.pdf:/home/lisnux/Zotero/storage/LLTUSBME/Shneiderman_1996_The Eyes Have It.pdf:application/pdf}
}

@article{baldwin_idiocy_2013,
	title = {The {Idiocy} of the {Digital} {Literary} (and what does it have to do with digital humanities)},
	volume = {7},
	url = {http://digitalhumanities.org/dhq/vol/7/1/000155/000155.html},
	abstract = {This essay considers the "idiocy" of the literary: its unaccountable singularity, which guarantees that we continue to return to it as a source, inspiration, and challenge. As a consequence, digital humanities is inspired and irritated by the literary.

My essay shows this in three ways. First, through a speculative exploration of the relation between digital humanities and the category of "the literary." Second, through a quick survey of the use of literature in digital humanities project. Thirdly, through a specific examination of TEI and character rendering as digital humanities concerns that necessarily engage with the literary. Once again, the literary remains singular and not abstract, literal in a way that challenges and provokes us towards new digital humanities work.},
	language = {en},
	number = {7.1},
	author = {Baldwin, Sandy},
	year = {2013},
	keywords = {meta\_Assessing, obj\_DigitalHumanities, meta\_Theorizing, act\_Conceptualizing, goal\_Interpretation, act\_Annotating, goal\_Create, obj\_Data/Databases}
}

@article{clement_makings_2009,
	title = {The {Makings} of {Digital} {Modernism}: {Rereading} {Gertrude} {Stein}'s {The} {Making} of {Americans} and {Poetry} by {Elsa} von {Freytag}-{Loringhoven}},
	shorttitle = {The {Makings} of {Digital} {Modernism}},
	url = {http://drum.lib.umd.edu/handle/1903/9160},
	abstract = {In this dissertation, I argue that digital methodologies offer new kinds of evidence and uncover new opportunities for changing how we do research and what we value as objects for literary study. In particular, I show how text mining, visualizations, digital editing, and social networks can be applied to make new readings of texts that have historically been undervalued within academic research. For example, I read Gertrude Stein's {\textless}italic{\textgreater}The Making of Americans{\textless}/italic{\textgreater} at a distance by analyzing large sets of data mined from the text and visualized within various applications. I also perform close readings of the poetry of Elsa von Freytag-Loringhoven differently by engaging online social networks in which textual performance, an ever-changing interpretive presentation of text, is enacted. By facilitating readings that allow submerged textual and social patterns to emerge, this research resituates digital methodologies and these modernist works within literary studies.},
	language = {en},
	urldate = {2012-04-25},
	author = {Clement, Tanya E},
	year = {2009},
	keywords = {goal\_Interpretation, goal\_Analysis}
}

@inproceedings{mccarty_residue_2012,
	address = {Köln},
	title = {The residue of uniqueness},
	url = {http://www.cceh.uni-koeln.de/files/McCarty.pdf},
	abstract = {To build an argument for the supervening importance of agenda, I locate the digital
humanities within the context of a central human predicament: the anxiety of
identity stemming from the problematic relation of human to non-human, both
animal and machine. I identify modelling as the fundamental activity of the digital
humanities and draw a parallel between it and our developing confrontation with the
not-us. I then go on to argue that the demographics of infrastructure within the
digital humanities, therefore in part its emphasis, is historically due to the socially
inferior role assigned to those who in the early years found para-academic
employment in service to the humanities.  I do not specify an agenda, rather conclude that modelling, pursued within its humane context, offers a cornucopia of agenda if
only the “mind-forged manacles” of servitude’s mind-set can be broken.},
	language = {en},
	author = {McCarty, Willard},
	month = apr,
	year = {2012},
	keywords = {obj\_AnyObject, act\_Modeling},
	pages = {23}
}

@inproceedings{card_structure_1997,
	title = {The {Structure} of the {Information} {Visualization} {Design} {Space}},
	url = {http://www.cs.ubc.ca/~tmm/courses/old533/readings/card96structure.pdf},
	doi = {10.1109/INFVIS.1997.636792},
	abstract = {Research on information visualization has reached the
place where a number of successful point designs have
been proposed and a number of techniques of been
discovered. It is now appropriate to begin to describe and
analyze portions of the design space so as to understand
the differences among designs and to suggest new
possibilities. This paper proposes an organization of the
information visualization literature and illustrates it with a
series of examples. The result is a framework for
designing new visualizations and augmenting existing
designs},
	language = {en},
	booktitle = {Proceedings of {IEEE} {Symposium} on {Information} {Visualization} 1997},
	author = {Card, Stuart K. and Mackinlay, Jock},
	year = {1997},
	pages = {92--99}
}

@book{tufte_visual_2001,
	address = {Cheshire, Conn},
	edition = {2nd ed},
	title = {The visual display of quantitative information},
	isbn = {978-0-9613921-4-7},
	url = {http://www.edwardtufte.com/tufte/books_vdqi},
	abstract = {The classic book on statistical graphics, charts, tables. Theory and practice in the design of data graphics, 250 illustrations of the best (and a few of the worst) statistical graphics, with detailed analysis of how to display data for precise, effective, quick analysis. Design of the high-resolution displays, small multiples. Editing and improving graphics. The data-ink ratio. Time-series, relational graphics, data maps, multivariate designs. Detection of graphical deception: design variation vs. data variation. Sources of deception. Aesthetics and data graphical displays.

This is the second edition of The Visual Display of Quantitative Information. Recently published, this new edition provides excellent color reproductions of the many graphics of William Playfair, adds color to other images, and includes all the changes and corrections accumulated during 17 printings of the first edition.},
	language = {en},
	publisher = {Graphics Press},
	author = {Tufte, Edward R.},
	year = {2001},
	keywords = {act\_Visualizing}
}

@book{lima_visual_2011,
	address = {Princeton, N.J.},
	title = {Visual {Complexity}: {Mapping} {Patterns} of {Information}},
	isbn = {978-1-56898-936-5},
	shorttitle = {Visual {Complexity}},
	url = {http://www.amazon.de/exec/obidos/ASIN/1568989369/visualcompl0f-20/},
	abstract = {Several researchers, scientists and designers across the globe are trying to make sense of a variety of complex networks employing an innovative mix of colors, symbols, graphics, algorithms, and interactivity to clarify, and often beautify, the clutter. By doing this they are in many ways creating the syntax of a new language. This book can be seen as the first dictionary of this new lexicon.

In Visual Complexity: Mapping Patterns of Information, Manuel Lima collects and presents almost three hundred of the most compelling examples of information design — everything from representing networks of followers on Twitter and the eighty-five recorded covers of Joy Division’s “Love Will Tear Us Apart” to depicting interconnections between members of the Al Queda network and interactions among proteins in a human cell. Lima also looks at the long tradition of mapping complex networks, offering the first book to integrate a thorough history of network vizualization with an examination of the real-life situations from which these graphics are generated.},
	language = {en},
	publisher = {Princeton Univ. Press},
	author = {Lima, Manuel},
	year = {2011},
	keywords = {act\_Visualizing, meta\_GiveOverview}
}

@inproceedings{ahlberg_visual_1994,
	address = {New York},
	title = {Visual {Information} {Seeking}: {Tight} {Coupling} of {Dynamic} {Query} {Filters} with {Starfield} {Displays}},
	url = {http://dl.acm.org/citation.cfm?id=191775},
	abstract = {This paper offers new principles for visual information
seeking (VIS). A key concept is to support browsing, which
is distinguished from familiar query composition and
information retrieval because of its emphasis on rapid
filtering to reduce result sets, progressive refinement of
search parameters, continuous reformulation of goals, and
visual scanning to identify results. VIS principles
developed include: dynamic query filters (query parameters
are rapidly adjusted with sliders, buttons, maps, etc.),
starfield displays (two-dimensional scatterplots to structure
tesult sets and zooming to reduce clutter), and tight coupling
(interrelating query components to preserve display
invariants and support progressive refinement combined
with an emphasis on using search output to foster search
input). A FilmFinder prototype using a movie database
demonstrates these principles in a VIS environment.},
	language = {en},
	booktitle = {Proceeding {CHI} '94 {Proceedings} of the {SIGCHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Ahlberg, Christopher and Shneiderman, Ben},
	year = {1994},
	pages = {313--317}
}

@book{cleveland_visualizing_1993,
	address = {Murray Hill, N.J. : [Summit, N.J},
	title = {Visualizing data},
	isbn = {978-0-9634884-0-4},
	url = {http://dl.acm.org/citation.cfm?id=529269},
	abstract = {Enormous quantities of data go unused or underused today, simply because people can't visualize the quantities and relationships in it. Using a downloadable programming environment developed by the author, Visualizing Data demonstrates methods for representing data accurately on the Web and elsewhere, complete with user interaction, animation, and more. How do the 3.1 billion A, C, G and T letters of the human genome compare to those of a chimp or a mouse? What do the paths that millions of visitors take through a web site look like? With Visualizing Data, you learn how to answer complex questions like these with thoroughly interactive displays. We're not talking about cookie-cutter charts and graphs. This book teaches you how to design entire interfaces around large, complex data sets with the help of a powerful new design and prototyping tool called "Processing". Used by many researchers and companies to convey specific data in a clear and understandable manner, the Processing beta is available free. With this tool and Visualizing Data as a guide, you'll learn basic visualization principles, how to choose the right kind of display for your purposes, and how to provide interactive features that will bring users to your site over and over. This book teaches you: The seven stages of visualizing data -- acquire, parse, filter, mine, represent, refine, and interactHow all data problems begin with a question and end with a narrative construct that provides a clear answer without extraneous detailsSeveral example projects with the code to make them workPositive and negative points of each representation discussed. The focus is on customization so that each one best suits what you want to convey about your data set The book does not provide ready-made "visualizations" that can be plugged into any data set. Instead, with chapters divided by types of data rather than types of display, you'll learn how each visualization conveys the unique properties of the data it represents -- why the data was collected, what's interesting about it, and what stories it can tell. Visualizing Data teaches you how to answer questions, not simply display information.},
	language = {en},
	publisher = {At\&T Bell Laboratories ; Published by Hobart Press},
	author = {Cleveland, William S.},
	year = {1993}
}

@book{fry_visualizing_2008,
	address = {Sebastopol, CA},
	title = {Visualizing {Data}: {Exploring} and {Explaining} {Data} with the {Processing} {Environment}},
	isbn = {978-0-596-51455-6},
	url = {http://books.google.de/books?hl=de&lr=&id=6jsVAiULQBgC&oi=fnd&pg=PR7&dq=Visualizing+data+fry&ots=2yn__7jiTQ&sig=RtGaSWoU-LZvvP3Rp8WBsGPXD8s#v=onepage&q=Visualizing%20data%20fry&f=false},
	abstract = {How you can take advantage of data that you might otherwise never use? With the help of a downloadable programming environment, this book helps you represent data accurately on the Web and elsewhere, complete with user interaction, animation, and more. You'll learn basic visualization principles, how to choose the right kind of display for your purposes, and how to provide interactive features to design entire interfaces around large, complex data sets.},
	language = {en},
	publisher = {O'Reilly Media, Inc},
	author = {Fry, Ben},
	year = {2008}
}

@inproceedings{chaney_visualizing_2012,
	title = {Visualizing {Topic} {Models}},
	url = {http://www.cs.columbia.edu/~blei/papers/ChaneyBlei2012.pdf},
	abstract = {Managing large collections of documents is an important
problem for many areas of science, industry, and
culture. Probabilistic topic modeling offers a promising
solution. Topic modeling is an unsupervised machine
learning method that learns the underlying themes in
a large collection of otherwise unorganized documents.
This discovered structure summarizes and organizes the
documents. However, topic models are high-level statistical
tools—a user must scrutinize numerical distributions
to understand and explore their results. In this
paper, we present a method for visualizing topic models.
Our method creates a navigator of the documents,
allowing users to explore the hidden structure that a
topic model discovers. These browsing interfaces reveal
meaningful patterns in a collection, helping end-users
explore and understand its contents in new ways. We
provide open source software of our method.},
	language = {en},
	booktitle = {{AAAI} {Publications}, {Sixth} {International} {AAAI} {Conference} on {Weblogs} and {Social} {Media}},
	author = {Chaney, Allison and Blei, David M.},
	year = {2012},
	keywords = {act\_Visualizing, AnalyzeStatistically},
	pages = {4}
}

@book{underwood_why_2013,
	address = {Standford},
	title = {Why literary periods mattered: historical contrast and the prestige of {English} studies},
	isbn = {978-0-8047-8446-7},
	shorttitle = {Why literary periods mattered},
	url = {http://www.sup.org/book.cgi?id=22262},
	abstract = {In the mid-nineteenth century, the study of English literature began to be divided into courses that surveyed discrete "periods." Since that time, scholars' definitions of literature and their rationales for teaching it have changed radically. But the periodized structure of the curriculum has remained oddly unshaken, as if the exercise of contrasting one literary period with another has an importance that transcends the content of any individual course.

Why Literary Periods Mattered explains how historical contrast became central to literary study, and why it remained institutionally central in spite of critical controversy about literature itself. Organizing literary history around contrast rather than causal continuity helped literature departments separate themselves from departments of history. But critics' long reliance on a rhetoric of contrasted movements and fateful turns has produced important blind spots in the discipline. In the twenty-first century, Underwood argues, literary study may need digital technology in particular to develop new methods of reasoning about gradual, continuous change.},
	language = {en},
	publisher = {Stanford University Press},
	author = {Underwood, Ted},
	year = {2013}
}

@article{epstein_why_2008,
	title = {Why {Model}?},
	volume = {11},
	copyright = {JASSS@soc.surrey.ac.uk},
	url = {http://jasss.soc.surrey.ac.uk/11/4/12.html},
	abstract = {This lecture treats some enduring misconceptions about modeling. One of these is that the goal is always prediction. The lecture distinguishes between explanation and prediction as modeling goals, and offers sixteen reasons other than prediction to build a model. It also challenges the common assumption that scientific theories arise from and 'summarize' data, when often, theories precede and guide data collection; without theory, in other words, it is not clear what data to collect. Among other things, it also argues that the modeling enterprise enforces habits of mind essential to freedom.  It is based on the author's 2008 Bastille Day keynote address to the Second World Congress on Social Simulation, George Mason University, and earlier addresses at the Institute of Medicine, the University of Michigan, and the Santa Fe Institute.},
	language = {en},
	number = {4},
	urldate = {2014-06-09},
	journal = {Journal of Artificial Societies and Social Simulation},
	author = {Epstein, Joshua M.},
	year = {2008},
	keywords = {act\_Modeling}
}

@misc{gulliver_100_2012-2,
	title = {100 {Commandments} of {Twitter} for {Academics}},
	url = {http://chronicle.com/article/10-Commandments-of-Twitter-for/131813/},
	language = {en},
	author = {Gulliver, Katrina},
	month = may,
	year = {2012}
}

@article{nicholas_policy_2009-1,
	title = {A {Policy} {Checklist} for {Enabling} {Persistence} of {Identifiers}},
	volume = {15},
	issn = {1082-9873},
	url = {http://www.dlib.org/dlib/january09/nicholas/01nicholas.html},
	doi = {10.1045/january2009-nicholas},
	abstract = {One of the main tasks of the Persistent Identifier Linking Infrastructure (PILIN) project on persistent identifiers was to establish a policy framework for managing identifiers and identifier providers. A major finding from the project was that policy is far more important in guaranteeing persistence of identifiers than technology. Key policy questions for guaranteeing identifier persistence include: what entities should be assigned persistent identifiers, how should those identifiers be exposed to services, and what guarantees does the provider make on how long various facets of the identifiers will persist.

To make an informed decision about what to identify, information modelling of the domain is critical. Identifier managers need to know what can be identified discretely (including not only concrete artefacts like files, but also abstractions such as works, versions, presentations, and aggregations); and for which of those objects it is a priority for users and managers to keep track. Without working out what actually needs to be identified, the commitment to keep identifiers persistent becomes meaningless.

To make sure persistent identifiers meet these requirements, the PILIN project has formulated a six-point checklist for integrating identifiers into information management, which we present here.},
	language = {en},
	number = {1/2},
	urldate = {2013-02-01},
	journal = {D-Lib Magazine},
	author = {Nicholas, Nick and Ward, Nigel and Blinco, Kerry},
	month = jan,
	year = {2009},
	keywords = {obj\_Research, act\_Identifying}
}

@techreport{edgar_survey_2010-1,
	title = {A survey of the scholarly journals using open journal systems. {Scholarly} and {Research} {Communication}},
	url = {http://src-online.ca/index.php/src/article/view/24/41},
	abstract = {A survey of 998 scholarly journals that use Open Journal Systems (OJS), an open source journal software platform, captures the characteristics of an emerging class of scholar-publisher open access journals. The journals in the sample follow traditional norms for peer-reviewing, acceptance rates, and disciplinary focus, but as a group are distinguished by the number that offer open access to their content, growth rates in new titles, participation rates from developing countries, and extremely low operating budgets. The survey also documents the limited degree to which open source software can alter a field of communication, for OJS appears to have created a third path, dedicated to maximizing access to research and scholarship, as an alternative to traditional scholarly society and commercial publishing routes.},
	language = {en},
	institution = {Stanford University},
	author = {Edgar, B.D. and Willinsky, John},
	year = {2010},
	keywords = {obj\_Tools, act\_Publishing, obj\_ResearchResults}
}

@misc{noauthor_vos_2011-1,
	title = {A vos marques, prêts, bloguez !},
	url = {http://bublog.upmf-grenoble.fr/2011/11/17/bloguez/},
	language = {fr},
	urldate = {2011-11-26},
	journal = {Le temps d'un blog},
	month = nov,
	year = {2011}
}

@article{noauthor_aahc_2000-1,
	title = {{AAHC} {Suggested} {Guidelines} for {Evaluating} {Digital} {Media} {Activities} in {Tenure}, {Review}, and {Promotion} - {Neue} {Version}},
	volume = {3},
	url = {http://hdl.handle.net/2027/spo.3310410.0003.311},
	abstract = {Note: These recommendations were developed in consultation with the Modern Language Association and the American Political Science Association.},
	language = {en},
	number = {3},
	journal = {Journal of the Association for History and Computing},
	year = {2000},
	keywords = {meta\_Assessing, meta\_DefinePolicy}
}

@article{nicholas_abstract_2010-1,
	title = {Abstract {Modelling} of {Digital} {Identifiers}},
	volume = {62},
	issn = {1361-3200},
	url = {http://www.ariadne.ac.uk/issue62/nicholas-et-al/},
	abstract = {Discussion of digital identifiers, and persistent identifiers in particular, has often been confused by differences in underlying assumptions and approaches. To bring more clarity to such discussions, the PILIN Project has devised an abstract model of identifiers and identifier services, which is presented here in summary. Given such an abstract model, it is possible to compare different identifier schemes, despite variations in terminology; and policies and strategies can be formulated for persistence without committing to particular systems. The abstract model is formal and layered; in this article, we give an overview of the distinctions made in the model. This presentation is not exhaustive, but it presents some of the key concepts represented, and some of the insights that result.

The main goal of the Persistent Identifier Linking Infrastructure (PILIN) project [1] has been to scope the infrastructure necessary for a national persistent identifier service. There are a variety of approaches and technologies already on offer for persistent digital identification of objects. But true identity persistence cannot be bound to particular technologies, domain policies, or information models: any formulation of a persistent identifier strategy needs to outlast current technologies, if the identifiers are to remain persistent in the long term.

For that reason, PILIN has modelled the digital identifier space in the abstract. It has arrived at an ontology [2] and a service model [3] for digital identifiers, and for how they are used and managed, building on previous work in the identifier field [4] (including the thinking behind URI [5], DOI [6], XRI [7] and ARK [8]), as well as semiotic theory [9]. The ontology, as an abstract model, addresses the question ‘what is (and isn’t) an identifier?’ and ‘what does an identifier management system do?’. This more abstract view also brings clarity to the ongoing conversation of whether URIs can be (and should be) universal persistent identifiers.},
	language = {en},
	urldate = {2010-02-23},
	journal = {Ariadne},
	author = {Nicholas, Nick and Ward, Nigel and Blinco, Kerry},
	month = jan,
	year = {2010},
	keywords = {bigdata, obj\_Research, bigdata{\textasciitilde}, act\_Identifying}
}

@article{noauthor_academia_2012-1,
	title = {Academia in the {Age} of {Digital} {Reproduction}; {Or}, the {Journal} {System}, {Redeemed}},
	url = {http://thedisorderofthings.com/2012/02/08/academia-in-the-age-of-digital-reproduction-or-the-journal-system-redeemed/},
	abstract = {It took at least 200 years for the novel to emerge as an expressive form after the invention of the printing press.

So said Bob Stein in an interesting roundtable on the digital university from ...},
	language = {en},
	urldate = {2012-02-09},
	journal = {The Disorder Of Things},
	month = feb,
	year = {2012},
	keywords = {act\_Publishing, meta\_Advocating, obj\_ResearchResults}
}

@misc{carrigan_academic_2013-1,
	title = {Academic blogging – both/and rather than either/or},
	url = {http://markcarrigan.net/2013/01/10/academic-blogging-bothand-rather-than-eitheror/},
	language = {en},
	journal = {mikecarrigan.net},
	author = {Carrigan, Mike},
	month = jan,
	year = {2013},
	keywords = {meta\_Assessing, act\_Publishing, obj\_ResearchResults}
}

@article{xiao_academic_2014-1,
	title = {Academic opinions of {Wikipedia} and open-access publishing},
	volume = {38},
	issn = {1468-4527},
	language = {en},
	number = {3},
	urldate = {2014-05-14},
	journal = {Online Information Review},
	author = {Xiao, Lu and Askin, Nicole},
	month = apr,
	year = {2014},
	pages = {2--2}
}

@misc{hanson_academic_2008-1,
	title = {Academic {Publishing} in the {Digital} {Age}},
	url = {http://hastac.org/forums/hastac-scholars-discussions/academic-publishing-digital-age},
	language = {en},
	journal = {HASTAC Scholars Forum},
	author = {Hanson, Christopher},
	year = {2008}
}

@techreport{finch_accessibility_2012-1,
	title = {Accessibility, sustainability, excellence: how to expand access to research publications},
	url = {http://www.researchinfonet.org/publish/finch/},
	abstract = {Report of the Working Group on Expanding Access to Published Research Findings – the Finch Group},
	language = {en},
	author = {Finch},
	month = jun,
	year = {2012}
}

@article{anderson-wilk_achieving_2011-1,
	title = {Achieving rigor and relevance in online multimedia scholarly publishing},
	volume = {16},
	url = {http://firstmonday.org/htbin/cgiwrap/bin/ojs/index.php/fm/article/view/3762/3119},
	abstract = {This paper discusses the importance of relevance and rigor in scholarly publishing in a new media–rich world. We defend that scholarship should be useful and engaging to audiences through the use of new media, and at the same time scholarly publishers must develop and maintain methods of ensuring content accuracy and providing quality controls in the production of scholarly multimedia products. We review examples and a case study of existing scholarly publishing venues that attempt to maintain quality control standards while embracing innovative multimedia formats. We also present lessons learned from the case experience and challenges that face us in the scholarly publication of multimedia.},
	language = {en},
	number = {12},
	journal = {First Monday},
	author = {Anderson-Wilk, Mark and Hino, Jeff},
	year = {2011}
}

@article{jessen_aggregated_2012-1,
	title = {Aggregated trustworthiness: {Redefining} online credibility through social validation},
	volume = {17},
	url = {http://firstmonday.org/htbin/cgiwrap/bin/ojs/index.php/fm/article/view/3731/3132},
	abstract = {This article investigates the impact of social dynamics on online credibility. Empirical studies by Pettingill (2006) and Hargittai, et al. (2010) suggest that social validation and online trustees play increasingly important roles when evaluating credibility online. This dynamic puts pressure on the dominant theory of online credibility presented by Fogg and Tseng (1999). To remedy this problem we present a new theory we call “aggregated trustworthiness” based on social dynamics and online navigational practices.},
	language = {en},
	number = {1-2},
	journal = {First Monday},
	author = {Jessen, Johan and Jørgensen, Anker Helms},
	year = {2012},
	keywords = {meta\_Assessing, obj\_ResearchResults}
}

@misc{priem_alt-metrics_2010-1,
	title = {Alt-metrics: {A} {Manifesto}},
	url = {http://altmetrics.org/manifesto/},
	language = {en},
	journal = {altmetrics.org},
	author = {Priem, Jason and Taraborelli, D. and Groth, P. and Neylon, C.},
	year = {2010},
	keywords = {bigdata{\textasciitilde}}
}

@book{harley_assessing_2010-1,
	address = {Berkeley},
	title = {Assessing the {Future} {Landscape} of {Scholarly} {Communication}: {An} {Exploration} of {Faculty} {Values} and {Needs} in {Seven} {Disciplines}},
	url = {http://www.escholarship.org/uc/item/15x7385g},
	abstract = {Since 2005, the Center for Studies in Higher Education (CSHE), with generous funding from the
Andrew W. Mellon Foundation, has been conducting research to understand the needs and
practices of faculty for in-progress scholarly communication (i.e., forms of communication
employed as research is being executed) as well as archival publication. This report brings
together the responses of 160 interviewees across 45, mostly elite, research institutions in seven
selected academic fields: archaeology, astrophysics, biology, economics, history, music, and
political science. The overview document summarizes the main practices we explored across all
seven disciplines: tenure and promotion, dissemination, sharing, collaboration, resource creation
and consumption, and public engagement. We published the report online in such a way that
readers can search various topics within and across case studies.∗ Our premise has always been
that disciplinary conventions matter and that social realities (and individual personality) will
dictate how new practices, including those under the rubric of Web 2.0 or cyberinfrastructure,
are adopted by scholars. That is, the academic values embodied in disciplinary cultures, as well
as the interests of individual players, have to be considered when envisioning new schemata for
the communication of scholarship at its various stages.
We identified five key topics, addressed in detail in the case studies, that require real attention:
(1) The development of more nuanced tenure and promotion practices that do not rely
exclusively on the imprimatur of the publication or easily gamed citation metrics,
(2) A reexamination of the locus, mechanisms, timing, and meaning of peer review,
(3) Competitive, high-quality, and affordable journals and monograph publishing platforms
(with strong editorial boards, peer review, and sustainable business models),
(4) New models of publication that can accommodate arguments of varied length, rich
media, and embedded links to data; plus institutional assistance to manage permissions
of copyrighted material, and
(5) Support for managing and preserving new research methods and products, including
components of natural language processing, visualization, complex distributed
databases, and GIS, among many others.
Although robust infrastructures are needed locally and beyond, the sheer diversity of scholars’
needs across the disciplines and the rapid evolution of the technologies themselves means that
one-size-fits-all solutions will almost always fall short. As faculty continue to innovate and pursue
new avenues in their research, both the technical and human infrastructure will have to evolve
with the ever-shifting needs of scholars. This infrastructure will, by necessity, be built within the
context of disciplinary conventions, reward systems, and the practice of peer review, all of which
undergird the growth and evolution of superlative academic endeavors.},
	language = {en},
	publisher = {CSHE, UC Berkeley},
	author = {Harley, Diane and Acord, Sophia Krzys and Earl-Novell, Sarah and Lawrence, Shannon and King, C. Judson},
	month = jan,
	year = {2010},
	keywords = {goal\_Dissemination, obj\_ResearchResults}
}

@misc{montgomery_assessing_2011-1,
	title = {Assessing {Work} on {Digital} {Projects} for {Hiring} and {Tenure}: {Discouraging} {Young} {Scholars}?},
	url = {http://hastac.org/blogs/katherine-f-montgomery/2011/11/27/assessing-work-digital-projects-hiring-and-tenure-discouragi},
	abstract = {One of the longest-running digital projects at the University of Iowa is the Walt Whitman Archive, a rich and constantly-growing interactive collection of Whitman’s life, letters, manuscripts, writing, criticism, recordings, and any and all digital (or digitizable) material on Walt Whitman.},
	language = {en},
	urldate = {2011-11-29},
	journal = {HASTAC Blog},
	author = {Montgomery, Katherine F.},
	month = nov,
	year = {2011}
}

@article{bar-ilan_beyond_2012,
	title = {Beyond citations: {Scholars}' visibility on the social {Web}},
	volume = {abs/1205.5611},
	url = {http://arxiv.org/abs/1205.5611},
	abstract = {Traditionally, scholarly impact and visibility have been measured by counting publications and
citations in the scholarly literature. However, increasingly scholars are also visible on the Web,
establishing presences in a growing variety of social ecosystems. But how wide and established is
this presence, and how do measures of social Web impact relate to their more traditional
counterparts? To answer this, we sampled 57 presenters from the 2010 Leiden STI Conference,
gathering publication and citations counts as well as data from the presenters’ Web “footprints.”
We found Web presence widespread and diverse: 84\% of scholars had homepages, 70\% were on
LinkedIn, 23\% had public Google Scholar profiles, and 16\% were on Twitter. For sampled
scholars’ publications, social reference manager bookmarks were compared to Scopus and Web
of Science citations; we found that Mendeley covers more than 80\% of sampled articles, and that
Mendeley bookmarks are significantly correlated (r=.45) to Scopus citation counts.},
	language = {en},
	journal = {CoRR},
	author = {Bar-Ilan, Judit and Haustein, Stefanie and Peters, Isabella and Priem, Jason and Shema, Hadas and Terliesner, Jens},
	year = {2012}
}

@article{cronin_bibliometrics_2001,
	title = {Bibliometrics and beyond: some thoughts on web-based citation analysis},
	volume = {27},
	issn = {0165-5515},
	shorttitle = {Bibliometrics and beyond},
	url = {http://jis.sagepub.com/cgi/doi/10.1177/016555150102700101},
	doi = {10.1177/016555150102700101},
	abstract = {The idea of a unified citation index to the literature of science was first outlined by Eugene Garfield [1] in 1955 in the journal Science. Science Citation Index has since established itself as the gold standard for scientific information retrieval. It has also become the database of choice for citation analysts and evaluative bibliometricians worldwide. As scientific publication moves to the web, and novel approaches to scholarly communication and peer review establish themselves, new methods of citation and link analysis will emerge to capture often liminal expressions of peer esteem, influence and approbation. The web thus affords bibliometricians rich opportunities to apply and adapt their techniques to new contexts and content: the age of ‘bibliometric spectroscopy’ [2] is dawning.},
	language = {en},
	number = {1},
	urldate = {2012-06-18},
	journal = {Journal of Information Science},
	author = {Cronin, B.},
	month = feb,
	year = {2001},
	pages = {1--7}
}

@book{rettberg_blogging_2013,
	edition = {Second Edition},
	series = {Digital {Media} and {Society}},
	title = {Blogging},
	isbn = {978-0-7456-4134-8},
	url = {http://books.google.com/books/about/Blogging.html?id=lo0WUF0YpsMC},
	abstract = {Blogging has profoundly influenced not only the nature of the internet today, but also the nature of modern communication, despite being a genre invented less than a decade ago. This book–length study of a now everyday phenomenon provides a close look at blogging while placing it in a historical, theoretical and contemporary context. Scholars, students and bloggers will find a lively survey of blogging that contextualises blogs in terms of critical theory and the history of digital media. Authored by a scholar–blogger, the book is packed with examples that show how blogging and related genres are changing media and communication. It gives definitions and explains how blogs work, shows how blogs relate to the historical development of publishing and communication and looks at the ways blogs structure social networks and at how social networking sites like MySpace and Facebook incorporate blogging in their design. Specific kinds of blogs discussed include political blogs, citizen journalism, confessional blogs and commercial blogs.},
	language = {en},
	publisher = {Polity Press},
	author = {Rettberg, Jill Walker},
	year = {2013},
	keywords = {act\_Publishing, goal\_Dissemination, obj\_ResearchResults}
}

@article{nolin_boundaries_2011,
	title = {Boundaries of research disciplines are paper constructs: {Digital} {Web}-based information as a challenge to disciplinary research},
	volume = {16},
	url = {http://firstmonday.org/htbin/cgiwrap/bin/ojs/index.php/fm/article/view/3669/3080},
	abstract = {Boundaries of research disciplines are paper constructs: Digital Web-based information as a challenge to disciplinary research Jan Michael Nolin Abstract Modern disciplinary research is partly constructed, and limited, by the medium of paper. It is possible to bypass the restraints imposed by paper in modern Web publication. Still, the research sector keeps publishing as if the qualities of the hard copy should be forced on the Web. This article discusses the role of paper in the construction of the boundaries of disciplines and the challenges from digital Web-based publication.},
	language = {en},
	number = {11},
	journal = {First Monday},
	author = {Nolin, Jan Michael},
	year = {2011}
}

@book{chambers_catalogue_2010,
	address = {London},
	title = {Catalogue 2.0},
	isbn = {978-1-85604-716-6},
	url = {http://www.ala.org/transforminglibraries/catalogue-20},
	abstract = {New digital technologies, the Internet, and user expectations have changed the role of the catalogue in libraries considerably in recent years. This timely book takes into account developments that influence catalogue potential and patrons’ needs, such as competition from popular Web sites like Facebook, Twitter, and Wikipedia. Here, key leaders in the field like Karen Calhoun, past OCLC Vice-President, and Marshall Breeding, author of Cloud Computing for Libraries, explore concepts like:What do your users want?},
	language = {en},
	publisher = {Facet},
	author = {Chambers, Sally},
	year = {2010},
	keywords = {obj\_Data/Databases, act\_Query/Retrieve}
}

@techreport{kelleher_changing_2011,
	address = {Young Researchers Forum ESF Humanities, June 2011, Maynooth, Ireland.},
	title = {Changing {Publication} {Practices} in the {Humanities}.},
	url = {http://www.esf.org/fileadmin/Public_documents/Publications/Changing_Publication_Cultures_Humanities.pdf},
	language = {en},
	institution = {European Science Foundation (ESF)},
	author = {Kelleher, Margaret and Hoogland, Eva},
	year = {2011},
	keywords = {goal\_Dissemination, meta\_Advocating, obj\_ResearchResults}
}

@techreport{noauthor_changing_2011,
	title = {Changing the {Conduct} of {Science} in the {Information} {Age}. {Summary} {Report} of {Workshop} {Held} on {November} 12, 2010},
	url = {http://www.nsf.gov/pubs/2011/oise11003/},
	language = {en},
	institution = {National Science Foundation (NSF)},
	month = jun,
	year = {2011}
}

@incollection{morrison_chapter_2011,
	title = {Chapter {Two}: {Scholarly} {Communication} in {Crisis}},
	url = {http://pages.cmns.sfu.ca/heather-morrison/chapter-two-scholarly-communication-in-crisis/},
	abstract = {"Scholarly communication at present is a complex system characterized by expansion of capitalism into scholarly publishing and a process of rationalization that at times leads to irrational results in conflict with the basic goals or values of scholars. The increasing enclosure of knowledge and information through the concept of intellectual property is key in the process of commodification of resources once considered a classical public good as nonrivalrous and nonexcludable. Alternatives identified to date include the commons, cooperative approaches, open access and emerging new publishers such as libraries."},
	language = {en},
	urldate = {2012-01-09},
	booktitle = {Freedom for scholarship in the internet age. {Doctoral} dissertation (in process)},
	publisher = {Simon Fraser University, School of Communication},
	author = {Morrison, Heather},
	month = nov,
	year = {2011}
}

@article{lawrence_citation_2011,
	title = {Citation and {Peer} {Review} of {Data}: {Moving} {Towards} {Formal} {Data} {Publication}},
	volume = {6},
	copyright = {Copyright for articles published in this journal is retained by the authors, with first publication rights granted to the University of Bath. By virtue of their appearance in this open access journal, articles are free to use, with proper attribution, in educational and other non-commercial settings.      This work is licenced under a  Creative Commons Licence .},
	issn = {1746-8256},
	shorttitle = {Citation and {Peer} {Review} of {Data}},
	url = {http://www.ijdc.net/index.php/ijdc/article/view/181},
	abstract = {This paper discusses many of the issues associated with formally publishing data in academia, focusing primarily on the structures that need to be put in place for peer review and formal citation of datasets. Data publication is becoming increasingly important to the scientific community, as it will provide a mechanism for those who create data to receive academic credit for their work and will allow the conclusions arising from an analysis to be more readily verifiable, thus promoting transparency in the scientific process. Peer review of data will also provide a mechanism for ensuring the quality of datasets, and we provide suggestions on the types of activities one expects to see in the peer review of data. A simple taxonomy of data publication methodologies is presented and evaluated, and the paper concludes with a discussion of dataset granularity, transience and semantics, along with a recommended human-readable citation syntax.},
	language = {en},
	number = {2},
	urldate = {2011-11-26},
	journal = {International Journal of Digital Curation},
	author = {Lawrence, Bryan and Jones, Catherine and Matthews, Brian and Pepler, Sam and Callaghan, Sarah},
	month = oct,
	year = {2011},
	keywords = {meta\_Assessing, act\_Publishing, obj\_Data, obj\_Metadata}
}

@article{fitzpatrick_commentpress_2007,
	title = {{CommentPress}: {New} ({Social}) {Structures} for {New} ({Networked}) {Texts}},
	volume = {10},
	url = {http://quod.lib.umich.edu/cgi/t/text/text-idx?c=jep;view=text;rgn=main;idno=3336451.0010.305},
	doi = {http://dx.doi.org/10.3998/3336451.0010.305},
	language = {en},
	number = {3},
	journal = {Journal of Electronic Publishing},
	author = {Fitzpatrick, Kathleen},
	year = {2007},
	file = {Fitzpatrick_2007_CommentPress.pdf:/home/lisnux/Zotero/storage/FQNDBGXJ/Fitzpatrick_2007_CommentPress.pdf:application/pdf}
}

@techreport{noauthor_commission_2012,
	address = {Brussels},
	title = {Commission {Recommendation} on access to and preservation of scientific information},
	url = {http://ec.europa.eu/research/science-society/document_library/pdf_06/recommendation-access-and-preservation-scientific-information_en.pdf},
	language = {en},
	institution = {European Commission},
	month = jul,
	year = {2012}
}

@misc{berners-lee_cool_1998,
	title = {Cool {URIs} don't {Change}},
	url = {http://www.w3.org/Provider/Style/URI},
	abstract = {There are no reasons at all in theory for people to change URIs (or stop maintaining documents), but millions of reasons in practice.

In theory, the domain name space owner owns the domain name space and therefore all URIs in it. Except insolvency, nothing prevents the domain name owner from keeping the name. And in theory the URI space under your domain name is totally under your control, so you can make it as stable as you like. Pretty much the only good reason for a document to disappear from the Web is that the company which owned the domain name went out of business or can no longer afford to keep the server running. Then why are there so many dangling links in the world? Part of it is just lack of forethought. Here are some reasons you hear out there:},
	language = {en},
	author = {Berners-Lee, Tim},
	year = {1998},
	keywords = {obj\_Research, act\_Identifying}
}

@book{de_santis_crossing_2019,
	address = {Berlin, Boston},
	title = {Crossing {Experiences} in {Digital} {Epigraphy}, {From} {Practice} to {Discipline}},
	isbn = {978-3-11-060719-2},
	url = {https://www.degruyter.com/viewbooktoc/product/506243#},
	language = {ENGL},
	urldate = {2019-08-21},
	publisher = {De Gruyter},
	author = {De Santis, Annamaria and Rossi, Irene},
	year = {2019},
	doi = {10.1515/9783110607208},
	file = {De Santis_Rossi_2019_Crossing Experiences in Digital Epigraphy, From Practice to Discipline.pdf:/home/lisnux/Zotero/storage/59TSS2EY/De Santis_Rossi_2019_Crossing Experiences in Digital Epigraphy, From Practice to Discipline.pdf:application/pdf}
}

@article{conway_curating_2011,
	title = {Curating {Scientific} {Research} {Data} for the {Long} {Term}: {A} {Preservation} {Analysis} {Method} in {Context}},
	volume = {6},
	copyright = {Copyright for articles published in this journal is retained by the authors, with first publication rights granted to the University of Bath. By virtue of their appearance in this open access journal, articles are free to use, with proper attribution, in educational and other non-commercial settings.      This work is licenced under a  Creative Commons Licence .},
	issn = {1746-8256},
	shorttitle = {Curating {Scientific} {Research} {Data} for the {Long} {Term}},
	url = {http://www.ijdc.net/index.php/ijdc/article/view/182},
	abstract = {The challenge of digital preservation of scientific data lies in the need to preserve not only the dataset itself but also the ability it has to deliver knowledge to a future user community. A true scientific research asset allows future users to reanalyze the data within new contexts. Thus, in order to carry out meaningful preservation we need to ensure that future users are equipped with the necessary information to re-use the data. This paper presents an overview of a preservation analysis methodology which was developed in response to that need on the CASPAR and Digital Curation Centre SCARP projects. We intend to place it in relation to other digital preservation practices, discussing how they can interact to provide archives caring for scientific data sets with the full arsenal of tools and techniques necessary to rise to this challenge.},
	language = {en},
	number = {2},
	urldate = {2011-11-26},
	journal = {International Journal of Digital Curation},
	author = {Conway, Esther and Giaretta, David and Lambert, Simon and Matthews, Brian},
	month = oct,
	year = {2011},
	keywords = {act\_Archiving}
}

@article{stober_internet_2004,
	title = {Das {Internet} als {Medium} geistes- und kulturwissenschaftlicher {Publikation}. {Pragmatische} und epistemologische {Fragestellungen}},
	url = {http://web.fu-berlin.de/phin/beiheft2/b2t15.htm},
	abstract = {The article starts with pointing out both the fundamental advantages and the actual problems created by electronic publishing in the humanities while detailing recent approaches which aim at solving these problems. In addition to this pragmatic perspective which illustrates how electronic publishing might optimize communication within the humanities, the article focuses furthermore on an epistemological perspective considering the way in which the hypertextuality of digital media could influence scientific discourse and contribute to new ways of knowledge representation.},
	language = {de},
	number = {2},
	journal = {Philologie im Netz (PhiN): Beihefte},
	author = {Stöber, Thomas},
	year = {2004},
	keywords = {goal\_Dissemination, obj\_ResearchResults},
	pages = {282--296}
}

@article{rapp_memex-idee_2013-1,
	title = {Die {Memex}-{Idee}. {Vannevar} {Bush} und die maschinelle {Erweiterung} des {Denkens}},
	volume = {4},
	issn = {1863-8937},
	url = {http://www.z-i-g.de/vorschau.cfm?akt=1},
	language = {de},
	author = {Rapp, Andrea and Bender, Michael},
	year = {2013},
	keywords = {obj\_Text, obj\_Infrastructures, obj\_Documents, meta\_Collaborating, obj\_Knowledge},
	pages = {53--64}
}

@article{landes_schriften_2011,
	title = {Die {Schriften} der anderen. {Rezensionskultur} im {Umbruch}},
	volume = {62},
	language = {de},
	number = {11-12},
	journal = {Geschichte in Wissenschaft und Unterricht},
	author = {Landes, Lilian},
	year = {2011},
	note = {Dossier: "Internetressourcen zur Geschichte"},
	pages = {669--672}
}

@misc{mounier_werkstatt_2011,
	title = {Die {Werkstatt} des {Historikers} öffnen: {Soziale} {Medien} und {Wissenschaftsblogs}},
	shorttitle = {Die {Werkstatt} des {Historikers} öffnen},
	url = {http://dhdhi.hypotheses.org/591},
	abstract = {In welcher Weise verändern die digitalen Technologien die Arbeitsbedingungen des Historikers? Bereits seit mehreren Jahrzehnten gibt es darauf Antworten. Die Erstellung quantitativer Datenbanken, die Digitalisierung wichtiger Quellen, die kartografische Darstellung und die Analyse sozialer Netzwerke mit IT-Werkzeugen sind dabei die ältesten und wichtigsten Meilensteine. Die Retrodigitalisierung und Online-Veröffentlichung akademischer Literatur der Disziplin, seien es Zeitschriften oder Bücher, stellen eine weitere Etappe in dieser Richtung dar. Die digitale Technologie hat bis heute zugleich den Werkzeugkasten des Historikers – um den schönen Ausdruck aus dem gleichnamigen Blog La Boite à Outil des Historien aufzugreifen – und seine Publikationsmöglichkeiten grundlegend verändert.},
	language = {de},
	urldate = {2011-11-15},
	journal = {Digital Humanities am DHIP},
	author = {Mounier, Pierre},
	month = nov,
	year = {2011},
	keywords = {X-CHECK}
}
